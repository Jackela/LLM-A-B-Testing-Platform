#!/usr/bin/env python3
"""
Merge ARC-Easy Complete Dataset Test Results
æ‹¼æ¥ARC-Easyå®Œæ•´æ•°æ®é›†æµ‹è¯•ç»“æœ
"""

import json
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def load_all_test_results() -> List[Dict[str, Any]]:
    """åŠ è½½æ‰€æœ‰æµ‹è¯•ç»“æœæ–‡ä»¶"""
    results_dir = Path("logs/complete_arc_easy_results")
    all_results = []
    
    # æ‰¾åˆ°æ‰€æœ‰ç»“æœæ–‡ä»¶ï¼ŒæŒ‰æ—¶é—´æ’åº
    result_files = []
    
    # åŸå§‹æµ‹è¯•æ–‡ä»¶ (904æ ·æœ¬)
    original_files = [
        "arc_easy_complete_test_interim_904_20250814_220436.json"
    ]
    
    # æœ€ç»ˆæµ‹è¯•æ–‡ä»¶ (1422æ ·æœ¬)
    final_files = [
        "arc_easy_complete_dataset_1422samples_20250815_070804.json"
    ]
    
    for filename in original_files + final_files:
        file_path = results_dir / filename
        if file_path.exists():
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                all_results.append({
                    'filename': filename,
                    'data': data,
                    'sample_count': len(data['test_results'])
                })
                logger.info(f"åŠ è½½æ–‡ä»¶: {filename} - {len(data['test_results'])}æ ·æœ¬")
    
    return all_results

def merge_results(all_results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """åˆå¹¶æ‰€æœ‰æµ‹è¯•ç»“æœ"""
    
    if not all_results:
        raise ValueError("æ²¡æœ‰æ‰¾åˆ°æµ‹è¯•ç»“æœæ–‡ä»¶")
    
    # ä½¿ç”¨æœ€åä¸€ä¸ªæ–‡ä»¶ä½œä¸ºåŸºç¡€æ¨¡æ¿
    base_result = all_results[-1]['data'].copy()
    
    # åˆå¹¶æ‰€æœ‰test_results
    merged_test_results = []
    sample_id_map = set()
    
    # å…ˆæ·»åŠ åŸå§‹904æ ·æœ¬çš„ç»“æœ
    for result_file in all_results:
        for test_result in result_file['data']['test_results']:
            sample_id = test_result['sample_data']['id']
            if sample_id not in sample_id_map:
                merged_test_results.append(test_result)
                sample_id_map.add(sample_id)
    
    # æ›´æ–°åˆå¹¶åçš„ç»“æœ
    base_result['test_results'] = merged_test_results
    
    # é‡æ–°è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    total_samples = 5197  # ARC-Easyå®Œæ•´æ•°æ®é›†å¤§å°
    completed_tests = len(merged_test_results)
    
    # è®¡ç®—æˆæœ¬
    total_cost = 0.0
    model_stats = {'openai': {'wins': 0, 'requests': 0, 'tokens': 0, 'cost': 0.0, 'response_times': [], 'error_count': 0},
                   'anthropic': {'wins': 0, 'requests': 0, 'tokens': 0, 'cost': 0.0, 'response_times': [], 'error_count': 0},
                   'google': {'wins': 0, 'requests': 0, 'tokens': 0, 'cost': 0.0, 'response_times': [], 'error_count': 0}}
    ties = 0
    
    for test_result in merged_test_results:
        # è®¡ç®—æˆæœ¬
        model_a_cost = test_result['responses']['model_a']['cost']
        model_b_cost = test_result['responses']['model_b']['cost']
        judge_cost = test_result['evaluation']['judge_cost']
        total_cost += model_a_cost + model_b_cost + judge_cost
        
        # æ›´æ–°æ¨¡å‹ç»Ÿè®¡
        provider_a = test_result['responses']['model_a']['provider']
        provider_b = test_result['responses']['model_b']['provider']
        
        model_stats[provider_a]['requests'] += 1
        model_stats[provider_a]['tokens'] += test_result['responses']['model_a']['tokens']
        model_stats[provider_a]['cost'] += model_a_cost
        model_stats[provider_a]['response_times'].append(test_result['responses']['model_a']['response_time'])
        
        model_stats[provider_b]['requests'] += 1
        model_stats[provider_b]['tokens'] += test_result['responses']['model_b']['tokens']
        model_stats[provider_b]['cost'] += model_b_cost
        model_stats[provider_b]['response_times'].append(test_result['responses']['model_b']['response_time'])
        
        # æ›´æ–°èƒœè´Ÿç»Ÿè®¡
        winner = test_result['evaluation']['winner']
        if winner == 'model_a':
            model_stats[provider_a]['wins'] += 1
        elif winner == 'model_b':
            model_stats[provider_b]['wins'] += 1
        else:
            ties += 1
    
    # è®¡ç®—å¹³å‡å“åº”æ—¶é—´
    for provider, stats in model_stats.items():
        if stats['response_times']:
            stats['avg_response_time'] = sum(stats['response_times']) / len(stats['response_times'])
            del stats['response_times']
    
    # æ›´æ–°æµ‹è¯•ä¿¡æ¯
    base_result['test_info'].update({
        'test_name': 'Complete ARC-Easy Dataset Test - Merged Results',
        'merge_timestamp': datetime.now().isoformat(),
        'total_samples': total_samples,
        'completed_samples': completed_tests,
        'completion_rate': completed_tests / total_samples,
        'files_merged': [r['filename'] for r in all_results],
        'merge_note': 'Merged from original 904 samples + continuation 1422 samples = 2326 total samples'
    })
    
    # æ›´æ–°æ±‡æ€»ç»Ÿè®¡
    base_result['summary'] = {
        'total_time': sum(r['data'].get('summary', {}).get('total_time', 0) for r in all_results),
        'completed_tests': completed_tests,
        'completion_rate': completed_tests / total_samples,
        'total_cost': total_cost,
        'budget_used_percentage': (total_cost / 10.0 * 100),  # å‡è®¾é¢„ç®—æ˜¯$10
        'avg_cost_per_test': total_cost / completed_tests if completed_tests > 0 else 0,
        'model_stats': model_stats,
        'ties': ties
    }
    
    return base_result

def save_merged_results(merged_data: Dict[str, Any]) -> Path:
    """ä¿å­˜åˆå¹¶åçš„ç»“æœ"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    completed = merged_data['summary']['completed_tests']
    filename = f"arc_easy_complete_merged_{completed}samples_{timestamp}.json"
    
    results_dir = Path("logs/complete_arc_easy_results")
    filepath = results_dir / filename
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(merged_data, f, indent=2, ensure_ascii=False)
    
    logger.info(f"åˆå¹¶ç»“æœå·²ä¿å­˜åˆ°: {filepath}")
    return filepath

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”„ å¼€å§‹æ‹¼æ¥ARC-Easyå®Œæ•´æ•°æ®é›†æµ‹è¯•ç»“æœ")
    print("=" * 60)
    
    # åŠ è½½æ‰€æœ‰ç»“æœ
    all_results = load_all_test_results()
    
    if not all_results:
        print("âŒ æœªæ‰¾åˆ°æµ‹è¯•ç»“æœæ–‡ä»¶")
        return
    
    print(f"âœ… æ‰¾åˆ° {len(all_results)} ä¸ªç»“æœæ–‡ä»¶")
    
    # åˆå¹¶ç»“æœ
    merged_data = merge_results(all_results)
    
    # ä¿å­˜åˆå¹¶ç»“æœ
    filepath = save_merged_results(merged_data)
    
    # æ˜¾ç¤ºæ‘˜è¦
    summary = merged_data['summary']
    test_info = merged_data['test_info']
    
    print(f"\nğŸ“Š åˆå¹¶ç»“æœæ‘˜è¦:")
    print(f"  åŸå§‹æ•°æ®é›†å¤§å°: {test_info['total_samples']:,}")
    print(f"  å®Œæˆæµ‹è¯•: {summary['completed_tests']:,}")
    print(f"  å®Œæˆç‡: {summary['completion_rate']:.1%}")
    print(f"  æ€»æˆæœ¬: ${summary['total_cost']:.6f}")
    print(f"  é¢„ç®—ä½¿ç”¨: {summary['budget_used_percentage']:.1f}%")
    print(f"  å¹³å‡æˆæœ¬/æµ‹è¯•: ${summary['avg_cost_per_test']:.6f}")
    print(f"  å¹³å±€æ¬¡æ•°: {summary['ties']:,}")
    
    print(f"\nğŸ† æœ€ç»ˆæ¨¡å‹è¡¨ç°:")
    for model, stats in summary['model_stats'].items():
        if stats['requests'] > 0:
            win_rate = stats['wins'] / stats['requests'] * 100
            print(f"  {model.upper()}:")
            print(f"    å‚ä¸æµ‹è¯•: {stats['requests']:,}æ¬¡")
            print(f"    è·èƒœ: {stats['wins']:,}æ¬¡ ({win_rate:.1f}%)")
            print(f"    Tokenä½¿ç”¨: {stats['tokens']:,}")
            print(f"    æˆæœ¬: ${stats['cost']:.6f}")
            print(f"    é”™è¯¯æ¬¡æ•°: {stats['error_count']:,}")
    
    print(f"\nâœ… æ‹¼æ¥å®Œæˆï¼")
    print(f"ğŸ“ åˆå¹¶ç»“æœä¿å­˜åˆ°: {filepath}")

if __name__ == "__main__":
    main()