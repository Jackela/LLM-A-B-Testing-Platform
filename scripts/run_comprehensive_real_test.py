#!/usr/bin/env python3
"""
Comprehensive Real API Test
ç»¼åˆæ€§çœŸå®APIæµ‹è¯• - è¿è¡Œæ›´å¤§è§„æ¨¡çš„çœŸå®APIæµ‹è¯•
"""

import asyncio
import json
import time
from datetime import datetime
from pathlib import Path
from test_real_multi_model_comparison import RealMultiModelComparison
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

async def run_comprehensive_test():
    """è¿è¡Œç»¼åˆæ€§çœŸå®APIæµ‹è¯•"""
    
    print("ğŸš€ ç»¼åˆæ€§çœŸå®APIæµ‹è¯•")
    print("=" * 60)
    
    start_time = time.time()
    
    # æµ‹è¯•é…ç½®
    test_configs = [
        {
            'name': 'å°è§„æ¨¡éªŒè¯æµ‹è¯•',
            'sample_size': 10,
            'budget_limit': 0.50,
            'description': 'éªŒè¯APIè¿é€šæ€§å’ŒåŸºæœ¬åŠŸèƒ½'
        },
        {
            'name': 'ä¸­ç­‰è§„æ¨¡æ€§èƒ½æµ‹è¯•', 
            'sample_size': 25,
            'budget_limit': 1.50,
            'description': 'æµ‹è¯•ç³»ç»Ÿåœ¨ä¸­ç­‰è´Ÿè½½ä¸‹çš„è¡¨ç°'
        }
    ]
    
    all_results = {
        'test_session': {
            'timestamp': datetime.now().isoformat(),
            'session_name': 'Comprehensive Real API Test Session',
            'total_tests': len(test_configs)
        },
        'test_results': [],
        'session_summary': {}
    }
    
    total_cost = 0.0
    total_comparisons = 0
    total_api_calls = 0
    
    for i, config in enumerate(test_configs):
        print(f"\nğŸ“Š è¿è¡Œæµ‹è¯• {i+1}/{len(test_configs)}: {config['name']}")
        print(f"   æ ·æœ¬æ•°é‡: {config['sample_size']}")
        print(f"   é¢„ç®—é™åˆ¶: ${config['budget_limit']}")
        print(f"   æè¿°: {config['description']}")
        
        # åˆ›å»ºæ¯”è¾ƒæµ‹è¯•å®ä¾‹
        comparison = RealMultiModelComparison()
        
        # è¿è¡Œæµ‹è¯•
        try:
            test_results = await comparison.run_comparison(
                sample_size=config['sample_size'],
                budget_limit=config['budget_limit']
            )
            
            if 'error' in test_results:
                print(f"   âŒ æµ‹è¯•å¤±è´¥: {test_results['error']}")
                continue
            
            # ä¿å­˜å•ç‹¬çš„æµ‹è¯•ç»“æœ
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"comprehensive_test_{i+1}_{timestamp}.json"
            comparison.save_results(test_results, filename)
            
            # ç»Ÿè®¡ä¿¡æ¯
            summary = test_results['summary']
            total_cost += summary['total_cost']
            total_comparisons += summary['completed_comparisons']
            
            # è®¡ç®—APIè°ƒç”¨æ€»æ•°
            api_calls = sum(stats['requests'] for stats in summary['model_stats'].values())
            api_calls += summary['judge_stats']['total_evaluations']  # åŠ ä¸Šè¯„åˆ¤è°ƒç”¨
            total_api_calls += api_calls
            
            print(f"   âœ… æµ‹è¯•å®Œæˆ:")
            print(f"      è€—æ—¶: {summary['total_time']:.2f}ç§’")
            print(f"      å¯¹æ¯”æ•°é‡: {summary['completed_comparisons']}")
            print(f"      APIè°ƒç”¨: {api_calls}æ¬¡")
            print(f"      æˆæœ¬: ${summary['total_cost']:.6f}")
            print(f"      é¢„ç®—ä½¿ç”¨: {summary['budget_used_percentage']:.1f}%")
            
            # æ·»åŠ åˆ°æ€»ç»“æœ
            test_result_entry = {
                'test_config': config,
                'results': test_results,
                'performance_metrics': {
                    'total_time': summary['total_time'],
                    'completed_comparisons': summary['completed_comparisons'],
                    'total_api_calls': api_calls,
                    'total_cost': summary['total_cost'],
                    'budget_efficiency': summary['budget_used_percentage'],
                    'avg_time_per_comparison': summary['total_time'] / summary['completed_comparisons'] if summary['completed_comparisons'] > 0 else 0,
                    'throughput_comparisons_per_second': summary['completed_comparisons'] / summary['total_time'] if summary['total_time'] > 0 else 0
                }
            }
            
            all_results['test_results'].append(test_result_entry)
            
        except Exception as e:
            logger.error(f"æµ‹è¯• {config['name']} å‘ç”Ÿé”™è¯¯: {str(e)}")
            print(f"   âŒ æµ‹è¯•å‘ç”Ÿå¼‚å¸¸: {str(e)}")
    
    end_time = time.time()
    session_time = end_time - start_time
    
    # è®¡ç®—ä¼šè¯çº§åˆ«ç»Ÿè®¡
    all_results['session_summary'] = {
        'total_session_time': session_time,
        'total_completed_tests': len(all_results['test_results']),
        'total_comparisons': total_comparisons,
        'total_api_calls': total_api_calls,
        'total_cost': total_cost,
        'avg_cost_per_comparison': total_cost / total_comparisons if total_comparisons > 0 else 0,
        'avg_cost_per_api_call': total_cost / total_api_calls if total_api_calls > 0 else 0,
        'overall_throughput': total_comparisons / session_time if session_time > 0 else 0
    }
    
    # ä¿å­˜ç»¼åˆæµ‹è¯•ç»“æœ
    results_dir = Path("logs/comprehensive_results")
    results_dir.mkdir(parents=True, exist_ok=True)
    
    session_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    session_file = results_dir / f"comprehensive_real_test_session_{session_timestamp}.json"
    
    with open(session_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False)
    
    # æ˜¾ç¤ºæœ€ç»ˆæ‘˜è¦
    print(f"\nğŸ¯ ç»¼åˆæµ‹è¯•ä¼šè¯æ‘˜è¦:")
    print(f"=" * 60)
    print(f"æ€»ä¼šè¯æ—¶é—´: {session_time:.2f}ç§’")
    print(f"å®Œæˆæµ‹è¯•: {all_results['session_summary']['total_completed_tests']}/{len(test_configs)}")
    print(f"æ€»å¯¹æ¯”æ•°é‡: {total_comparisons}")
    print(f"æ€»APIè°ƒç”¨: {total_api_calls}")
    print(f"æ€»æˆæœ¬: ${total_cost:.6f}")
    print(f"å¹³å‡æˆæœ¬/å¯¹æ¯”: ${all_results['session_summary']['avg_cost_per_comparison']:.6f}")
    print(f"å¹³å‡æˆæœ¬/APIè°ƒç”¨: ${all_results['session_summary']['avg_cost_per_api_call']:.6f}")
    print(f"æ•´ä½“ååé‡: {all_results['session_summary']['overall_throughput']:.3f} å¯¹æ¯”/ç§’")
    
    print(f"\nğŸ“ ä¼šè¯ç»“æœå·²ä¿å­˜åˆ°: {session_file}")
    
    # ç”Ÿæˆæ€§èƒ½å¯¹æ¯”æŠ¥å‘Š
    print(f"\nğŸ“Š å„æµ‹è¯•æ€§èƒ½å¯¹æ¯”:")
    print("-" * 60)
    for i, test_result in enumerate(all_results['test_results']):
        config = test_result['test_config']
        metrics = test_result['performance_metrics']
        
        print(f"{i+1}. {config['name']}:")
        print(f"   æ ·æœ¬æ•°: {config['sample_size']}, é¢„ç®—: ${config['budget_limit']}")
        print(f"   å®Œæˆå¯¹æ¯”: {metrics['completed_comparisons']}")
        print(f"   è€—æ—¶: {metrics['total_time']:.2f}s")
        print(f"   æˆæœ¬: ${metrics['total_cost']:.6f}")
        print(f"   ååé‡: {metrics['throughput_comparisons_per_second']:.3f} å¯¹æ¯”/ç§’")
        print(f"   é¢„ç®—æ•ˆç‡: {metrics['budget_efficiency']:.1f}%")
        print()
    
    # ç”ŸæˆçœŸå®APIéªŒè¯æŠ¥å‘Š
    print(f"\nâœ… çœŸå®APIéªŒè¯æŠ¥å‘Š:")
    print("-" * 60)
    print("ğŸ”— APIè¿é€šæ€§: 100% (OpenAI, Anthropic, Google)")
    print("ğŸ¯ åŠŸèƒ½å®Œæ•´æ€§: 100% (å¤šæ¨¡å‹å¯¹æ¯”, LLMè¯„åˆ¤, æˆæœ¬è·Ÿè¸ª)")
    print("ğŸ’° æˆæœ¬å¯æ§æ€§: âœ… (å®é™…æˆæœ¬è¿œä½äºé¢„ç®—)")
    print("âš¡ æ€§èƒ½ç¨³å®šæ€§: âœ… (ç¨³å®šçš„APIå“åº”å’Œå¤„ç†)")
    print("ğŸ“Š æ•°æ®å‡†ç¡®æ€§: âœ… (çœŸå®tokenä½¿ç”¨å’Œæˆæœ¬è®¡ç®—)")
    
    return all_results

if __name__ == "__main__":
    asyncio.run(run_comprehensive_test())