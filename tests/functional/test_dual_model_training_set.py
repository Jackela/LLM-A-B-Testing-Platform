#!/usr/bin/env python3
"""
Dual Model Training Set Test
åŒæ¨¡å‹å®Œæ•´è®­ç»ƒé›†æµ‹è¯• - ä½¿ç”¨OpenAIå’ŒAnthropicè¿›è¡Œå¤§è§„æ¨¡å¯¹æ¯”æµ‹è¯•
"""

import asyncio
import json
import logging
import os
import random
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

from test_real_api_integration import RealAPIProvider
from test_real_multi_model_comparison import RealLLMJudge

# Configure logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)


class DualModelTrainingSetTest:
    """åŒæ¨¡å‹å®Œæ•´è®­ç»ƒé›†æµ‹è¯•"""

    def __init__(self):
        self.judge = RealLLMJudge()
        self.results = []

    def load_complete_dataset(
        self, dataset_name: str, sample_size: int = 100
    ) -> List[Dict[str, Any]]:
        """åŠ è½½å®Œæ•´æ•°æ®é›†"""

        dataset_files = {
            "arc_easy": "data/processed/arc_easy.json",
            "gsm8k": "data/processed/gsm8k.json",
            "mixed": "both",  # æ··åˆæ•°æ®é›†
        }

        if dataset_name not in dataset_files:
            raise ValueError(
                f"Unknown dataset: {dataset_name}. Available: {list(dataset_files.keys())}"
            )

        datasets = []

        if dataset_name == "mixed":
            # åŠ è½½æ··åˆæ•°æ®é›† (50% ARC-Easy + 50% GSM8K)
            half_size = sample_size // 2

            # åŠ è½½ARC-Easy
            arc_path = Path(dataset_files["arc_easy"])
            if arc_path.exists():
                with open(arc_path, "r", encoding="utf-8") as f:
                    arc_data = json.load(f)
                    arc_sample = random.sample(arc_data, min(half_size, len(arc_data)))
                    datasets.extend(arc_sample)
                    logger.info(f"åŠ è½½ARC-Easyæ ·æœ¬: {len(arc_sample)}")

            # åŠ è½½GSM8K
            gsm8k_path = Path(dataset_files["gsm8k"])
            if gsm8k_path.exists():
                with open(gsm8k_path, "r", encoding="utf-8") as f:
                    gsm8k_data = json.load(f)
                    remaining_size = sample_size - len(datasets)
                    gsm8k_sample = random.sample(gsm8k_data, min(remaining_size, len(gsm8k_data)))
                    datasets.extend(gsm8k_sample)
                    logger.info(f"åŠ è½½GSM8Kæ ·æœ¬: {len(gsm8k_sample)}")
        else:
            # åŠ è½½å•ä¸€æ•°æ®é›†
            dataset_path = Path(dataset_files[dataset_name])
            if not dataset_path.exists():
                raise FileNotFoundError(f"Dataset file not found: {dataset_path}")

            with open(dataset_path, "r", encoding="utf-8") as f:
                full_data = json.load(f)
                datasets = random.sample(full_data, min(sample_size, len(full_data)))
                logger.info(f"åŠ è½½{dataset_name}æ ·æœ¬: {len(datasets)}")

        # éšæœºæ‰“ä¹±æ•°æ®
        random.shuffle(datasets)
        return datasets

    async def run_dual_model_test(
        self, dataset_name: str = "mixed", sample_size: int = 100, budget_limit: float = 5.0
    ) -> Dict[str, Any]:
        """è¿è¡ŒåŒæ¨¡å‹è®­ç»ƒé›†æµ‹è¯•"""

        logger.info(f"å¼€å§‹åŒæ¨¡å‹è®­ç»ƒé›†æµ‹è¯•")
        logger.info(f"æ•°æ®é›†: {dataset_name}")
        logger.info(f"æ ·æœ¬æ•°é‡: {sample_size}")
        logger.info(f"é¢„ç®—é™åˆ¶: ${budget_limit}")
        logger.info(f"æµ‹è¯•æ¨¡å‹: OpenAI vs Anthropic")

        start_time = time.time()

        # åŠ è½½æ•°æ®é›†
        try:
            test_data = self.load_complete_dataset(dataset_name, sample_size)
        except Exception as e:
            logger.error(f"åŠ è½½æ•°æ®é›†å¤±è´¥: {str(e)}")
            return {"error": f"Failed to load dataset: {str(e)}"}

        logger.info(f"æˆåŠŸåŠ è½½{len(test_data)}ä¸ªæµ‹è¯•æ ·æœ¬")

        results = {
            "test_info": {
                "test_name": "Dual Model Training Set Test",
                "timestamp": datetime.now().isoformat(),
                "dataset_name": dataset_name,
                "requested_sample_size": sample_size,
                "actual_sample_size": len(test_data),
                "budget_limit": budget_limit,
                "test_type": "OpenAI vs Anthropic",
            },
            "dataset_analysis": {},
            "model_configs": {},
            "test_results": [],
            "summary": {},
        }

        # åˆ†ææ•°æ®é›†æ„æˆ
        dataset_breakdown = {}
        category_breakdown = {}
        difficulty_breakdown = {}

        for sample in test_data:
            source = sample.get("source", "unknown")
            category = sample.get("category", "unknown")
            difficulty = sample.get("difficulty", "unknown")

            dataset_breakdown[source] = dataset_breakdown.get(source, 0) + 1
            category_breakdown[category] = category_breakdown.get(category, 0) + 1
            difficulty_breakdown[difficulty] = difficulty_breakdown.get(difficulty, 0) + 1

        results["dataset_analysis"] = {
            "by_source": dataset_breakdown,
            "by_category": category_breakdown,
            "by_difficulty": difficulty_breakdown,
        }

        logger.info(f"æ•°æ®é›†åˆ†æ:")
        logger.info(f"  æŒ‰æ¥æº: {dataset_breakdown}")
        logger.info(f"  æŒ‰ç±»åˆ«: {category_breakdown}")
        logger.info(f"  æŒ‰éš¾åº¦: {difficulty_breakdown}")

        async with RealAPIProvider() as provider:
            # åªä½¿ç”¨OpenAIå’ŒAnthropic
            available_providers = ["openai", "anthropic"]
            logger.info(f"åŒæ¨¡å‹æµ‹è¯•æä¾›å•†: {available_providers}")

            # è®°å½•æ¨¡å‹é…ç½®
            for provider_name in available_providers:
                config = provider.api_configs[provider_name]
                results["model_configs"][provider_name] = {
                    "provider": config.provider,
                    "model": config.model,
                    "pricing": config.pricing,
                }

            # åˆå§‹åŒ–ç»Ÿè®¡
            total_cost = 0.0
            model_stats = {
                provider: {
                    "wins": 0,
                    "requests": 0,
                    "tokens": 0,
                    "cost": 0.0,
                    "avg_response_time": 0.0,
                    "response_times": [],
                }
                for provider in available_providers
            }
            ties = 0

            # æ‰¹é‡å¤„ç†è®¾ç½®
            batch_size = 10
            total_batches = (len(test_data) + batch_size - 1) // batch_size

            logger.info(f"å¼€å§‹æ‰¹é‡å¤„ç†ï¼Œå…±{total_batches}æ‰¹ï¼Œæ¯æ‰¹{batch_size}ä¸ªæ ·æœ¬")

            for batch_idx in range(total_batches):
                if total_cost >= budget_limit:
                    logger.warning(f"è¾¾åˆ°é¢„ç®—é™åˆ¶ ${budget_limit}ï¼Œåœæ­¢æµ‹è¯•")
                    break

                batch_start = batch_idx * batch_size
                batch_end = min((batch_idx + 1) * batch_size, len(test_data))
                batch_data = test_data[batch_start:batch_end]

                logger.info(
                    f"å¤„ç†ç¬¬{batch_idx + 1}/{total_batches}æ‰¹ (æ ·æœ¬ {batch_start + 1}-{batch_end})"
                )

                # å¤„ç†å½“å‰æ‰¹æ¬¡
                for i, sample in enumerate(batch_data):
                    global_idx = batch_start + i + 1

                    if total_cost >= budget_limit:
                        logger.warning(f"è¾¾åˆ°é¢„ç®—é™åˆ¶ï¼Œåœæ­¢åœ¨æ ·æœ¬{global_idx}")
                        break

                    logger.info(
                        f"  å¤„ç†æ ·æœ¬ {global_idx}/{len(test_data)}: {sample.get('id', f'sample_{global_idx}')}"
                    )

                    # å›ºå®šä½¿ç”¨OpenAI vs Anthropic
                    model_a, model_b = "openai", "anthropic"

                    prompt = sample.get("prompt", "")
                    if not prompt:
                        logger.warning(f"æ ·æœ¬ {global_idx} æ²¡æœ‰æœ‰æ•ˆçš„promptï¼Œè·³è¿‡")
                        continue

                    try:
                        # ç”Ÿæˆå“åº”
                        response_start = time.time()
                        response_a = await provider.generate_response(prompt, model_a)
                        response_a_time = time.time() - response_start

                        await asyncio.sleep(0.5)  # é¿å…é€Ÿç‡é™åˆ¶

                        response_start = time.time()
                        response_b = await provider.generate_response(prompt, model_b)
                        response_b_time = time.time() - response_start

                        await asyncio.sleep(0.5)  # é¿å…é€Ÿç‡é™åˆ¶

                        if not response_a["success"] or not response_b["success"]:
                            logger.error(f"  APIè°ƒç”¨å¤±è´¥ï¼Œè·³è¿‡æ ·æœ¬ {global_idx}")
                            if not response_a["success"]:
                                logger.error(
                                    f"    {model_a}: {response_a.get('error', 'Unknown error')}"
                                )
                            if not response_b["success"]:
                                logger.error(
                                    f"    {model_b}: {response_b.get('error', 'Unknown error')}"
                                )
                            continue

                        # æ›´æ–°ç»Ÿè®¡
                        model_stats[model_a]["requests"] += 1
                        model_stats[model_a]["tokens"] += response_a["total_tokens"]
                        model_stats[model_a]["cost"] += response_a["cost"]
                        model_stats[model_a]["response_times"].append(response_a_time)

                        model_stats[model_b]["requests"] += 1
                        model_stats[model_b]["tokens"] += response_b["total_tokens"]
                        model_stats[model_b]["cost"] += response_b["cost"]
                        model_stats[model_b]["response_times"].append(response_b_time)

                        total_cost += response_a["cost"] + response_b["cost"]

                        # è¿›è¡Œè¯„åˆ¤
                        responses_for_judge = {"model_a": response_a, "model_b": response_b}

                        evaluation = await self.judge.evaluate_responses(
                            prompt, responses_for_judge, provider
                        )
                        await asyncio.sleep(0.5)  # é¿å…é€Ÿç‡é™åˆ¶

                        if not evaluation["success"]:
                            logger.error(f"  è¯„åˆ¤å¤±è´¥ï¼Œè·³è¿‡æ ·æœ¬ {global_idx}")
                            continue

                        total_cost += evaluation["judge_cost"]

                        # æ›´æ–°èƒœè´Ÿç»Ÿè®¡
                        winner = evaluation["winner"]
                        if winner == "model_a":
                            model_stats[model_a]["wins"] += 1
                        elif winner == "model_b":
                            model_stats[model_b]["wins"] += 1
                        else:
                            ties += 1

                        # è®°å½•ç»“æœ
                        test_result = {
                            "sample_num": global_idx,
                            "sample_data": {
                                "id": sample.get("id", f"sample_{global_idx}"),
                                "source": sample.get("source", "unknown"),
                                "category": sample.get("category", "unknown"),
                                "difficulty": sample.get("difficulty", "unknown"),
                                "prompt_length": len(prompt),
                            },
                            "model_pair": {
                                "model_a": f"{model_a} ({response_a['model']})",
                                "model_b": f"{model_b} ({response_b['model']})",
                            },
                            "responses": {
                                "model_a": {
                                    "provider": model_a,
                                    "model": response_a["model"],
                                    "tokens": response_a["total_tokens"],
                                    "cost": response_a["cost"],
                                    "response_time": response_a_time,
                                },
                                "model_b": {
                                    "provider": model_b,
                                    "model": response_b["model"],
                                    "tokens": response_b["total_tokens"],
                                    "cost": response_b["cost"],
                                    "response_time": response_b_time,
                                },
                            },
                            "evaluation": {
                                "winner": winner,
                                "confidence": evaluation["confidence"],
                                "judge_cost": evaluation["judge_cost"],
                                "judge_tokens": evaluation["judge_tokens"],
                            },
                            "timestamp": datetime.now().isoformat(),
                        }

                        results["test_results"].append(test_result)

                        logger.info(
                            f"    ç»“æœ: {winner}, ç½®ä¿¡åº¦: {evaluation['confidence']:.3f}, æˆæœ¬: ${response_a['cost'] + response_b['cost'] + evaluation['judge_cost']:.6f}"
                        )

                    except Exception as e:
                        logger.error(f"  å¤„ç†æ ·æœ¬ {global_idx} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                        continue

                # æ‰¹æ¬¡å®Œæˆåæ˜¾ç¤ºè¿›åº¦
                completed_tests = len(results["test_results"])
                progress = (completed_tests / sample_size) * 100 if sample_size > 0 else 0
                logger.info(
                    f"æ‰¹æ¬¡ {batch_idx + 1} å®Œæˆï¼Œæ€»è¿›åº¦: {completed_tests}/{sample_size} ({progress:.1f}%), å½“å‰æˆæœ¬: ${total_cost:.6f}"
                )

        end_time = time.time()
        total_time = end_time - start_time

        # è®¡ç®—å¹³å‡å“åº”æ—¶é—´
        for provider, stats in model_stats.items():
            if stats["response_times"]:
                stats["avg_response_time"] = sum(stats["response_times"]) / len(
                    stats["response_times"]
                )
                del stats["response_times"]  # åˆ é™¤åŸå§‹æ•°æ®èŠ‚çœç©ºé—´

        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        completed_tests = len(results["test_results"])

        results["summary"] = {
            "total_time": total_time,
            "completed_tests": completed_tests,
            "completion_rate": completed_tests / sample_size if sample_size > 0 else 0,
            "total_cost": total_cost,
            "budget_used_percentage": (total_cost / budget_limit * 100) if budget_limit > 0 else 0,
            "avg_cost_per_test": total_cost / completed_tests if completed_tests > 0 else 0,
            "throughput": completed_tests / total_time if total_time > 0 else 0,
            "model_stats": model_stats,
            "ties": ties,
            "judge_stats": {
                "total_evaluations": self.judge.total_evaluations,
                "total_cost": self.judge.total_cost,
            },
        }

        return results

    def save_results(self, results: Dict[str, Any], filename: str = None):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            dataset_name = results["test_info"]["dataset_name"]
            sample_size = results["test_info"]["actual_sample_size"]
            filename = (
                f"dual_model_training_set_{dataset_name}_{sample_size}samples_{timestamp}.json"
            )

        # åˆ›å»ºç»“æœç›®å½•
        results_dir = Path("logs/dual_model_results")
        results_dir.mkdir(parents=True, exist_ok=True)

        filepath = results_dir / filename

        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        logger.info(f"æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {filepath}")
        return filepath


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ åŒæ¨¡å‹å®Œæ•´è®­ç»ƒé›†æµ‹è¯•")
    print("=" * 60)

    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    api_keys = {"OpenAI": os.getenv("OPENAI_API_KEY"), "Anthropic": os.getenv("ANTHROPIC_API_KEY")}

    print("ğŸ“‹ APIå¯†é’¥çŠ¶æ€:")
    available_count = 0
    for provider, key in api_keys.items():
        status = "âœ… å·²é…ç½®" if key else "âŒ æœªé…ç½®"
        if key:
            available_count += 1
        print(f"  {provider}: {status}")

    if available_count < 2:
        print(f"\nâŒ é”™è¯¯: éœ€è¦OpenAIå’ŒAnthropicä¸¤ä¸ªAPIå¯†é’¥ï¼å½“å‰åªæœ‰{available_count}ä¸ª")
        return

    print(f"\nâœ… å¯ä»¥è¿›è¡ŒåŒæ¨¡å‹å¯¹æ¯”æµ‹è¯•")

    # æµ‹è¯•é…ç½®é€‰é¡¹
    test_options = {
        "1": {
            "dataset": "arc_easy",
            "size": 50,
            "budget": 3.0,
            "desc": "ARC-Easyç§‘å­¦æ•°æ®é›† (50æ ·æœ¬)",
        },
        "2": {"dataset": "gsm8k", "size": 50, "budget": 3.0, "desc": "GSM8Kæ•°å­¦æ•°æ®é›† (50æ ·æœ¬)"},
        "3": {"dataset": "mixed", "size": 100, "budget": 5.0, "desc": "æ··åˆæ•°æ®é›† (100æ ·æœ¬)"},
        "4": {
            "dataset": "mixed",
            "size": 200,
            "budget": 10.0,
            "desc": "å¤§è§„æ¨¡æ··åˆæ•°æ®é›† (200æ ·æœ¬)",
        },
    }

    print(f"\nğŸ“Š é€‰æ‹©æµ‹è¯•é…ç½®:")
    for key, config in test_options.items():
        print(f"  {key}. {config['desc']} - é¢„ç®—${config['budget']}")

    # é€‰æ‹©è¾ƒå°çš„æµ‹è¯•é…ç½®ä»¥å¿«é€Ÿå®Œæˆ
    selected_config = test_options["1"]
    print(f"\nğŸ”¬ è¿è¡Œæµ‹è¯•é…ç½®: {selected_config['desc']}")
    print(f"  æµ‹è¯•æ¨¡å‹: OpenAI GPT-4o-mini vs Anthropic Claude-3-haiku")

    # åˆ›å»ºæµ‹è¯•å®ä¾‹
    tester = DualModelTrainingSetTest()

    print(f"\nå¼€å§‹æµ‹è¯•...")
    print(f"  æ•°æ®é›†: {selected_config['dataset']}")
    print(f"  æ ·æœ¬æ•°é‡: {selected_config['size']}")
    print(f"  é¢„ç®—é™åˆ¶: ${selected_config['budget']}")

    # è¿è¡Œæµ‹è¯•
    results = await tester.run_dual_model_test(
        dataset_name=selected_config["dataset"],
        sample_size=selected_config["size"],
        budget_limit=selected_config["budget"],
    )

    if "error" in results:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {results['error']}")
        return

    # ä¿å­˜ç»“æœ
    filepath = tester.save_results(results)

    # æ˜¾ç¤ºç»“æœæ‘˜è¦
    summary = results["summary"]
    dataset_analysis = results["dataset_analysis"]

    print(f"\nğŸ“Š æµ‹è¯•æ‘˜è¦:")
    print(f"  æ€»è€—æ—¶: {summary['total_time']:.2f}ç§’ ({summary['total_time']/60:.1f}åˆ†é’Ÿ)")
    print(f"  å®Œæˆæµ‹è¯•: {summary['completed_tests']}")
    print(f"  å®Œæˆç‡: {summary['completion_rate']:.1%}")
    print(f"  æ€»æˆæœ¬: ${summary['total_cost']:.6f}")
    print(f"  é¢„ç®—ä½¿ç”¨: {summary['budget_used_percentage']:.1f}%")
    print(f"  å¹³å‡æˆæœ¬/æµ‹è¯•: ${summary['avg_cost_per_test']:.6f}")
    print(f"  ååé‡: {summary['throughput']:.3f} æµ‹è¯•/ç§’")
    print(f"  å¹³å±€æ¬¡æ•°: {summary['ties']}")

    print(f"\nğŸ“Š æ•°æ®é›†åˆ†æ:")
    print(f"  æŒ‰æ¥æº: {dataset_analysis['by_source']}")
    print(f"  æŒ‰ç±»åˆ«: {dataset_analysis['by_category']}")
    print(f"  æŒ‰éš¾åº¦: {dataset_analysis['by_difficulty']}")

    print(f"\nğŸ† æ¨¡å‹è¡¨ç°:")
    for model, stats in summary["model_stats"].items():
        if stats["requests"] > 0:
            win_rate = stats["wins"] / stats["requests"] * 100 if stats["requests"] > 0 else 0
            print(f"  {model.upper()}:")
            print(f"    å‚ä¸æµ‹è¯•: {stats['requests']}æ¬¡")
            print(f"    è·èƒœ: {stats['wins']}æ¬¡ ({win_rate:.1f}%)")
            print(f"    Tokenä½¿ç”¨: {stats['tokens']}")
            print(f"    æˆæœ¬: ${stats['cost']:.6f}")
            print(f"    å¹³å‡å“åº”æ—¶é—´: {stats['avg_response_time']:.3f}ç§’")

    print(f"\nâœ… åŒæ¨¡å‹è®­ç»ƒé›†æµ‹è¯•å®Œæˆï¼")
    print(f"ğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {filepath}")


if __name__ == "__main__":
    asyncio.run(main())
