#!/usr/bin/env python3
"""
Multi-Model Comparison Test
å¤šæ¨¡å‹æ¨ªå‘å¯¹æ¯”æµ‹è¯• - æ”¯æŒ3-5ä¸ªä¸åŒæ¨¡å‹çš„å…¨é¢å¯¹æ¯”
"""

import asyncio
import itertools
import json
import logging
import math
import random
import time
from collections import defaultdict
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

# Configure logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)


class MultiModelProvider:
    """å¤šæ¨¡å‹æä¾›å•† - æ”¯æŒå¤šç§ä¸åŒçš„æ¨¡å‹é£æ ¼å’Œèƒ½åŠ›"""

    def __init__(self, provider_name: str, model_name: str, model_config: Dict[str, Any]):
        self.provider_name = provider_name
        self.model_name = model_name
        self.model_config = model_config
        self.request_count = 0
        self.total_tokens = 0
        self.total_cost = 0.0

        # æ‰©å±•çš„ä»·æ ¼è¡¨ (æ¯1K tokens, USD)
        self.pricing = {
            "gemini-pro": {"input": 0.0005, "output": 0.0015},
            "gemini-flash": {"input": 0.000075, "output": 0.0003},
            "claude-3.5-sonnet": {"input": 0.003, "output": 0.015},
            "claude-3-haiku": {"input": 0.00025, "output": 0.00125},
            "gpt-4o": {"input": 0.005, "output": 0.015},
            "gpt-4o-mini": {"input": 0.00015, "output": 0.0006},
            "gpt-3.5-turbo": {"input": 0.0005, "output": 0.0015},
            "llama-3.1-70b": {"input": 0.0009, "output": 0.0009},  # ä¼°ç®—ä»·æ ¼
            "mistral-large": {"input": 0.008, "output": 0.024},  # ä¼°ç®—ä»·æ ¼
        }

        # æ¨¡å‹ç‰¹æ€§é…ç½®
        self.model_characteristics = {
            "gemini-pro": {
                "reasoning_style": "systematic",
                "detail_level": "high",
                "creativity": "medium",
                "accuracy_bias": 0.85,
                "response_length_multiplier": 1.4,
            },
            "gemini-flash": {
                "reasoning_style": "efficient",
                "detail_level": "medium",
                "creativity": "high",
                "accuracy_bias": 0.80,
                "response_length_multiplier": 1.1,
            },
            "claude-3.5-sonnet": {
                "reasoning_style": "analytical",
                "detail_level": "very_high",
                "creativity": "high",
                "accuracy_bias": 0.90,
                "response_length_multiplier": 1.6,
            },
            "claude-3-haiku": {
                "reasoning_style": "concise",
                "detail_level": "medium",
                "creativity": "medium",
                "accuracy_bias": 0.85,
                "response_length_multiplier": 0.8,
            },
            "gpt-4o": {
                "reasoning_style": "comprehensive",
                "detail_level": "very_high",
                "creativity": "very_high",
                "accuracy_bias": 0.92,
                "response_length_multiplier": 1.5,
            },
            "gpt-4o-mini": {
                "reasoning_style": "balanced",
                "detail_level": "medium",
                "creativity": "medium",
                "accuracy_bias": 0.82,
                "response_length_multiplier": 1.0,
            },
            "gpt-3.5-turbo": {
                "reasoning_style": "conversational",
                "detail_level": "medium",
                "creativity": "medium",
                "accuracy_bias": 0.78,
                "response_length_multiplier": 1.2,
            },
            "llama-3.1-70b": {
                "reasoning_style": "structured",
                "detail_level": "high",
                "creativity": "medium",
                "accuracy_bias": 0.83,
                "response_length_multiplier": 1.3,
            },
            "mistral-large": {
                "reasoning_style": "precise",
                "detail_level": "high",
                "creativity": "medium",
                "accuracy_bias": 0.87,
                "response_length_multiplier": 1.2,
            },
        }

    async def generate_response(
        self, prompt: str, context: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """ç”Ÿæˆæ¨¡å‹ç‰¹å¼‚æ€§å“åº”"""
        # åŸºäºæ¨¡å‹ç‰¹æ€§è°ƒæ•´å»¶è¿Ÿ
        char = self.model_characteristics.get(self.model_name, {})
        base_delay = char.get("response_length_multiplier", 1.0) * 0.3
        await asyncio.sleep(base_delay + random.uniform(0.1, 0.3))

        # ç”Ÿæˆå“åº”å†…å®¹
        content = self._generate_model_specific_response(prompt, context, char)

        # è®¡ç®—tokenä½¿ç”¨é‡
        input_tokens = self._calculate_tokens(prompt)
        output_tokens = self._calculate_tokens(content)

        # è®¡ç®—æˆæœ¬
        cost = self._calculate_cost(input_tokens, output_tokens)

        # æ›´æ–°ç»Ÿè®¡
        self.request_count += 1
        self.total_tokens += input_tokens + output_tokens
        self.total_cost += cost

        return {
            "content": content,
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "cost": cost,
            "model_name": self.model_name,
            "provider": self.provider_name,
            "characteristics": char,
        }

    def _generate_model_specific_response(
        self, prompt: str, context: Dict[str, Any], char: Dict[str, Any]
    ) -> str:
        """åŸºäºæ¨¡å‹ç‰¹æ€§ç”Ÿæˆç‰¹å¼‚æ€§å“åº”"""
        if not context:
            return self._generate_generic_response(prompt, char)

        source = context.get("source", "")
        category = context.get("category", "")

        if source == "ARC-Easy" or category == "science":
            return self._generate_science_response(prompt, context, char)
        elif source == "GSM8K" or category == "math":
            return self._generate_math_response(prompt, context, char)
        else:
            return self._generate_generic_response(prompt, char)

    def _generate_science_response(
        self, prompt: str, context: Dict[str, Any], char: Dict[str, Any]
    ) -> str:
        """ç”Ÿæˆç§‘å­¦é—®é¢˜å“åº”"""
        expected = context.get("expected_output", "scientific conclusion")
        reasoning_style = char.get("reasoning_style", "systematic")
        detail_level = char.get("detail_level", "medium")
        accuracy_bias = char.get("accuracy_bias", 0.8)

        # åŸºäºå‡†ç¡®æ€§åå·®å†³å®šæ˜¯å¦ç»™å‡ºæ­£ç¡®ç­”æ¡ˆ
        is_accurate = random.random() < accuracy_bias
        answer = expected if is_accurate else self._generate_plausible_wrong_answer(expected)

        if reasoning_style == "systematic":
            if detail_level == "very_high":
                return f"""Let me approach this scientific question systematically.

First, I'll analyze the fundamental principles involved: {answer}

This conclusion is based on established scientific theories and can be verified through empirical evidence. The underlying mechanisms involve well-documented scientific processes."""
            elif detail_level == "high":
                return f"Analyzing this systematically, the scientific principle leads us to: {answer}. This follows from fundamental scientific knowledge."
            else:
                return f"Scientific analysis shows: {answer}"

        elif reasoning_style == "analytical":
            if detail_level == "very_high":
                return f"""From an analytical perspective, this question requires careful examination of the scientific concepts involved.

The correct answer is: {answer}

This conclusion follows from a thorough analysis of the underlying scientific principles and their practical applications in this context."""
            else:
                return f"Analytical assessment: {answer}. This conclusion is supported by scientific reasoning."

        elif reasoning_style == "efficient":
            return f"Based on scientific principles: {answer}."

        elif reasoning_style == "concise":
            return f"{answer}"

        elif reasoning_style == "comprehensive":
            return f"""This scientific question requires a comprehensive understanding of multiple factors.

Answer: {answer}

Explanation: This conclusion integrates various scientific principles and demonstrates how they interact in this specific context. The scientific basis is well-established."""

        elif reasoning_style == "conversational":
            return f"Looking at this science question, I'd say the answer is {answer}. This makes sense when you consider the basic scientific principles at work here."

        elif reasoning_style == "structured":
            return f"""1. Question analysis: Scientific reasoning problem
2. Relevant principles: Established scientific knowledge
3. Conclusion: {answer}
4. Verification: Consistent with scientific theory"""

        elif reasoning_style == "precise":
            return f"Scientific determination: {answer}. Basis: established principles."

        else:  # balanced/default
            return f"Based on scientific principles, the answer is {answer}. This follows from established knowledge in the field."

    def _generate_math_response(
        self, prompt: str, context: Dict[str, Any], char: Dict[str, Any]
    ) -> str:
        """ç”Ÿæˆæ•°å­¦é—®é¢˜å“åº”"""
        answer = context.get("ground_truth", "calculated result")
        reasoning_style = char.get("reasoning_style", "systematic")
        detail_level = char.get("detail_level", "medium")
        accuracy_bias = char.get("accuracy_bias", 0.8)

        # åŸºäºå‡†ç¡®æ€§åå·®å†³å®šæ˜¯å¦ç»™å‡ºæ­£ç¡®ç­”æ¡ˆ
        is_accurate = random.random() < accuracy_bias
        final_answer = answer if is_accurate else self._generate_wrong_math_answer(answer)

        if reasoning_style == "systematic":
            if detail_level == "very_high":
                return f"""I'll solve this step-by-step using a systematic approach:

Step 1: Identify the given information and what we need to find
Step 2: Determine the appropriate mathematical method
Step 3: Set up the equation or calculation
Step 4: Perform the computation carefully
Step 5: Verify the result

Final answer: {final_answer}

This solution follows standard mathematical procedures and can be verified through alternative methods."""
            elif detail_level == "high":
                return f"Solving systematically: First, I'll identify the key information. Then I'll apply the appropriate mathematical operations. The answer is {final_answer}."
            else:
                return f"Step-by-step solution yields: {final_answer}"

        elif reasoning_style == "analytical":
            return f"Mathematical analysis: By breaking down this problem and applying appropriate mathematical reasoning, I determine that {final_answer}."

        elif reasoning_style == "efficient":
            return f"Quick calculation: {final_answer}"

        elif reasoning_style == "concise":
            return f"{final_answer}"

        elif reasoning_style == "comprehensive":
            return f"""This mathematical problem requires comprehensive analysis of the given information.

Solution approach: I need to carefully examine the problem structure and apply appropriate mathematical concepts.

Final answer: {final_answer}

This result represents the accurate mathematical solution based on the given parameters."""

        elif reasoning_style == "conversational":
            return (
                f"Let me work through this math problem. When I calculate it, I get {final_answer}."
            )

        elif reasoning_style == "structured":
            return f"""Problem type: Mathematical calculation
Given data: [extracted from problem]
Method: Standard mathematical operations
Result: {final_answer}"""

        elif reasoning_style == "precise":
            return f"Mathematical result: {final_answer}"

        else:  # balanced/default
            return f"Solving this mathematically: {final_answer}"

    def _generate_generic_response(self, prompt: str, char: Dict[str, Any]) -> str:
        """ç”Ÿæˆé€šç”¨å“åº”"""
        reasoning_style = char.get("reasoning_style", "systematic")
        detail_level = char.get("detail_level", "medium")

        if detail_level == "very_high":
            return f"This question requires comprehensive analysis considering multiple perspectives and factors. I'll provide a detailed examination to ensure thorough understanding of all relevant aspects."
        elif detail_level == "high":
            return f"This requires careful analysis. Let me provide a detailed response addressing the key aspects of your question."
        else:
            return f"Here's my response addressing your question with appropriate consideration of the relevant factors."

    def _generate_plausible_wrong_answer(self, correct_answer: str) -> str:
        """ç”Ÿæˆçœ‹ä¼¼åˆç†çš„é”™è¯¯ç­”æ¡ˆ"""
        # ç®€åŒ–çš„é”™è¯¯ç­”æ¡ˆç”Ÿæˆ
        alternatives = [
            "absorbed light from atmospheric particles",
            "internal luminescence from moon's core",
            "electromagnetic radiation from Earth",
            "thermal emission from surface heating",
        ]
        return (
            random.choice(alternatives) if correct_answer else "alternative scientific explanation"
        )

    def _generate_wrong_math_answer(self, correct_answer: str) -> str:
        """ç”Ÿæˆé”™è¯¯çš„æ•°å­¦ç­”æ¡ˆ"""
        try:
            if correct_answer.isdigit():
                num = int(correct_answer)
                # ç”Ÿæˆæ¥è¿‘ä½†é”™è¯¯çš„ç­”æ¡ˆ
                error_factor = random.choice([0.9, 1.1, 0.8, 1.2])
                wrong_num = int(num * error_factor)
                return (
                    str(wrong_num) if wrong_num != num else str(num + random.choice([-1, 1, -2, 2]))
                )
            else:
                return str(random.randint(10, 100))
        except:
            return "incorrect calculation"

    def _calculate_tokens(self, text: str) -> int:
        """è®¡ç®—tokenæ•°é‡"""
        word_count = len(text.split())
        char_count = len(text)

        # åŸºäºæ¨¡å‹ç‰¹æ€§è°ƒæ•´tokenè®¡ç®—
        char = self.model_characteristics.get(self.model_name, {})
        multiplier = char.get("response_length_multiplier", 1.0)

        token_estimate = max(
            int(word_count * 1.3 * multiplier), int(char_count * 0.25 * multiplier)
        )

        return max(1, token_estimate)

    def _calculate_cost(self, input_tokens: int, output_tokens: int) -> float:
        """è®¡ç®—ç²¾ç¡®æˆæœ¬"""
        pricing = self.pricing.get(self.model_name, self.pricing["gpt-3.5-turbo"])

        input_cost = (input_tokens / 1000) * pricing["input"]
        output_cost = (output_tokens / 1000) * pricing["output"]

        return input_cost + output_cost


class TournamentJudge:
    """é”¦æ ‡èµ›å¼è¯„åˆ¤è€… - æ”¯æŒå¤šæ¨¡å‹ä¸¤ä¸¤å¯¹æ¯”"""

    def __init__(self):
        self.evaluation_count = 0
        self.pairwise_results = defaultdict(lambda: defaultdict(int))

    async def evaluate_pairwise(
        self,
        question: str,
        response_a: Dict[str, Any],
        response_b: Dict[str, Any],
        model_a: str,
        model_b: str,
        context: Dict[str, Any] = None,
    ) -> Dict[str, Any]:
        """æ‰§è¡Œä¸¤ä¸¤å¯¹æ¯”è¯„ä¼°"""

        # æ¨¡æ‹Ÿè¯„ä¼°å»¶è¿Ÿ
        await asyncio.sleep(random.uniform(0.3, 0.7))

        # é«˜çº§è¯„ä¼°é€»è¾‘
        evaluation = self._advanced_pairwise_evaluation(
            question,
            response_a["content"],
            response_b["content"],
            response_a["characteristics"],
            response_b["characteristics"],
            context,
        )

        # è®°å½•å¯¹æ¯”ç»“æœ
        if evaluation["winner"] == "A":
            self.pairwise_results[model_a][model_b] += 1
        elif evaluation["winner"] == "B":
            self.pairwise_results[model_b][model_a] += 1
        # å¹³å±€ä¸è®¡å…¥èƒœè´Ÿå…³ç³»

        self.evaluation_count += 1
        return evaluation

    def _advanced_pairwise_evaluation(
        self,
        question: str,
        response_a: str,
        response_b: str,
        char_a: Dict[str, Any],
        char_b: Dict[str, Any],
        context: Dict[str, Any],
    ) -> Dict[str, Any]:
        """é«˜çº§ä¸¤ä¸¤å¯¹æ¯”è¯„ä¼°"""

        # å¤šç»´åº¦è¯„åˆ†
        scores_a = self._calculate_comprehensive_scores(response_a, char_a, context)
        scores_b = self._calculate_comprehensive_scores(response_b, char_b, context)

        # åŠ æƒè¯„åˆ†
        weights = {"accuracy": 0.4, "clarity": 0.2, "completeness": 0.2, "reasoning_quality": 0.2}

        weighted_score_a = sum(scores_a.get(dim, 7.0) * weight for dim, weight in weights.items())
        weighted_score_b = sum(scores_b.get(dim, 7.0) * weight for dim, weight in weights.items())

        # å¼•å…¥ä¸€äº›éšæœºæ€§ä»¥æ¨¡æ‹ŸçœŸå®è¯„ä¼°çš„ä¸ç¡®å®šæ€§
        noise_a = random.uniform(-0.2, 0.2)
        noise_b = random.uniform(-0.2, 0.2)

        final_score_a = weighted_score_a + noise_a
        final_score_b = weighted_score_b + noise_b

        # ç¡®å®šè·èƒœè€…
        score_diff = abs(final_score_a - final_score_b)

        if final_score_a > final_score_b + 0.3:
            winner = "A"
            confidence = min(0.95, 0.6 + score_diff / 8)
        elif final_score_b > final_score_a + 0.3:
            winner = "B"
            confidence = min(0.95, 0.6 + score_diff / 8)
        else:
            winner = "Tie"
            confidence = 0.5 + random.uniform(-0.1, 0.1)

        # ç”Ÿæˆè¯„ä¼°ç†ç”±
        reasoning = self._generate_comparison_reasoning(scores_a, scores_b, char_a, char_b, winner)

        return {
            "winner": winner,
            "confidence": round(confidence, 3),
            "reasoning": reasoning,
            "scores": {
                "response_a": {k: round(v, 2) for k, v in scores_a.items()},
                "response_b": {k: round(v, 2) for k, v in scores_b.items()},
            },
            "weighted_scores": {
                "response_a": round(final_score_a, 2),
                "response_b": round(final_score_b, 2),
            },
        }

    def _calculate_comprehensive_scores(
        self, response: str, characteristics: Dict[str, Any], context: Dict[str, Any]
    ) -> Dict[str, float]:
        """è®¡ç®—ç»¼åˆè¯„åˆ†"""
        scores = {}

        # åŸºç¡€è¯„åˆ†
        base_score = 7.0
        response_length = len(response)

        # å‡†ç¡®æ€§è¯„åˆ† - åŸºäºæ¨¡å‹çš„å‡†ç¡®æ€§åå·®
        accuracy_bias = characteristics.get("accuracy_bias", 0.8)
        accuracy_score = base_score + (accuracy_bias - 0.8) * 5  # è½¬æ¢ä¸ºè¯„åˆ†

        # æ£€æŸ¥æ˜¯å¦åŒ…å«é¢„æœŸç­”æ¡ˆ
        if context:
            expected = str(context.get("expected_output", "")).lower()
            ground_truth = str(context.get("ground_truth", "")).lower()

            if expected and expected in response.lower():
                accuracy_score += 1.5
            elif ground_truth and ground_truth in response.lower():
                accuracy_score += 1.0

        scores["accuracy"] = min(10.0, max(5.0, accuracy_score))

        # æ¸…æ™°åº¦è¯„åˆ† - åŸºäºæ¨ç†é£æ ¼å’Œç»†èŠ‚çº§åˆ«
        clarity_score = base_score
        detail_level = characteristics.get("detail_level", "medium")

        if detail_level == "very_high":
            clarity_score += 1.5
        elif detail_level == "high":
            clarity_score += 1.0
        elif detail_level == "medium":
            clarity_score += 0.5

        # åŸºäºå“åº”ç»“æ„æ€§
        structure_indicators = ["step", "first", "analysis", "conclusion", "therefore"]
        structure_count = sum(
            1 for indicator in structure_indicators if indicator in response.lower()
        )
        clarity_score += min(1.0, structure_count * 0.2)

        scores["clarity"] = min(10.0, max(5.0, clarity_score))

        # å®Œæ•´æ€§è¯„åˆ† - åŸºäºå“åº”é•¿åº¦å’Œç»†èŠ‚çº§åˆ«
        completeness_score = base_score
        length_multiplier = characteristics.get("response_length_multiplier", 1.0)

        expected_length = 100 * length_multiplier
        if response_length >= expected_length:
            completeness_score += 1.5
        elif response_length >= expected_length * 0.7:
            completeness_score += 1.0
        elif response_length >= expected_length * 0.5:
            completeness_score += 0.5

        scores["completeness"] = min(10.0, max(5.0, completeness_score))

        # æ¨ç†è´¨é‡è¯„åˆ†
        reasoning_quality_score = base_score
        reasoning_style = characteristics.get("reasoning_style", "systematic")

        style_bonus = {
            "systematic": 1.5,
            "analytical": 1.3,
            "comprehensive": 1.4,
            "structured": 1.2,
            "precise": 1.1,
            "efficient": 0.8,
            "conversational": 0.7,
            "concise": 0.6,
            "balanced": 1.0,
        }

        reasoning_quality_score += style_bonus.get(reasoning_style, 1.0)
        scores["reasoning_quality"] = min(10.0, max(5.0, reasoning_quality_score))

        return scores

    def _generate_comparison_reasoning(
        self,
        scores_a: Dict[str, float],
        scores_b: Dict[str, float],
        char_a: Dict[str, Any],
        char_b: Dict[str, Any],
        winner: str,
    ) -> str:
        """ç”Ÿæˆå¯¹æ¯”è¯„ä¼°ç†ç”±"""
        if winner == "Tie":
            return "ä¸¤ä¸ªå“åº”åœ¨å„è¯„ä¼°ç»´åº¦ä¸Šè¡¨ç°ç›¸è¿‘ï¼Œéƒ½æœ‰å„è‡ªçš„ä¼˜åŠ¿ã€‚"

        # æ‰¾å‡ºä¸»è¦å·®å¼‚ç»´åº¦
        max_diff = 0
        max_diff_dim = "overall"

        for dim in scores_a:
            if dim in scores_b:
                diff = abs(scores_a[dim] - scores_b[dim])
                if diff > max_diff:
                    max_diff = diff
                    max_diff_dim = dim

        # è·å–è·èƒœæ¨¡å‹çš„ç‰¹å¾
        if winner == "A":
            winner_char = char_a
            winner_scores = scores_a
            loser_scores = scores_b
        else:
            winner_char = char_b
            winner_scores = scores_b
            loser_scores = scores_a

        reasoning_style = winner_char.get("reasoning_style", "systematic")
        detail_level = winner_char.get("detail_level", "medium")

        dim_names = {
            "accuracy": "å‡†ç¡®æ€§",
            "clarity": "æ¸…æ™°åº¦",
            "completeness": "å®Œæ•´æ€§",
            "reasoning_quality": "æ¨ç†è´¨é‡",
        }

        dim_cn = dim_names.get(max_diff_dim, "ç»¼åˆè¯„ä¼°")
        winner_score = winner_scores.get(max_diff_dim, 7.0)
        loser_score = loser_scores.get(max_diff_dim, 7.0)

        style_descriptions = {
            "systematic": "ç³»ç»Ÿæ€§æ–¹æ³•",
            "analytical": "åˆ†ææ€§æ€ç»´",
            "comprehensive": "å…¨é¢æ€§åˆ†æ",
            "structured": "ç»“æ„åŒ–è¡¨è¾¾",
            "precise": "ç²¾ç¡®æ€§",
            "efficient": "é«˜æ•ˆæ€§",
            "conversational": "å¯¹è¯æ€§",
            "concise": "ç®€æ´æ€§",
        }

        style_desc = style_descriptions.get(reasoning_style, "å¹³è¡¡æ€§")

        return f"è·èƒœå“åº”åœ¨{dim_cn}æ–¹é¢è¡¨ç°çªå‡º ({winner_score:.1f} vs {loser_score:.1f})ï¼Œä½“ç°äº†{style_desc}å’Œ{detail_level}è¯¦ç»†ç¨‹åº¦çš„ä¼˜åŠ¿ã€‚"

    def get_tournament_rankings(self, models: List[str]) -> List[Tuple[str, int]]:
        """è·å–é”¦æ ‡èµ›æ’å"""
        scores = defaultdict(int)

        # è®¡ç®—æ¯ä¸ªæ¨¡å‹çš„æ€»èƒœåœºæ•°
        for model_a in models:
            for model_b in models:
                if model_a != model_b:
                    scores[model_a] += self.pairwise_results[model_a][model_b]

        # æŒ‰èƒœåœºæ•°æ’åº
        return sorted(scores.items(), key=lambda x: x[1], reverse=True)


class MultiModelTestRunner:
    """å¤šæ¨¡å‹æµ‹è¯•è¿è¡Œå™¨"""

    def __init__(self, max_cost_usd: float = 3.0):
        # åˆå§‹åŒ–å¤šä¸ªä¸åŒçš„æ¨¡å‹
        self.models = {
            "gemini-pro": MultiModelProvider("Google", "gemini-pro", {"tier": "premium"}),
            "gemini-flash": MultiModelProvider("Google", "gemini-flash", {"tier": "fast"}),
            "claude-3.5-sonnet": MultiModelProvider(
                "Anthropic", "claude-3.5-sonnet", {"tier": "premium"}
            ),
            "claude-3-haiku": MultiModelProvider("Anthropic", "claude-3-haiku", {"tier": "fast"}),
            "gpt-4o": MultiModelProvider("OpenAI", "gpt-4o", {"tier": "premium"}),
            "gpt-4o-mini": MultiModelProvider("OpenAI", "gpt-4o-mini", {"tier": "fast"}),
            "gpt-3.5-turbo": MultiModelProvider("OpenAI", "gpt-3.5-turbo", {"tier": "standard"}),
        }

        self.judge = TournamentJudge()
        self.max_cost_usd = max_cost_usd
        self.test_results = []
        self.current_cost = 0.0

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_comparisons": 0,
            "model_performance": defaultdict(lambda: {"wins": 0, "losses": 0, "ties": 0}),
            "pairwise_matrix": defaultdict(lambda: defaultdict(int)),
            "cost_breakdown": defaultdict(float),
            "timing_stats": [],
        }

    def select_models_for_test(
        self, model_names: List[str] = None
    ) -> Dict[str, MultiModelProvider]:
        """é€‰æ‹©å‚ä¸æµ‹è¯•çš„æ¨¡å‹"""
        if model_names:
            selected = {name: self.models[name] for name in model_names if name in self.models}
        else:
            # é»˜è®¤é€‰æ‹©å¤šæ ·åŒ–çš„5ä¸ªæ¨¡å‹
            default_selection = [
                "gemini-flash",  # å¿«é€Ÿä¸”ä¾¿å®œ
                "claude-3.5-sonnet",  # é«˜è´¨é‡
                "gpt-4o-mini",  # å¹³è¡¡æ€§ä»·æ¯”
                "gpt-3.5-turbo",  # åŸºå‡†æ¨¡å‹
                "claude-3-haiku",  # å¿«é€Ÿé«˜è´¨é‡
            ]
            selected = {name: self.models[name] for name in default_selection}

        logger.info(f"ğŸ¤– é€‰ä¸­çš„æ¨¡å‹: {', '.join(selected.keys())}")
        return selected

    def load_test_samples(self, sample_count: int = 300) -> List[Dict[str, Any]]:
        """åŠ è½½æµ‹è¯•æ ·æœ¬"""
        datasets_dir = Path("data/processed")
        samples = []

        # å¹³è¡¡åŠ è½½ä¸åŒç±»å‹çš„æ•°æ®
        target_per_dataset = sample_count // 2

        # ARC-Easy
        arc_file = datasets_dir / "arc_easy.json"
        if arc_file.exists():
            with open(arc_file, "r", encoding="utf-8") as f:
                arc_data = json.load(f)
            samples.extend(random.sample(arc_data, min(target_per_dataset, len(arc_data))))

        # GSM8K
        gsm8k_file = datasets_dir / "gsm8k.json"
        if gsm8k_file.exists():
            with open(gsm8k_file, "r", encoding="utf-8") as f:
                gsm8k_data = json.load(f)
            samples.extend(random.sample(gsm8k_data, min(target_per_dataset, len(gsm8k_data))))

        # éšæœºæ‰“ä¹±å¹¶é™åˆ¶æ•°é‡
        random.shuffle(samples)
        return samples[:sample_count]

    async def run_multi_model_comparison(
        self, sample_count: int = 300, selected_models: List[str] = None
    ) -> Dict[str, Any]:
        """è¿è¡Œå¤šæ¨¡å‹å¯¹æ¯”æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹å¤šæ¨¡å‹æ¨ªå‘å¯¹æ¯”æµ‹è¯•")
        logger.info("=" * 80)

        # é€‰æ‹©å‚ä¸æµ‹è¯•çš„æ¨¡å‹
        test_models = self.select_models_for_test(selected_models)
        model_names = list(test_models.keys())

        # è®¡ç®—æ‰€æœ‰å¯èƒ½çš„æ¨¡å‹å¯¹æ¯”ç»„åˆ
        model_pairs = list(itertools.combinations(model_names, 2))
        total_comparisons = len(model_pairs) * sample_count

        logger.info(f"ğŸ“Š æµ‹è¯•é…ç½®:")
        logger.info(f"  å‚ä¸æ¨¡å‹: {len(test_models)}ä¸ª")
        logger.info(f"  å¯¹æ¯”ç»„åˆ: {len(model_pairs)}å¯¹")
        logger.info(f"  æµ‹è¯•æ ·æœ¬: {sample_count}ä¸ª")
        logger.info(f"  æ€»å¯¹æ¯”æ¬¡æ•°: {total_comparisons:,}")

        # æˆæœ¬é¢„ä¼°
        estimated_cost = self._estimate_total_cost(test_models, total_comparisons)
        logger.info(f"  é¢„ä¼°æˆæœ¬: ${estimated_cost:.4f}")

        if estimated_cost > self.max_cost_usd:
            logger.warning(f"âš ï¸ é¢„ä¼°æˆæœ¬è¶…å‡ºé¢„ç®—ï¼Œè¯·è€ƒè™‘å‡å°‘æ¨¡å‹æ•°é‡æˆ–æ ·æœ¬æ•°")
            return {}

        # åŠ è½½æµ‹è¯•æ•°æ®
        test_samples = self.load_test_samples(sample_count)
        logger.info(f"ğŸ“š å·²åŠ è½½ {len(test_samples)} ä¸ªæµ‹è¯•æ ·æœ¬")

        # å¼€å§‹æµ‹è¯•
        start_time = time.time()

        # ä¸ºæ¯ä¸ªæ ·æœ¬è¿è¡Œæ‰€æœ‰æ¨¡å‹å¯¹æ¯”
        for sample_idx, sample in enumerate(test_samples, 1):
            logger.info(f"\nğŸ“ å¤„ç†æ ·æœ¬ {sample_idx}/{len(test_samples)}")

            # ç”Ÿæˆæ‰€æœ‰æ¨¡å‹çš„å“åº”
            responses = {}
            for model_name, model in test_models.items():
                response = await model.generate_response(sample["prompt"], sample)
                responses[model_name] = response

            # è¿›è¡Œæ‰€æœ‰ä¸¤ä¸¤å¯¹æ¯”
            for model_a, model_b in model_pairs:
                evaluation = await self.judge.evaluate_pairwise(
                    sample["prompt"],
                    responses[model_a],
                    responses[model_b],
                    model_a,
                    model_b,
                    sample,
                )

                # è®°å½•ç»“æœ
                self._record_comparison_result(model_a, model_b, evaluation, sample_idx)

            # æ›´æ–°æˆæœ¬ç»Ÿè®¡
            self._update_cost_tracking(test_models)

            # è¿›åº¦æŠ¥å‘Š
            if sample_idx % 50 == 0 or sample_idx <= 5:
                progress = (sample_idx / len(test_samples)) * 100
                logger.info(
                    f"ğŸ“ˆ è¿›åº¦: {sample_idx}/{len(test_samples)} ({progress:.1f}%) | å½“å‰æˆæœ¬: ${self.current_cost:.4f}"
                )

        total_time = time.time() - start_time

        # ç”Ÿæˆç»¼åˆåˆ†æ
        summary = self._generate_multi_model_summary(
            test_models, model_pairs, total_time, sample_count
        )

        # ä¿å­˜ç»“æœ
        self._save_multi_model_results(summary)

        return summary

    def _estimate_total_cost(
        self, models: Dict[str, MultiModelProvider], total_comparisons: int
    ) -> float:
        """ä¼°ç®—æ€»æˆæœ¬"""
        avg_cost_per_response = sum(
            (100 / 1000) * model.pricing[model.model_name]["input"]
            + (80 / 1000) * model.pricing[model.model_name]["output"]
            for model in models.values()
        ) / len(models)

        # æ¨¡å‹å“åº”æˆæœ¬ + è¯„ä¼°æˆæœ¬
        response_cost = (
            avg_cost_per_response
            * len(models)
            * (total_comparisons // len(list(itertools.combinations(models.keys(), 2))))
        )
        evaluation_cost = 0.000065 * total_comparisons  # è¯„ä¼°æˆæœ¬

        return response_cost + evaluation_cost

    def _record_comparison_result(
        self, model_a: str, model_b: str, evaluation: Dict[str, Any], sample_idx: int
    ):
        """è®°å½•å¯¹æ¯”ç»“æœ"""
        winner = evaluation["winner"]

        if winner == "A":
            self.stats["model_performance"][model_a]["wins"] += 1
            self.stats["model_performance"][model_b]["losses"] += 1
            self.stats["pairwise_matrix"][model_a][model_b] += 1
        elif winner == "B":
            self.stats["model_performance"][model_b]["wins"] += 1
            self.stats["model_performance"][model_a]["losses"] += 1
            self.stats["pairwise_matrix"][model_b][model_a] += 1
        else:  # Tie
            self.stats["model_performance"][model_a]["ties"] += 1
            self.stats["model_performance"][model_b]["ties"] += 1

        self.stats["total_comparisons"] += 1

        # ä¿å­˜è¯¦ç»†ç»“æœ
        self.test_results.append(
            {
                "sample_idx": sample_idx,
                "model_a": model_a,
                "model_b": model_b,
                "evaluation": evaluation,
            }
        )

    def _update_cost_tracking(self, models: Dict[str, MultiModelProvider]):
        """æ›´æ–°æˆæœ¬è·Ÿè¸ª"""
        total_cost = 0.0
        for model_name, model in models.items():
            model_cost = model.total_cost
            self.stats["cost_breakdown"][model_name] = model_cost
            total_cost += model_cost

        self.current_cost = total_cost

    def _generate_multi_model_summary(
        self,
        models: Dict[str, MultiModelProvider],
        model_pairs: List[Tuple[str, str]],
        total_time: float,
        sample_count: int,
    ) -> Dict[str, Any]:
        """ç”Ÿæˆå¤šæ¨¡å‹å¯¹æ¯”æ±‡æ€»"""

        # è®¡ç®—æ’å
        model_names = list(models.keys())
        rankings = self.judge.get_tournament_rankings(model_names)

        # è®¡ç®—èƒœç‡ç»Ÿè®¡
        win_rates = {}
        for model_name in model_names:
            stats = self.stats["model_performance"][model_name]
            total_games = stats["wins"] + stats["losses"] + stats["ties"]
            win_rate = stats["wins"] / total_games if total_games > 0 else 0
            win_rates[model_name] = {
                "win_rate": round(win_rate, 3),
                "wins": stats["wins"],
                "losses": stats["losses"],
                "ties": stats["ties"],
                "total_games": total_games,
            }

        # æ¨¡å‹ç‰¹æ€§åˆ†æ
        model_analysis = {}
        for model_name, model in models.items():
            char = model.model_characteristics[model.model_name]
            cost_per_response = (
                model.total_cost / model.request_count if model.request_count > 0 else 0
            )

            model_analysis[model_name] = {
                "provider": model.provider_name,
                "characteristics": char,
                "performance": win_rates[model_name],
                "cost_efficiency": round(cost_per_response, 6),
                "tokens_used": model.total_tokens,
                "requests": model.request_count,
            }

        # å¯¹æ¯”çŸ©é˜µ
        pairwise_matrix = {}
        for model_a in model_names:
            pairwise_matrix[model_a] = {}
            for model_b in model_names:
                if model_a != model_b:
                    wins = self.stats["pairwise_matrix"][model_a][model_b]
                    losses = self.stats["pairwise_matrix"][model_b][model_a]
                    pairwise_matrix[model_a][model_b] = {
                        "wins": wins,
                        "losses": losses,
                        "win_rate": round(wins / (wins + losses), 3) if (wins + losses) > 0 else 0,
                    }
                else:
                    pairwise_matrix[model_a][model_b] = {"wins": 0, "losses": 0, "win_rate": 0}

        return {
            "test_info": {
                "test_name": "Multi-Model Comparison Test",
                "timestamp": datetime.now().isoformat(),
                "models_tested": len(models),
                "model_pairs": len(model_pairs),
                "samples_per_pair": sample_count,
                "total_comparisons": self.stats["total_comparisons"],
                "total_time": round(total_time, 2),
            },
            "rankings": rankings,
            "model_analysis": model_analysis,
            "pairwise_matrix": pairwise_matrix,
            "cost_analysis": {
                "total_cost": round(self.current_cost, 6),
                "cost_per_comparison": (
                    round(self.current_cost / self.stats["total_comparisons"], 6)
                    if self.stats["total_comparisons"] > 0
                    else 0
                ),
                "cost_breakdown": {k: round(v, 6) for k, v in self.stats["cost_breakdown"].items()},
            },
            "performance_metrics": {
                "comparisons_per_second": (
                    round(self.stats["total_comparisons"] / total_time, 2) if total_time > 0 else 0
                ),
                "avg_time_per_comparison": (
                    round(total_time / self.stats["total_comparisons"], 3)
                    if self.stats["total_comparisons"] > 0
                    else 0
                ),
            },
            "detailed_results": self.test_results[:20],  # ä¿å­˜å‰20ä¸ªè¯¦ç»†ç»“æœ
        }

    def _save_multi_model_results(self, summary: Dict[str, Any]):
        """ä¿å­˜å¤šæ¨¡å‹æµ‹è¯•ç»“æœ"""
        results_dir = Path("logs/multi_model_results")
        results_dir.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = results_dir / f"multi_model_test_{timestamp}.json"

        with open(results_file, "w", encoding="utf-8") as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)

        logger.info(f"\nğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {results_file}")

    def display_multi_model_summary(self, summary: Dict[str, Any]):
        """æ˜¾ç¤ºå¤šæ¨¡å‹æµ‹è¯•æ±‡æ€»"""
        if not summary:
            return

        logger.info("\n" + "=" * 80)
        logger.info("ğŸ† å¤šæ¨¡å‹æ¨ªå‘å¯¹æ¯”æµ‹è¯•æ±‡æ€»")
        logger.info("=" * 80)

        test_info = summary["test_info"]
        rankings = summary["rankings"]
        model_analysis = summary["model_analysis"]
        cost = summary["cost_analysis"]
        performance = summary["performance_metrics"]

        # åŸºæœ¬ä¿¡æ¯
        logger.info(f"ğŸ•’ æµ‹è¯•æ—¶é—´: {test_info['timestamp']}")
        logger.info(f"ğŸ¤– å‚ä¸æ¨¡å‹: {test_info['models_tested']}ä¸ª")
        logger.info(f"âš”ï¸ å¯¹æ¯”æ¬¡æ•°: {test_info['total_comparisons']:,}")
        logger.info(f"ğŸ“Š æ ·æœ¬æ•°é‡: {test_info['samples_per_pair']}ä¸ª")
        logger.info(f"â±ï¸ æ€»è€—æ—¶: {test_info['total_time']}ç§’")
        logger.info(f"ğŸš€ å¯¹æ¯”é€Ÿåº¦: {performance['comparisons_per_second']} å¯¹æ¯”/ç§’")

        # æ’åæ¦œ
        logger.info(f"\nğŸ† æ¨¡å‹æ’å (æŒ‰æ€»èƒœåœºæ•°):")
        for rank, (model_name, wins) in enumerate(rankings, 1):
            analysis = model_analysis[model_name]
            win_rate = analysis["performance"]["win_rate"]
            provider = analysis["provider"]
            cost_eff = analysis["cost_efficiency"]

            medal = "ğŸ¥‡" if rank == 1 else "ğŸ¥ˆ" if rank == 2 else "ğŸ¥‰" if rank == 3 else f"{rank}."
            logger.info(f"  {medal} {model_name} ({provider})")
            logger.info(f"      èƒœåœº: {wins} | èƒœç‡: {win_rate:.1%} | æˆæœ¬: ${cost_eff:.6f}/å“åº”")

        # æ¨¡å‹ç‰¹æ€§åˆ†æ
        logger.info(f"\nğŸ” æ¨¡å‹ç‰¹æ€§å¯¹æ¯”:")
        for model_name, analysis in model_analysis.items():
            char = analysis["characteristics"]
            perf = analysis["performance"]

            logger.info(f"  ğŸ“± {model_name}:")
            logger.info(
                f"    æ¨ç†é£æ ¼: {char['reasoning_style']} | è¯¦ç»†çº§åˆ«: {char['detail_level']}"
            )
            logger.info(
                f"    å‡†ç¡®æ€§åå·®: {char['accuracy_bias']:.2f} | å“åº”é•¿åº¦å€æ•°: {char['response_length_multiplier']:.1f}"
            )
            logger.info(
                f"    æˆ˜ç»©: {perf['wins']}èƒœ{perf['losses']}è´Ÿ{perf['ties']}å¹³ (èƒœç‡: {perf['win_rate']:.1%})"
            )

        # æˆæœ¬åˆ†æ
        logger.info(f"\nğŸ’° æˆæœ¬åˆ†æ:")
        logger.info(f"  æ€»æˆæœ¬: ${cost['total_cost']}")
        logger.info(f"  æ¯å¯¹æ¯”æˆæœ¬: ${cost['cost_per_comparison']}")
        logger.info(f"  æˆæœ¬åˆ†å¸ƒ:")
        for model_name, model_cost in cost["cost_breakdown"].items():
            percentage = (model_cost / cost["total_cost"]) * 100 if cost["total_cost"] > 0 else 0
            logger.info(f"    {model_name}: ${model_cost} ({percentage:.1f}%)")

        # å¯¹æ¯”çŸ©é˜µç²¾é€‰
        logger.info(f"\nâš”ï¸ å…³é”®å¯¹æ¯”ç»“æœ:")
        pairwise = summary["pairwise_matrix"]
        top_models = [model for model, _ in rankings[:3]]

        for i, model_a in enumerate(top_models):
            for model_b in top_models[i + 1 :]:
                if model_a in pairwise and model_b in pairwise[model_a]:
                    a_vs_b = pairwise[model_a][model_b]
                    b_vs_a = pairwise[model_b][model_a]

                    if a_vs_b["wins"] > b_vs_a["wins"]:
                        winner, loser = model_a, model_b
                        win_rate = a_vs_b["win_rate"]
                    else:
                        winner, loser = model_b, model_a
                        win_rate = b_vs_a["win_rate"]

                    logger.info(f"  {winner} vs {loser}: {win_rate:.1%} èƒœç‡")

        # ç»“è®º
        logger.info(f"\nğŸ¯ æµ‹è¯•ç»“è®º:")
        if rankings:
            champion = rankings[0][0]
            champion_analysis = model_analysis[champion]
            champion_style = champion_analysis["characteristics"]["reasoning_style"]
            champion_rate = champion_analysis["performance"]["win_rate"]

            logger.info(f"  ğŸ† å† å†›: {champion} (èƒœç‡: {champion_rate:.1%})")
            logger.info(f"  ğŸ¯ ä¼˜åŠ¿: {champion_style}æ¨ç†é£æ ¼è¡¨ç°æœ€ä½³")

            # æˆæœ¬æ•ˆç‡ä¹‹ç‹
            most_efficient = min(model_analysis.items(), key=lambda x: x[1]["cost_efficiency"])
            logger.info(
                f"  ğŸ’° æˆæœ¬æ•ˆç‡ç‹: {most_efficient[0]} (${most_efficient[1]['cost_efficiency']:.6f}/å“åº”)"
            )


async def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸ¯ å¤šæ¨¡å‹æ¨ªå‘å¯¹æ¯”æµ‹è¯•")
    logger.info("æµ‹è¯•5ä¸ªä¸åŒæ¨¡å‹çš„èƒ½åŠ›å¯¹æ¯”")

    # åˆ›å»ºæµ‹è¯•è¿è¡Œå™¨
    tester = MultiModelTestRunner(max_cost_usd=3.0)

    # è¿è¡Œå¤šæ¨¡å‹å¯¹æ¯”æµ‹è¯•
    summary = await tester.run_multi_model_comparison(
        sample_count=200, selected_models=None  # æ¯å¯¹æ¯”200ä¸ªæ ·æœ¬  # ä½¿ç”¨é»˜è®¤çš„5ä¸ªæ¨¡å‹
    )

    if summary:
        # æ˜¾ç¤ºç»“æœ
        tester.display_multi_model_summary(summary)
        logger.info("\nâœ… å¤šæ¨¡å‹å¯¹æ¯”æµ‹è¯•å®Œæˆï¼")
    else:
        logger.error("âŒ æµ‹è¯•å¤±è´¥")

    return summary


if __name__ == "__main__":
    asyncio.run(main())
