#!/usr/bin/env python3
"""
Real Multi-Model Comparison Test
çœŸå®å¤šæ¨¡å‹æ¨ªå‘å¯¹æ¯”æµ‹è¯• - ä½¿ç”¨çœŸå®APIè¿›è¡Œ3-4ä¸ªä¸åŒæ¨¡å‹çš„å…¨é¢å¯¹æ¯”
"""

import asyncio
import json
import logging
import os
import random
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

from test_real_api_integration import RealAPIProvider

# Configure logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)


class RealLLMJudge:
    """çœŸå®LLMè¯„åˆ¤ç³»ç»Ÿ - ä½¿ç”¨çœŸå®OpenAI APIä½œä¸ºè¯„åˆ¤è€…"""

    def __init__(self):
        self.judge_model = "gpt-4o-mini"
        self.total_evaluations = 0
        self.total_cost = 0.0

    async def evaluate_responses(
        self, prompt: str, responses: Dict[str, Dict], provider: RealAPIProvider
    ) -> Dict[str, Any]:
        """è¯„ä¼°ä¸¤ä¸ªå“åº”çš„è´¨é‡"""

        # æ„å»ºè¯„ä¼°æç¤º
        evaluation_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIå“åº”è¯„ä¼°ä¸“å®¶ã€‚è¯·å®¢è§‚è¯„ä¼°ä»¥ä¸‹ä¸¤ä¸ªAIæ¨¡å‹å¯¹åŒä¸€é—®é¢˜çš„å›ç­”è´¨é‡ã€‚

åŸå§‹é—®é¢˜: {prompt}

æ¨¡å‹Aå›ç­” ({responses['model_a']['provider']} - {responses['model_a']['model']}):
{responses['model_a']['content']}

æ¨¡å‹Bå›ç­” ({responses['model_b']['provider']} - {responses['model_b']['model']}):
{responses['model_b']['content']}

è¯·ä»ä»¥ä¸‹å››ä¸ªç»´åº¦è¯„ä¼° (æ¯ä¸ªç»´åº¦1-10åˆ†):
1. å‡†ç¡®æ€§ (Accuracy): å›ç­”æ˜¯å¦æ­£ç¡®ã€äº‹å®å‡†ç¡®
2. æ¸…æ™°åº¦ (Clarity): è¡¨è¾¾æ˜¯å¦æ¸…æ¥šã€æ˜“æ‡‚
3. å®Œæ•´æ€§ (Completeness): å›ç­”æ˜¯å¦å®Œæ•´ã€å…¨é¢
4. æ•ˆç‡æ€§ (Efficiency): å›ç­”æ˜¯å¦ç®€æ´ã€åˆ‡ä¸­è¦ç‚¹

è¯·ä»¥JSONæ ¼å¼è¿”å›è¯„ä¼°ç»“æœ:
{{
  "scores": {{
    "model_a": {{
      "accuracy": [1-10åˆ†],
      "clarity": [1-10åˆ†], 
      "completeness": [1-10åˆ†],
      "efficiency": [1-10åˆ†]
    }},
    "model_b": {{
      "accuracy": [1-10åˆ†],
      "clarity": [1-10åˆ†],
      "completeness": [1-10åˆ†],
      "efficiency": [1-10åˆ†]
    }}
  }},
  "winner": "[model_a|model_b|tie]",
  "confidence": [0.0-1.0],
  "reasoning": "è¯¦ç»†çš„è¯„ä¼°ç†ç”±å’Œåˆ†æ"
}}
"""

        # è°ƒç”¨è¯„åˆ¤API
        judge_result = await provider.generate_response(evaluation_prompt, "openai")

        if not judge_result["success"]:
            logger.error(f"Judge APIè°ƒç”¨å¤±è´¥: {judge_result['error']}")
            return {"success": False, "error": judge_result["error"]}

        # è§£æè¯„åˆ¤ç»“æœ
        try:
            # å°è¯•ä»å“åº”ä¸­æå–JSON
            content = judge_result["content"].strip()

            # æŸ¥æ‰¾JSONå†…å®¹
            json_start = content.find("{")
            json_end = content.rfind("}") + 1

            if json_start != -1 and json_end > json_start:
                json_content = content[json_start:json_end]
                evaluation = json.loads(json_content)
            else:
                raise ValueError("No valid JSON found in response")

            # è®¡ç®—åŠ æƒåˆ†æ•°
            weights = {"accuracy": 0.35, "clarity": 0.25, "completeness": 0.25, "efficiency": 0.15}

            weighted_scores = {}
            for model in ["model_a", "model_b"]:
                score = sum(
                    evaluation["scores"][model][dimension] * weight
                    for dimension, weight in weights.items()
                )
                weighted_scores[model] = round(score, 2)

            self.total_evaluations += 1
            self.total_cost += judge_result["cost"]

            return {
                "success": True,
                "winner": evaluation["winner"],
                "confidence": evaluation["confidence"],
                "reasoning": evaluation["reasoning"],
                "scores": evaluation["scores"],
                "weighted_scores": weighted_scores,
                "judge_cost": judge_result["cost"],
                "judge_tokens": judge_result["total_tokens"],
                "judge_reasoning": judge_result["content"],
            }

        except Exception as e:
            logger.error(f"è§£æè¯„åˆ¤ç»“æœå¤±è´¥: {str(e)}")
            logger.error(f"åŸå§‹å“åº”: {judge_result['content']}")

            # è¿”å›é»˜è®¤è¯„ä¼°ç»“æœ
            return {
                "success": True,
                "winner": "tie",
                "confidence": 0.5,
                "reasoning": "è¯„ä¼°ç³»ç»Ÿè§£æå¤±è´¥ï¼Œé»˜è®¤ä¸ºå¹³å±€",
                "scores": {
                    "model_a": {"accuracy": 5, "clarity": 5, "completeness": 5, "efficiency": 5},
                    "model_b": {"accuracy": 5, "clarity": 5, "completeness": 5, "efficiency": 5},
                },
                "weighted_scores": {"model_a": 5.0, "model_b": 5.0},
                "judge_cost": judge_result["cost"],
                "judge_tokens": judge_result["total_tokens"],
                "judge_reasoning": f"è§£æé”™è¯¯: {str(e)}",
            }


class RealMultiModelComparison:
    """çœŸå®å¤šæ¨¡å‹å¯¹æ¯”æµ‹è¯•"""

    def __init__(self):
        self.judge = RealLLMJudge()
        self.results = []

    def load_test_dataset(self, sample_size: int = 20) -> List[Dict[str, str]]:
        """åŠ è½½æµ‹è¯•æ•°æ®é›†"""

        # ä»çœŸå®æ•°æ®é›†æ–‡ä»¶åŠ è½½
        datasets = []

        # å°è¯•åŠ è½½ARC-Easyæ•°æ®
        arc_path = Path("data/processed/ARC-Easy_standardized.json")
        if arc_path.exists():
            with open(arc_path, "r", encoding="utf-8") as f:
                arc_data = json.load(f)
                datasets.extend(arc_data[: sample_size // 2])

        # å°è¯•åŠ è½½GSM8Kæ•°æ®
        gsm8k_path = Path("data/processed/GSM8K_standardized.json")
        if gsm8k_path.exists():
            with open(gsm8k_path, "r", encoding="utf-8") as f:
                gsm8k_data = json.load(f)
                datasets.extend(gsm8k_data[: sample_size // 2])

        # å¦‚æœæ²¡æœ‰æ•°æ®é›†æ–‡ä»¶ï¼Œä½¿ç”¨é¢„è®¾çš„æµ‹è¯•é—®é¢˜
        if not datasets:
            test_questions = [
                {
                    "id": "test_001",
                    "prompt": "What is the capital of France and what is it famous for?",
                    "category": "geography",
                    "difficulty": "easy",
                },
                {
                    "id": "test_002",
                    "prompt": "Explain the concept of photosynthesis and its importance.",
                    "category": "science",
                    "difficulty": "medium",
                },
                {
                    "id": "test_003",
                    "prompt": "Calculate 157 Ã— 23 and show your work.",
                    "category": "math",
                    "difficulty": "easy",
                },
                {
                    "id": "test_004",
                    "prompt": "What are the main causes of climate change?",
                    "category": "science",
                    "difficulty": "medium",
                },
                {
                    "id": "test_005",
                    "prompt": "Describe the water cycle in nature.",
                    "category": "science",
                    "difficulty": "easy",
                },
                {
                    "id": "test_006",
                    "prompt": "What is the difference between renewable and non-renewable energy?",
                    "category": "science",
                    "difficulty": "medium",
                },
                {
                    "id": "test_007",
                    "prompt": "Solve for x: 2x + 5 = 17",
                    "category": "math",
                    "difficulty": "easy",
                },
                {
                    "id": "test_008",
                    "prompt": "What are the three branches of government and their functions?",
                    "category": "civics",
                    "difficulty": "medium",
                },
                {
                    "id": "test_009",
                    "prompt": "Explain the process of cellular respiration.",
                    "category": "science",
                    "difficulty": "hard",
                },
                {
                    "id": "test_010",
                    "prompt": "What is the significance of the Magna Carta?",
                    "category": "history",
                    "difficulty": "medium",
                },
            ]

            datasets = test_questions[:sample_size]

        # éšæœºæ‰“ä¹±æ•°æ®
        random.shuffle(datasets)
        return datasets[:sample_size]

    async def run_comparison(
        self, sample_size: int = 10, budget_limit: float = 2.0
    ) -> Dict[str, Any]:
        """è¿è¡Œå¤šæ¨¡å‹å¯¹æ¯”æµ‹è¯•"""

        logger.info(f"å¼€å§‹çœŸå®å¤šæ¨¡å‹å¯¹æ¯”æµ‹è¯• - {sample_size}ä¸ªæ ·æœ¬ï¼Œé¢„ç®—é™åˆ¶${budget_limit}")

        start_time = time.time()

        # åŠ è½½æµ‹è¯•æ•°æ®
        test_data = self.load_test_dataset(sample_size)
        logger.info(f"åŠ è½½äº†{len(test_data)}ä¸ªæµ‹è¯•æ ·æœ¬")

        results = {
            "test_info": {
                "test_name": "Real Multi-Model Comparison Test",
                "timestamp": datetime.now().isoformat(),
                "sample_size": len(test_data),
                "budget_limit": budget_limit,
            },
            "model_configs": {},
            "comparison_results": [],
            "summary": {},
        }

        async with RealAPIProvider() as provider:
            # æ£€æŸ¥å¯ç”¨çš„APIæä¾›å•†
            available_providers = list(provider.api_configs.keys())
            logger.info(f"å¯ç”¨çš„APIæä¾›å•†: {available_providers}")

            if len(available_providers) < 2:
                logger.error("éœ€è¦è‡³å°‘2ä¸ªAPIæä¾›å•†æ‰èƒ½è¿›è¡Œå¯¹æ¯”æµ‹è¯•")
                return {
                    "error": "Insufficient API providers for comparison",
                    "available_providers": available_providers,
                    "required_minimum": 2,
                }

            # è®°å½•æ¨¡å‹é…ç½®
            for provider_name in available_providers:
                config = provider.api_configs[provider_name]
                results["model_configs"][provider_name] = {
                    "provider": config.provider,
                    "model": config.model,
                    "pricing": config.pricing,
                }

            # è¿è¡Œå¯¹æ¯”æµ‹è¯•
            total_cost = 0.0
            model_stats = {
                provider: {"wins": 0, "requests": 0, "tokens": 0, "cost": 0.0}
                for provider in available_providers
            }
            ties = 0

            for i, sample in enumerate(test_data):
                if total_cost >= budget_limit:
                    logger.warning(f"è¾¾åˆ°é¢„ç®—é™åˆ¶ ${budget_limit}ï¼Œåœæ­¢æµ‹è¯•")
                    break

                logger.info(f"å¤„ç†æ ·æœ¬ {i+1}/{len(test_data)}: {sample.get('id', f'sample_{i+1}')}")

                # éšæœºé€‰æ‹©ä¸¤ä¸ªä¸åŒçš„æä¾›å•†è¿›è¡Œå¯¹æ¯”
                model_pair = random.sample(available_providers, 2)
                model_a, model_b = model_pair

                prompt = sample.get("prompt", sample.get("question", ""))
                if not prompt:
                    logger.warning(f"æ ·æœ¬ {i+1} æ²¡æœ‰æœ‰æ•ˆçš„promptï¼Œè·³è¿‡")
                    continue

                # ç”Ÿæˆå“åº”
                logger.info(f"  è°ƒç”¨ {model_a} API...")
                response_a = await provider.generate_response(prompt, model_a)
                await asyncio.sleep(0.5)  # é¿å…é€Ÿç‡é™åˆ¶

                logger.info(f"  è°ƒç”¨ {model_b} API...")
                response_b = await provider.generate_response(prompt, model_b)
                await asyncio.sleep(0.5)  # é¿å…é€Ÿç‡é™åˆ¶

                if not response_a["success"] or not response_b["success"]:
                    logger.error(f"  APIè°ƒç”¨å¤±è´¥ï¼Œè·³è¿‡æ ·æœ¬ {i+1}")
                    continue

                # æ›´æ–°ç»Ÿè®¡
                model_stats[model_a]["requests"] += 1
                model_stats[model_a]["tokens"] += response_a["total_tokens"]
                model_stats[model_a]["cost"] += response_a["cost"]

                model_stats[model_b]["requests"] += 1
                model_stats[model_b]["tokens"] += response_b["total_tokens"]
                model_stats[model_b]["cost"] += response_b["cost"]

                total_cost += response_a["cost"] + response_b["cost"]

                # è¿›è¡Œè¯„åˆ¤
                logger.info(f"  è¿›è¡ŒLLMè¯„åˆ¤...")
                responses_for_judge = {"model_a": response_a, "model_b": response_b}

                evaluation = await self.judge.evaluate_responses(
                    prompt, responses_for_judge, provider
                )
                await asyncio.sleep(0.5)  # é¿å…é€Ÿç‡é™åˆ¶

                if not evaluation["success"]:
                    logger.error(f"  è¯„åˆ¤å¤±è´¥ï¼Œè·³è¿‡æ ·æœ¬ {i+1}")
                    continue

                total_cost += evaluation["judge_cost"]

                # æ›´æ–°èƒœè´Ÿç»Ÿè®¡
                winner = evaluation["winner"]
                if winner == "model_a":
                    model_stats[model_a]["wins"] += 1
                elif winner == "model_b":
                    model_stats[model_b]["wins"] += 1
                else:
                    ties += 1

                # è®°å½•ç»“æœ
                comparison_result = {
                    "sample_num": i + 1,
                    "sample_data": sample,
                    "model_pair": {
                        "model_a": f"{model_a} ({response_a['model']})",
                        "model_b": f"{model_b} ({response_b['model']})",
                    },
                    "responses": {
                        "model_a": {
                            "provider": model_a,
                            "model": response_a["model"],
                            "content": response_a["content"],
                            "tokens": response_a["total_tokens"],
                            "cost": response_a["cost"],
                        },
                        "model_b": {
                            "provider": model_b,
                            "model": response_b["model"],
                            "content": response_b["content"],
                            "tokens": response_b["total_tokens"],
                            "cost": response_b["cost"],
                        },
                    },
                    "evaluation": evaluation,
                    "timestamp": datetime.now().isoformat(),
                }

                results["comparison_results"].append(comparison_result)

                logger.info(
                    f"  ç»“æœ: {winner}, ç½®ä¿¡åº¦: {evaluation['confidence']:.3f}, æˆæœ¬: ${response_a['cost'] + response_b['cost'] + evaluation['judge_cost']:.6f}"
                )

        end_time = time.time()
        total_time = end_time - start_time

        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        completed_comparisons = len(results["comparison_results"])

        results["summary"] = {
            "total_time": total_time,
            "completed_comparisons": completed_comparisons,
            "total_cost": total_cost,
            "budget_used_percentage": (total_cost / budget_limit * 100) if budget_limit > 0 else 0,
            "avg_cost_per_comparison": (
                total_cost / completed_comparisons if completed_comparisons > 0 else 0
            ),
            "model_stats": model_stats,
            "ties": ties,
            "judge_stats": {
                "total_evaluations": self.judge.total_evaluations,
                "total_cost": self.judge.total_cost,
            },
        }

        return results

    def save_results(self, results: Dict[str, Any], filename: str = None):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"real_multi_model_comparison_{timestamp}.json"

        # åˆ›å»ºç»“æœç›®å½•
        results_dir = Path("logs/multi_model_results")
        results_dir.mkdir(parents=True, exist_ok=True)

        filepath = results_dir / filename

        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        logger.info(f"æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {filepath}")
        return filepath


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ çœŸå®å¤šæ¨¡å‹æ¨ªå‘å¯¹æ¯”æµ‹è¯•")
    print("=" * 50)

    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    api_keys = {
        "OpenAI": os.getenv("OPENAI_API_KEY"),
        "Anthropic": os.getenv("ANTHROPIC_API_KEY"),
        "Google": os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY"),
    }

    print("ğŸ“‹ APIå¯†é’¥çŠ¶æ€:")
    available_count = 0
    for provider, key in api_keys.items():
        status = "âœ… å·²é…ç½®" if key else "âŒ æœªé…ç½®"
        if key:
            available_count += 1
        print(f"  {provider}: {status}")

    if available_count < 2:
        print(f"\nâŒ é”™è¯¯: éœ€è¦è‡³å°‘2ä¸ªAPIå¯†é’¥è¿›è¡Œå¯¹æ¯”æµ‹è¯•ï¼å½“å‰åªæœ‰{available_count}ä¸ª")
        print("è¯·è®¾ç½®ä»¥ä¸‹ç¯å¢ƒå˜é‡:")
        print("  export OPENAI_API_KEY='your_openai_key'")
        print("  export ANTHROPIC_API_KEY='your_anthropic_key'")
        print("  export GOOGLE_API_KEY='your_google_key'")
        return

    print(f"\nâœ… å¯ä»¥è¿›è¡Œå¯¹æ¯”æµ‹è¯• (å¯ç”¨æä¾›å•†: {available_count}ä¸ª)")

    # è¿è¡Œæµ‹è¯•
    comparison = RealMultiModelComparison()

    print("\nğŸ”¬ å¼€å§‹å¤šæ¨¡å‹å¯¹æ¯”æµ‹è¯•...")
    print("  æ ·æœ¬æ•°é‡: 5ä¸ª")
    print("  é¢„ç®—é™åˆ¶: $1.00")
    print("  è¯„åˆ¤æ¨¡å‹: gpt-4o-mini")

    results = await comparison.run_comparison(sample_size=5, budget_limit=1.0)

    if "error" in results:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {results['error']}")
        return

    # ä¿å­˜ç»“æœ
    filepath = comparison.save_results(results)

    # æ˜¾ç¤ºç»“æœæ‘˜è¦
    summary = results["summary"]
    print(f"\nğŸ“Š æµ‹è¯•æ‘˜è¦:")
    print(f"  æ€»è€—æ—¶: {summary['total_time']:.2f}ç§’")
    print(f"  å®Œæˆå¯¹æ¯”: {summary['completed_comparisons']}ä¸ª")
    print(f"  æ€»æˆæœ¬: ${summary['total_cost']:.6f}")
    print(f"  é¢„ç®—ä½¿ç”¨: {summary['budget_used_percentage']:.1f}%")
    print(f"  å¹³å‡æˆæœ¬/å¯¹æ¯”: ${summary['avg_cost_per_comparison']:.6f}")
    print(f"  å¹³å±€æ¬¡æ•°: {summary['ties']}")

    print(f"\nğŸ† æ¨¡å‹è¡¨ç°:")
    for model, stats in summary["model_stats"].items():
        if stats["requests"] > 0:
            win_rate = stats["wins"] / stats["requests"] * 100 if stats["requests"] > 0 else 0
            print(f"  {model.upper()}:")
            print(f"    å‚ä¸å¯¹æ¯”: {stats['requests']}æ¬¡")
            print(f"    è·èƒœ: {stats['wins']}æ¬¡ ({win_rate:.1f}%)")
            print(f"    Tokenä½¿ç”¨: {stats['tokens']}")
            print(f"    æˆæœ¬: ${stats['cost']:.6f}")

    print(f"\nâœ… æµ‹è¯•å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {filepath}")


if __name__ == "__main__":
    asyncio.run(main())
