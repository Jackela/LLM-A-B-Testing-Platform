#!/usr/bin/env python3
"""
Real Dataset LLM Evaluation Test
ä½¿ç”¨çœŸå®æ•°æ®é›†è¿›è¡ŒLLM as a Judgeè¯„ä¼°
"""

import asyncio
import json
import logging
import random
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

# Configure logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)


class RealDatasetLLMProvider:
    """åŸºäºçœŸå®æ•°æ®é›†çš„LLMæä¾›å•†æ¨¡æ‹Ÿ"""

    def __init__(self, provider_name: str, style: str):
        self.provider_name = provider_name
        self.style = style

    async def generate_response(self, question: str, context: Dict[str, Any] = None) -> str:
        """æ ¹æ®é—®é¢˜ç±»å‹å’Œé£æ ¼ç”Ÿæˆå“åº”"""
        await asyncio.sleep(random.uniform(0.1, 0.3))  # æ¨¡æ‹ŸçœŸå®APIå»¶è¿Ÿ

        if context and context.get("source") == "ARC-Easy":
            return self._handle_arc_question(question, context)
        elif context and context.get("source") == "GSM8K":
            return self._handle_math_question(question, context)
        else:
            return self._generate_generic_response(question)

    def _handle_arc_question(self, question: str, context: Dict[str, Any]) -> str:
        """å¤„ç†ARCç§‘å­¦æ¨ç†é—®é¢˜"""
        if "choice" in question.lower() or "which" in question.lower():
            if self.style == "detailed":
                return f"Based on scientific principles, I need to analyze each option carefully. {context.get('expected_output', 'The correct answer requires understanding the underlying scientific concepts.')}"
            else:
                return context.get(
                    "expected_output", "The answer is based on scientific reasoning."
                )

        # é€šç”¨ç§‘å­¦é—®é¢˜å¤„ç†
        if self.style == "detailed":
            return f"This is a scientific question that requires careful analysis. {context.get('expected_output', 'Scientific reasoning leads us to the answer.')}"
        else:
            return context.get("expected_output", "Scientific answer.")

    def _handle_math_question(self, question: str, context: Dict[str, Any]) -> str:
        """å¤„ç†GSM8Kæ•°å­¦é—®é¢˜"""
        if self.style == "detailed":
            # ç”Ÿæˆè¯¦ç»†çš„æ•°å­¦è§£é¢˜è¿‡ç¨‹
            steps = [
                "Let me solve this step by step:",
                "First, I'll identify what we know and what we need to find.",
                "Then I'll set up the equation or calculation.",
                "Finally, I'll solve and check the answer.",
            ]
            return f"{' '.join(steps)} {context.get('ground_truth', 'Mathematical calculation completed.')}"
        else:
            # ç®€æ´çš„ç­”æ¡ˆ
            return f"The answer is {context.get('ground_truth', 'calculated result')}."

    def _generate_generic_response(self, question: str) -> str:
        """ç”Ÿæˆé€šç”¨å“åº”"""
        if self.style == "detailed":
            return f"This question requires comprehensive analysis. I'll provide a detailed explanation to ensure clarity and understanding."
        else:
            return "Here's a concise answer to your question."


class AdvancedLLMJudge:
    """é«˜çº§LLMè¯„åˆ¤è€…ï¼Œé’ˆå¯¹ä¸åŒç±»å‹çš„é—®é¢˜ä½¿ç”¨ä¸åŒçš„è¯„ä¼°æ ‡å‡†"""

    def __init__(self):
        self.evaluation_criteria = {
            "science": {
                "accuracy": "Scientific correctness and factual accuracy",
                "reasoning": "Quality of scientific reasoning and logic",
                "clarity": "Clarity of explanation",
                "completeness": "Thoroughness of the answer",
            },
            "math": {
                "accuracy": "Mathematical correctness",
                "methodology": "Problem-solving approach and steps",
                "clarity": "Clarity of mathematical explanation",
                "efficiency": "Efficiency of solution method",
            },
            "general": {
                "accuracy": "Factual correctness",
                "helpfulness": "Usefulness to the user",
                "clarity": "Clear communication",
                "completeness": "Comprehensive coverage",
            },
        }

    async def evaluate_responses(
        self,
        question: str,
        response_a: str,
        response_b: str,
        provider_a: str,
        provider_b: str,
        context: Dict[str, Any] = None,
    ) -> Dict[str, Any]:
        """ä½¿ç”¨é«˜çº§è¯„ä¼°æ ‡å‡†è¯„ä¼°å“åº”"""

        # ç¡®å®šé—®é¢˜ç±»å‹å’Œè¯„ä¼°æ ‡å‡†
        question_type = self._determine_question_type(question, context)
        criteria = self.evaluation_criteria.get(question_type, self.evaluation_criteria["general"])

        # æ¨¡æ‹ŸLLMè¯„ä¼°è¿‡ç¨‹
        await asyncio.sleep(random.uniform(0.3, 0.7))

        # æ‰§è¡Œè¯„ä¼°
        evaluation = self._perform_evaluation(
            question, response_a, response_b, provider_a, provider_b, context, criteria
        )
        evaluation["question_type"] = question_type
        evaluation["evaluation_criteria"] = list(criteria.keys())

        return evaluation

    def _determine_question_type(self, question: str, context: Dict[str, Any] = None) -> str:
        """ç¡®å®šé—®é¢˜ç±»å‹"""
        if context:
            if context.get("source") == "ARC-Easy" or context.get("category") == "science":
                return "science"
            elif context.get("source") == "GSM8K" or context.get("category") == "math":
                return "math"

        # åŸºäºå…³é”®è¯çš„åˆ†ç±»
        if any(
            keyword in question.lower()
            for keyword in ["calculate", "solve", "math", "+", "-", "*", "/"]
        ):
            return "math"
        elif any(
            keyword in question.lower()
            for keyword in ["scientific", "experiment", "hypothesis", "theory"]
        ):
            return "science"

        return "general"

    def _perform_evaluation(
        self,
        question: str,
        response_a: str,
        response_b: str,
        provider_a: str,
        provider_b: str,
        context: Dict[str, Any],
        criteria: Dict[str, str],
    ) -> Dict[str, Any]:
        """æ‰§è¡Œè¯¦ç»†è¯„ä¼°"""

        # åŸºç¡€è¯„åˆ†è®¡ç®—
        def calculate_scores(response: str, provider: str) -> Dict[str, float]:
            scores = {}

            # æ ¹æ®å“åº”é•¿åº¦å’Œè´¨é‡è¯„ä¼°
            response_length = len(response)
            has_reasoning = any(
                word in response.lower()
                for word in ["because", "since", "therefore", "step", "first", "then"]
            )

            for criterion in criteria:
                base_score = 7.0

                if criterion in ["accuracy", "mathematical correctness", "scientific correctness"]:
                    # å‡†ç¡®æ€§è¯„ä¼° - åŸºäºæ˜¯å¦åŒ…å«é¢„æœŸç­”æ¡ˆ
                    if context and context.get("ground_truth"):
                        if str(context["ground_truth"]).lower() in response.lower():
                            base_score = 9.0
                        elif (
                            context.get("expected_output")
                            and context["expected_output"].lower() in response.lower()
                        ):
                            base_score = 8.5
                    scores[criterion] = base_score

                elif criterion in ["reasoning", "methodology"]:
                    # æ¨ç†è´¨é‡è¯„ä¼°
                    if has_reasoning:
                        base_score = 8.5 if "detailed" in provider.lower() else 7.0
                    scores[criterion] = base_score

                elif criterion in ["clarity", "clear communication"]:
                    # æ¸…æ™°åº¦è¯„ä¼°
                    if response_length > 20 and not response.startswith("This is"):
                        base_score = 8.0
                    scores[criterion] = base_score

                elif criterion in ["completeness", "comprehensive coverage", "efficiency"]:
                    # å®Œæ•´æ€§/æ•ˆç‡è¯„ä¼°
                    if "detailed" in provider.lower():
                        base_score = 8.5 if response_length > 50 else 7.5
                    else:
                        base_score = 7.5 if response_length < 100 else 8.0  # ç®€æ´æ€§å¥–åŠ±
                    scores[criterion] = base_score

                elif criterion in ["helpfulness", "usefulness"]:
                    # æœ‰ç”¨æ€§è¯„ä¼°
                    base_score = 8.0 if response_length > 30 else 7.0
                    scores[criterion] = base_score

                else:
                    scores[criterion] = base_score

            # è®¡ç®—æ€»ä½“è¯„åˆ†
            overall = sum(scores.values()) / len(scores)
            scores["overall"] = round(overall, 1)

            return {k: round(v, 1) for k, v in scores.items()}

        # è®¡ç®—ä¸¤ä¸ªå“åº”çš„è¯„åˆ†
        scores_a = calculate_scores(response_a, provider_a)
        scores_b = calculate_scores(response_b, provider_b)

        # ç¡®å®šè·èƒœè€…å’Œç½®ä¿¡åº¦
        overall_a = scores_a["overall"]
        overall_b = scores_b["overall"]

        if overall_a > overall_b:
            winner = "A"
            confidence = min(0.95, 0.6 + (overall_a - overall_b) / 10)
        elif overall_b > overall_a:
            winner = "B"
            confidence = min(0.95, 0.6 + (overall_b - overall_a) / 10)
        else:
            winner = "Tie"
            confidence = 0.5

        # ç”Ÿæˆè¯„ä¼°ç†ç”±
        reasoning = self._generate_reasoning(scores_a, scores_b, winner, criteria, context)

        return {
            "scores": {"response_a": scores_a, "response_b": scores_b},
            "winner": winner,
            "confidence": round(confidence, 2),
            "reasoning": reasoning,
            "judge_provider": "Advanced Claude Judge",
        }

    def _generate_reasoning(
        self,
        scores_a: Dict[str, float],
        scores_b: Dict[str, float],
        winner: str,
        criteria: Dict[str, str],
        context: Dict[str, Any],
    ) -> str:
        """ç”Ÿæˆè¯„ä¼°ç†ç”±"""

        if winner == "Tie":
            return "ä¸¤ä¸ªå“åº”åœ¨å„ä¸ªç»´åº¦ä¸Šè¡¨ç°ç›¸è¿‘ï¼Œéš¾åˆ†é«˜ä¸‹ã€‚"

        # æ‰¾å‡ºä¸»è¦å·®å¼‚
        score_diff = {}
        for criterion in criteria:
            if criterion in scores_a and criterion in scores_b:
                score_diff[criterion] = abs(scores_a[criterion] - scores_b[criterion])

        # æ‰¾åˆ°æœ€å¤§å·®å¼‚çš„ç»´åº¦
        if score_diff:
            max_diff_criterion = max(score_diff.keys(), key=lambda k: score_diff[k])

            criterion_names = {
                "accuracy": "å‡†ç¡®æ€§",
                "reasoning": "æ¨ç†è´¨é‡",
                "methodology": "è§£é¢˜æ–¹æ³•",
                "clarity": "æ¸…æ™°åº¦",
                "completeness": "å®Œæ•´æ€§",
                "efficiency": "æ•ˆç‡",
                "helpfulness": "æœ‰ç”¨æ€§",
            }

            criterion_cn = criterion_names.get(max_diff_criterion, max_diff_criterion)

            if winner == "A":
                return f"å“åº”Aåœ¨{criterion_cn}æ–¹é¢æ˜æ˜¾ä¼˜äºå“åº”Bï¼Œæ˜¾ç¤ºäº†æ›´å¥½çš„ç†è§£å’Œè¡¨è¾¾èƒ½åŠ›ã€‚"
            else:
                return f"å“åº”Båœ¨{criterion_cn}æ–¹é¢æ˜æ˜¾ä¼˜äºå“åº”Aï¼Œæä¾›äº†æ›´ä¼˜è´¨çš„å›ç­”ã€‚"

        return f"å“åº”{winner}åœ¨ç»¼åˆè¯„ä¼°ä¸­è¡¨ç°æ›´å¥½ã€‚"


class RealDatasetTestRunner:
    """çœŸå®æ•°æ®é›†æµ‹è¯•è¿è¡Œå™¨"""

    def __init__(self):
        # åˆå§‹åŒ–ä¸¤ä¸ªä¸åŒé£æ ¼çš„æä¾›å•†
        self.provider_a = RealDatasetLLMProvider("GPT-4-Analytical", "detailed")
        self.provider_b = RealDatasetLLMProvider("Claude-Efficient", "concise")
        self.judge = AdvancedLLMJudge()
        self.test_results = []

    def load_sample_data(self, sample_size: int = 6) -> List[Dict[str, Any]]:
        """ä»çœŸå®æ•°æ®é›†åŠ è½½æ ·æœ¬æ•°æ®"""
        datasets_dir = Path("data/processed")
        sample_data = []

        # ä»ARCæ•°æ®é›†åŠ è½½æ ·æœ¬
        arc_file = datasets_dir / "arc_easy.json"
        if arc_file.exists():
            with open(arc_file, "r", encoding="utf-8") as f:
                arc_data = json.load(f)
            sample_data.extend(random.sample(arc_data, min(3, len(arc_data))))

        # ä»GSM8Kæ•°æ®é›†åŠ è½½æ ·æœ¬
        gsm8k_file = datasets_dir / "gsm8k.json"
        if gsm8k_file.exists():
            with open(gsm8k_file, "r", encoding="utf-8") as f:
                gsm8k_data = json.load(f)
            sample_data.extend(random.sample(gsm8k_data, min(3, len(gsm8k_data))))

        return sample_data[:sample_size]

    async def run_single_evaluation(self, sample: Dict[str, Any]) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªæ ·æœ¬çš„è¯„ä¼°"""
        question = sample["prompt"]
        sample_id = sample["id"]

        logger.info(f"ğŸ” è¯„ä¼°æ ·æœ¬ {sample_id}")
        logger.info(f"ğŸ“ é—®é¢˜: {question[:100]}...")
        logger.info(
            f"ğŸ·ï¸ ç±»åˆ«: {sample.get('category', 'unknown')} | æ¥æº: {sample.get('source', 'unknown')}"
        )

        # è·å–ä¸¤ä¸ªæä¾›å•†çš„å“åº”
        start_time = time.time()
        response_a = await self.provider_a.generate_response(question, sample)
        response_b = await self.provider_b.generate_response(question, sample)
        response_time = time.time() - start_time

        logger.info(f"ğŸ“¤ Provider A: {response_a[:80]}...")
        logger.info(f"ğŸ“¤ Provider B: {response_b[:80]}...")

        # LLMè¯„ä¼°
        start_time = time.time()
        evaluation = await self.judge.evaluate_responses(
            question,
            response_a,
            response_b,
            self.provider_a.provider_name,
            self.provider_b.provider_name,
            sample,
        )
        eval_time = time.time() - start_time

        # ç¼–è¯‘ç»“æœ
        result = {
            "sample_data": sample,
            "responses": {
                "provider_a": {"name": self.provider_a.provider_name, "response": response_a},
                "provider_b": {"name": self.provider_b.provider_name, "response": response_b},
            },
            "evaluation": evaluation,
            "timing": {
                "response_time": round(response_time, 3),
                "evaluation_time": round(eval_time, 3),
            },
            "timestamp": datetime.now().isoformat(),
        }

        return result

    async def run_comprehensive_test(self, sample_size: int = 6) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„çœŸå®æ•°æ®é›†æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹çœŸå®æ•°æ®é›†LLM as a Judgeæµ‹è¯•")
        logger.info("=" * 70)

        # åŠ è½½æµ‹è¯•æ•°æ®
        sample_data = self.load_sample_data(sample_size)
        if not sample_data:
            logger.error("âŒ æ— æ³•åŠ è½½æµ‹è¯•æ•°æ®ï¼Œè¯·ç¡®ä¿æ•°æ®é›†æ–‡ä»¶å­˜åœ¨")
            return {}

        logger.info(f"ğŸ“Š å·²åŠ è½½ {len(sample_data)} ä¸ªæµ‹è¯•æ ·æœ¬")

        overall_start_time = time.time()

        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        for i, sample in enumerate(sample_data, 1):
            logger.info(f"\nğŸ“‹ æµ‹è¯• {i}/{len(sample_data)}")
            try:
                result = await self.run_single_evaluation(sample)
                self.test_results.append(result)

                # æ˜¾ç¤ºè¯„ä¼°ç»“æœ
                eval_data = result["evaluation"]
                winner = eval_data["winner"]
                confidence = eval_data["confidence"]
                question_type = eval_data.get("question_type", "unknown")

                logger.info(f"ğŸ† è·èƒœè€…: {winner} (ç½®ä¿¡åº¦: {confidence}) | ç±»å‹: {question_type}")
                logger.info(f"ğŸ’­ è¯„ä¼°ç†ç”±: {eval_data['reasoning']}")

            except Exception as e:
                logger.error(f"âŒ æµ‹è¯• {i} å¤±è´¥: {e}")

        total_time = time.time() - overall_start_time

        # ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡
        summary = self._generate_comprehensive_summary(total_time)

        # ä¿å­˜ç»“æœ
        self._save_results(summary)

        return summary

    def _generate_comprehensive_summary(self, total_time: float) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆæµ‹è¯•æ±‡æ€»"""
        if not self.test_results:
            return {}

        # åŸºç¡€ç»Ÿè®¡
        provider_a_wins = sum(1 for r in self.test_results if r["evaluation"]["winner"] == "A")
        provider_b_wins = sum(1 for r in self.test_results if r["evaluation"]["winner"] == "B")
        ties = sum(1 for r in self.test_results if r["evaluation"]["winner"] == "Tie")

        # æŒ‰é—®é¢˜ç±»å‹åˆ†æ
        type_analysis = {}
        for result in self.test_results:
            q_type = result["evaluation"].get("question_type", "unknown")
            if q_type not in type_analysis:
                type_analysis[q_type] = {"A": 0, "B": 0, "Tie": 0, "total": 0}

            winner = result["evaluation"]["winner"]
            type_analysis[q_type][winner] += 1
            type_analysis[q_type]["total"] += 1

        # è®¡ç®—å¹³å‡è¯„åˆ†å’Œç½®ä¿¡åº¦
        avg_confidence = round(
            sum(r["evaluation"]["confidence"] for r in self.test_results) / len(self.test_results),
            3,
        )

        # æ€§èƒ½æŒ‡æ ‡
        avg_response_time = round(
            sum(r["timing"]["response_time"] for r in self.test_results) / len(self.test_results), 3
        )
        avg_eval_time = round(
            sum(r["timing"]["evaluation_time"] for r in self.test_results) / len(self.test_results),
            3,
        )

        # æ•°æ®é›†åˆ†å¸ƒ
        source_distribution = {}
        for result in self.test_results:
            source = result["sample_data"].get("source", "unknown")
            source_distribution[source] = source_distribution.get(source, 0) + 1

        summary = {
            "test_info": {
                "test_name": "Real Dataset LLM as a Judge Test",
                "timestamp": datetime.now().isoformat(),
                "total_samples": len(self.test_results),
                "total_time": round(total_time, 2),
                "avg_confidence": avg_confidence,
            },
            "providers": {
                "provider_a": self.provider_a.provider_name,
                "provider_b": self.provider_b.provider_name,
            },
            "overall_results": {
                "provider_a_wins": provider_a_wins,
                "provider_b_wins": provider_b_wins,
                "ties": ties,
                "win_rate_a": round(provider_a_wins / len(self.test_results), 3),
                "win_rate_b": round(provider_b_wins / len(self.test_results), 3),
            },
            "type_analysis": type_analysis,
            "source_distribution": source_distribution,
            "performance_metrics": {
                "avg_response_time": avg_response_time,
                "avg_evaluation_time": avg_eval_time,
                "total_time": round(total_time, 2),
            },
            "detailed_results": self.test_results,
        }

        return summary

    def _save_results(self, summary: Dict[str, Any]):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        results_dir = Path("logs/real_dataset_results")
        results_dir.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = results_dir / f"real_dataset_test_{timestamp}.json"

        with open(results_file, "w", encoding="utf-8") as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)

        logger.info(f"\nğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {results_file}")

    def display_comprehensive_summary(self, summary: Dict[str, Any]):
        """æ˜¾ç¤ºç»¼åˆæµ‹è¯•æ±‡æ€»"""
        logger.info("\n" + "=" * 70)
        logger.info("ğŸ“Š çœŸå®æ•°æ®é›† LLM AS A JUDGE æµ‹è¯•æ±‡æ€»")
        logger.info("=" * 70)

        test_info = summary["test_info"]
        providers = summary["providers"]
        results = summary["overall_results"]
        type_analysis = summary["type_analysis"]
        source_dist = summary["source_distribution"]
        performance = summary["performance_metrics"]

        # åŸºæœ¬ä¿¡æ¯
        logger.info(f"ğŸ•’ æµ‹è¯•æ—¶é—´: {test_info['timestamp']}")
        logger.info(f"ğŸ“Š æµ‹è¯•æ ·æœ¬æ•°: {test_info['total_samples']}")
        logger.info(f"â±ï¸ æ€»è€—æ—¶: {test_info['total_time']}ç§’")
        logger.info(f"ğŸ¯ å¹³å‡ç½®ä¿¡åº¦: {test_info['avg_confidence']}")

        # æä¾›å•†ä¿¡æ¯
        logger.info(f"\nğŸ¤– æµ‹è¯•æä¾›å•†:")
        logger.info(f"  Provider A: {providers['provider_a']}")
        logger.info(f"  Provider B: {providers['provider_b']}")

        # æ€»ä½“ç»“æœ
        logger.info(f"\nğŸ† æ€»ä½“æ¯”èµ›ç»“æœ:")
        logger.info(
            f"  Provider A è·èƒœ: {results['provider_a_wins']} æ¬¡ ({results['win_rate_a']:.1%})"
        )
        logger.info(
            f"  Provider B è·èƒœ: {results['provider_b_wins']} æ¬¡ ({results['win_rate_b']:.1%})"
        )
        logger.info(f"  å¹³å±€: {results['ties']} æ¬¡")

        # æŒ‰é—®é¢˜ç±»å‹åˆ†æ
        logger.info(f"\nğŸ“‹ æŒ‰é—®é¢˜ç±»å‹åˆ†æ:")
        for q_type, type_data in type_analysis.items():
            total = type_data["total"]
            logger.info(f"  {q_type.upper()}ç±»å‹ (å…±{total}é¢˜):")
            logger.info(f"    Provider A: {type_data['A']} èƒœ ({type_data['A']/total:.1%})")
            logger.info(f"    Provider B: {type_data['B']} èƒœ ({type_data['B']/total:.1%})")
            if type_data["Tie"] > 0:
                logger.info(f"    å¹³å±€: {type_data['Tie']} æ¬¡")

        # æ•°æ®é›†åˆ†å¸ƒ
        logger.info(f"\nğŸ“š æ•°æ®é›†åˆ†å¸ƒ:")
        for source, count in source_dist.items():
            logger.info(f"  {source}: {count} ä¸ªæ ·æœ¬")

        # æ€§èƒ½æŒ‡æ ‡
        logger.info(f"\nâš¡ æ€§èƒ½æŒ‡æ ‡:")
        logger.info(f"  å¹³å‡å“åº”æ—¶é—´: {performance['avg_response_time']}ç§’")
        logger.info(f"  å¹³å‡è¯„ä¼°æ—¶é—´: {performance['avg_evaluation_time']}ç§’")

        # ç»“è®ºå’Œå»ºè®®
        logger.info(f"\nğŸ¯ æµ‹è¯•ç»“è®º:")
        if results["provider_a_wins"] > results["provider_b_wins"]:
            winner_name = providers["provider_a"]
            win_rate = results["win_rate_a"]
        elif results["provider_b_wins"] > results["provider_a_wins"]:
            winner_name = providers["provider_b"]
            win_rate = results["win_rate_b"]
        else:
            winner_name = "å¹³å±€"
            win_rate = 0.5

        if win_rate >= 0.7:
            conclusion = f"  ğŸ‰ {winner_name} è¡¨ç°æ˜¾è‘—ä¼˜ç§€ (èƒœç‡: {win_rate:.1%})"
        elif win_rate >= 0.6:
            conclusion = f"  âœ… {winner_name} è¡¨ç°è¾ƒå¥½ (èƒœç‡: {win_rate:.1%})"
        else:
            conclusion = f"  âš–ï¸ ä¸¤ä¸ªæ¨¡å‹è¡¨ç°ç›¸è¿‘ï¼Œå„æœ‰ä¼˜åŠ¿"

        logger.info(conclusion)
        logger.info(f"  ğŸ’¡ å»ºè®®: æ ¹æ®å…·ä½“åº”ç”¨åœºæ™¯é€‰æ‹©åˆé€‚çš„æ¨¡å‹")


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    logger.info("ğŸ¯ çœŸå®æ•°æ®é›† LLM as a Judge è¯„ä¼°æµ‹è¯•")
    logger.info("æœ¬æµ‹è¯•ä½¿ç”¨ä¸‹è½½çš„ARC-Easyå’ŒGSM8Kæ•°æ®é›†è¿›è¡ŒçœŸå®è¯„ä¼°")

    # åˆ›å»ºå¹¶è¿è¡Œæµ‹è¯•
    tester = RealDatasetTestRunner()
    summary = await tester.run_comprehensive_test(sample_size=6)

    if summary:
        # æ˜¾ç¤ºæ±‡æ€»ç»“æœ
        tester.display_comprehensive_summary(summary)
        logger.info("\nâœ… çœŸå®æ•°æ®é›† LLM as a Judge æµ‹è¯•å®Œæˆï¼")
    else:
        logger.error("âŒ æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®é›†æ–‡ä»¶")

    return summary


if __name__ == "__main__":
    asyncio.run(main())
