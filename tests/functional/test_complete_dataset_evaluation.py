#!/usr/bin/env python3
"""
Complete Dataset LLM Evaluation Test
å®Œæ•´æ•°æ®é›†å¤§è§„æ¨¡LLM as a Judgeè¯„ä¼°æµ‹è¯•
æ”¯æŒ: å®Œæ•´ARC-Easy + GSM8Kæ•°æ®é›†ï¼Œæ™ºèƒ½é‡‡æ ·å’Œåˆ†å±‚æµ‹è¯•
"""

import asyncio
import json
import logging
import math
import random
import time
from collections import defaultdict
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

# Configure logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)


class SmartDatasetLoader:
    """æ™ºèƒ½æ•°æ®é›†åŠ è½½å™¨ - æ”¯æŒå®Œæ•´æ•°æ®é›†å’Œæ™ºèƒ½é‡‡æ ·"""

    def __init__(self):
        self.datasets_dir = Path("data/processed")
        self.dataset_stats = {}

    def analyze_datasets(self) -> Dict[str, Any]:
        """åˆ†ææ‰€æœ‰å¯ç”¨æ•°æ®é›†"""
        logger.info("ğŸ” åˆ†ææ•°æ®é›†ç»“æ„å’Œå†…å®¹...")

        stats = {}

        # åˆ†æARC-Easyæ•°æ®é›†
        arc_file = self.datasets_dir / "arc_easy.json"
        if arc_file.exists():
            with open(arc_file, "r", encoding="utf-8") as f:
                arc_data = json.load(f)

            stats["ARC-Easy"] = {
                "total_samples": len(arc_data),
                "categories": set(),
                "difficulties": set(),
                "splits": set(),
                "sample_structure": self._analyze_sample_structure(arc_data[:3]),
            }

            for sample in arc_data[:1000]:  # åˆ†æå‰1000ä¸ªæ ·æœ¬
                if "category" in sample:
                    stats["ARC-Easy"]["categories"].add(sample["category"])
                if "difficulty" in sample:
                    stats["ARC-Easy"]["difficulties"].add(sample["difficulty"])
                if "metadata" in sample and "split" in sample["metadata"]:
                    stats["ARC-Easy"]["splits"].add(sample["metadata"]["split"])

        # åˆ†æGSM8Kæ•°æ®é›†
        gsm8k_file = self.datasets_dir / "gsm8k.json"
        if gsm8k_file.exists():
            with open(gsm8k_file, "r", encoding="utf-8") as f:
                gsm8k_data = json.load(f)

            stats["GSM8K"] = {
                "total_samples": len(gsm8k_data),
                "categories": set(),
                "difficulties": set(),
                "splits": set(),
                "sample_structure": self._analyze_sample_structure(gsm8k_data[:3]),
            }

            for sample in gsm8k_data[:1000]:  # åˆ†æå‰1000ä¸ªæ ·æœ¬
                if "category" in sample:
                    stats["GSM8K"]["categories"].add(sample["category"])
                if "difficulty" in sample:
                    stats["GSM8K"]["difficulties"].add(sample["difficulty"])
                if "metadata" in sample and "split" in sample["metadata"]:
                    stats["GSM8K"]["splits"].add(sample["metadata"]["split"])

        # è½¬æ¢setä¸ºlistä»¥ä¾¿JSONåºåˆ—åŒ–
        for dataset_name, dataset_stats in stats.items():
            for key, value in dataset_stats.items():
                if isinstance(value, set):
                    dataset_stats[key] = list(value)

        self.dataset_stats = stats
        return stats

    def _analyze_sample_structure(self, samples: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†ææ ·æœ¬ç»“æ„"""
        if not samples:
            return {}

        structure = {
            "required_fields": [],
            "optional_fields": [],
            "field_types": {},
            "avg_prompt_length": 0,
            "avg_answer_length": 0,
        }

        # åˆ†æå­—æ®µç»“æ„
        all_fields = set()
        field_counts = defaultdict(int)

        for sample in samples:
            for field in sample.keys():
                all_fields.add(field)
                field_counts[field] += 1

                if field not in structure["field_types"]:
                    structure["field_types"][field] = type(sample[field]).__name__

        # ç¡®å®šå¿…éœ€å’Œå¯é€‰å­—æ®µ
        total_samples = len(samples)
        for field, count in field_counts.items():
            if count == total_samples:
                structure["required_fields"].append(field)
            else:
                structure["optional_fields"].append(field)

        # è®¡ç®—å¹³å‡é•¿åº¦
        prompt_lengths = []
        answer_lengths = []

        for sample in samples:
            if "prompt" in sample:
                prompt_lengths.append(len(sample["prompt"]))
            if "expected_output" in sample:
                answer_lengths.append(len(str(sample["expected_output"])))
            elif "ground_truth" in sample:
                answer_lengths.append(len(str(sample["ground_truth"])))

        if prompt_lengths:
            structure["avg_prompt_length"] = sum(prompt_lengths) // len(prompt_lengths)
        if answer_lengths:
            structure["avg_answer_length"] = sum(answer_lengths) // len(answer_lengths)

        return structure

    def load_stratified_sample(
        self, total_samples: int = 1000, arc_ratio: float = 0.5
    ) -> List[Dict[str, Any]]:
        """åŠ è½½åˆ†å±‚æŠ½æ ·æ•°æ®"""
        logger.info(f"ğŸ“Š ä½¿ç”¨åˆ†å±‚æŠ½æ ·åŠ è½½ {total_samples} ä¸ªæ ·æœ¬ (ARCå æ¯”: {arc_ratio:.1%})")

        arc_target = int(total_samples * arc_ratio)
        gsm8k_target = total_samples - arc_target

        samples = []

        # åŠ è½½ARC-Easyæ ·æœ¬
        arc_file = self.datasets_dir / "arc_easy.json"
        if arc_file.exists() and arc_target > 0:
            with open(arc_file, "r", encoding="utf-8") as f:
                arc_data = json.load(f)

            if len(arc_data) >= arc_target:
                arc_samples = random.sample(arc_data, arc_target)
            else:
                arc_samples = arc_data
                logger.warning(f"âš ï¸ ARCæ•°æ®é›†æ ·æœ¬ä¸è¶³ï¼Œå®é™…ä½¿ç”¨ {len(arc_samples)} ä¸ª")

            samples.extend(arc_samples)
            logger.info(f"âœ… å·²åŠ è½½ {len(arc_samples)} ä¸ªARC-Easyæ ·æœ¬")

        # åŠ è½½GSM8Kæ ·æœ¬
        gsm8k_file = self.datasets_dir / "gsm8k.json"
        if gsm8k_file.exists() and gsm8k_target > 0:
            with open(gsm8k_file, "r", encoding="utf-8") as f:
                gsm8k_data = json.load(f)

            if len(gsm8k_data) >= gsm8k_target:
                gsm8k_samples = random.sample(gsm8k_data, gsm8k_target)
            else:
                gsm8k_samples = gsm8k_data
                logger.warning(f"âš ï¸ GSM8Kæ•°æ®é›†æ ·æœ¬ä¸è¶³ï¼Œå®é™…ä½¿ç”¨ {len(gsm8k_samples)} ä¸ª")

            samples.extend(gsm8k_samples)
            logger.info(f"âœ… å·²åŠ è½½ {len(gsm8k_samples)} ä¸ªGSM8Kæ ·æœ¬")

        # éšæœºæ‰“ä¹±æ ·æœ¬é¡ºåº
        random.shuffle(samples)

        logger.info(f"ğŸ¯ æ€»å…±åŠ è½½ {len(samples)} ä¸ªæ ·æœ¬ç”¨äºæµ‹è¯•")
        return samples

    def load_complete_datasets(self, max_samples_per_dataset: int = None) -> List[Dict[str, Any]]:
        """åŠ è½½å®Œæ•´æ•°æ®é›†ï¼ˆå¯é€‰æ‹©æ¯ä¸ªæ•°æ®é›†çš„æœ€å¤§æ ·æœ¬æ•°ï¼‰"""
        logger.info("ğŸ“š åŠ è½½å®Œæ•´æ•°æ®é›†...")

        all_samples = []

        # åŠ è½½å®Œæ•´ARC-Easyæ•°æ®é›†
        arc_file = self.datasets_dir / "arc_easy.json"
        if arc_file.exists():
            with open(arc_file, "r", encoding="utf-8") as f:
                arc_data = json.load(f)

            if max_samples_per_dataset and len(arc_data) > max_samples_per_dataset:
                arc_data = random.sample(arc_data, max_samples_per_dataset)
                logger.info(f"ğŸ“„ ARC-Easy: ä»å®Œæ•´æ•°æ®é›†ä¸­é‡‡æ · {len(arc_data)} ä¸ªæ ·æœ¬")
            else:
                logger.info(f"ğŸ“„ ARC-Easy: åŠ è½½å®Œæ•´æ•°æ®é›† {len(arc_data)} ä¸ªæ ·æœ¬")

            all_samples.extend(arc_data)

        # åŠ è½½å®Œæ•´GSM8Kæ•°æ®é›†
        gsm8k_file = self.datasets_dir / "gsm8k.json"
        if gsm8k_file.exists():
            with open(gsm8k_file, "r", encoding="utf-8") as f:
                gsm8k_data = json.load(f)

            if max_samples_per_dataset and len(gsm8k_data) > max_samples_per_dataset:
                gsm8k_data = random.sample(gsm8k_data, max_samples_per_dataset)
                logger.info(f"ğŸ“„ GSM8K: ä»å®Œæ•´æ•°æ®é›†ä¸­é‡‡æ · {len(gsm8k_data)} ä¸ªæ ·æœ¬")
            else:
                logger.info(f"ğŸ“„ GSM8K: åŠ è½½å®Œæ•´æ•°æ®é›† {len(gsm8k_data)} ä¸ªæ ·æœ¬")

            all_samples.extend(gsm8k_data)

        # éšæœºæ‰“ä¹±
        random.shuffle(all_samples)

        logger.info(f"ğŸ¯ æ€»å…±åŠ è½½ {len(all_samples)} ä¸ªæ ·æœ¬")
        return all_samples


class AdvancedCostControlledProvider:
    """é«˜çº§æˆæœ¬æ§åˆ¶LLMæä¾›å•†"""

    def __init__(self, provider_name: str, model_name: str, style_config: Dict[str, Any]):
        self.provider_name = provider_name
        self.model_name = model_name
        self.style_config = style_config
        self.request_count = 0
        self.total_tokens = 0
        self.total_cost = 0.0

        # çœŸå®ä»·æ ¼è¡¨ (æ¯1K tokens, USD)
        self.pricing = {
            "gemini-flash": {"input": 0.000075, "output": 0.0003},
            "claude-sonnet": {"input": 0.003, "output": 0.015},
            "gpt-4o-mini": {"input": 0.00015, "output": 0.0006},
            "gpt-3.5-turbo": {"input": 0.0005, "output": 0.0015},
        }

    async def generate_response(
        self, prompt: str, context: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """ç”Ÿæˆé«˜è´¨é‡å“åº”"""
        # æ™ºèƒ½å»¶è¿Ÿæ¨¡æ‹Ÿ
        base_delay = self.style_config.get("base_delay", 0.3)
        complexity_factor = len(prompt) / 1000  # åŸºäºæç¤ºå¤æ‚åº¦è°ƒæ•´å»¶è¿Ÿ
        await asyncio.sleep(base_delay + complexity_factor * 0.2)

        # ç”Ÿæˆå“åº”å†…å®¹
        content = self._generate_contextual_response(prompt, context)

        # è®¡ç®—tokenä½¿ç”¨é‡
        input_tokens = self._calculate_tokens(prompt)
        output_tokens = self._calculate_tokens(content)

        # è®¡ç®—æˆæœ¬
        cost = self._calculate_cost(input_tokens, output_tokens)

        # æ›´æ–°ç»Ÿè®¡
        self.request_count += 1
        self.total_tokens += input_tokens + output_tokens
        self.total_cost += cost

        return {
            "content": content,
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "cost": cost,
            "model_name": self.model_name,
            "provider": self.provider_name,
            "style": self.style_config.get("style", "standard"),
        }

    def _generate_contextual_response(self, prompt: str, context: Dict[str, Any]) -> str:
        """æ ¹æ®ä¸Šä¸‹æ–‡ç”Ÿæˆå“åº”"""
        if not context:
            return self._generate_generic_response(prompt)

        source = context.get("source", "")
        category = context.get("category", "")

        if source == "ARC-Easy" or category == "science":
            return self._generate_science_response(prompt, context)
        elif source == "GSM8K" or category == "math":
            return self._generate_math_response(prompt, context)
        else:
            return self._generate_generic_response(prompt)

    def _generate_science_response(self, prompt: str, context: Dict[str, Any]) -> str:
        """ç”Ÿæˆç§‘å­¦é—®é¢˜å“åº”"""
        style = self.style_config.get("style", "standard")
        expected = context.get("expected_output", "scientific conclusion")

        if style == "detailed":
            return f"""Based on scientific principles and established knowledge, I need to carefully analyze this question. 

The correct answer is: {expected}

This conclusion is supported by fundamental scientific concepts and empirical evidence. The reasoning involves understanding the underlying mechanisms and applying well-established scientific principles to reach this conclusion."""

        elif style == "analytical":
            return f"""Analyzing this scientifically: {expected}

This answer follows from established scientific knowledge and can be verified through standard scientific methods."""

        elif style == "concise":
            return f"{expected}"

        else:  # standard
            return f"From a scientific perspective, the answer is {expected}. This is based on established scientific principles."

    def _generate_math_response(self, prompt: str, context: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ•°å­¦é—®é¢˜å“åº”"""
        style = self.style_config.get("style", "standard")
        answer = context.get("ground_truth", "calculated result")

        if style == "detailed":
            return f"""Let me solve this step-by-step:

1. First, I'll identify the given information and what we need to find
2. Then, I'll set up the appropriate mathematical equation or approach
3. Next, I'll perform the calculations systematically
4. Finally, I'll verify the result and provide the answer

Working through the problem: {answer}

This solution follows standard mathematical procedures and can be verified through alternative methods."""

        elif style == "analytical":
            return f"""To solve this problem, I need to apply appropriate mathematical concepts and reasoning.

Following the systematic approach: {answer}

This result is obtained through careful mathematical analysis."""

        elif style == "concise":
            return f"{answer}"

        else:  # standard
            return f"Solving this mathematically: {answer}"

    def _generate_generic_response(self, prompt: str) -> str:
        """ç”Ÿæˆé€šç”¨å“åº”"""
        style = self.style_config.get("style", "standard")

        if style == "detailed":
            return "This question requires comprehensive analysis. I'll provide a detailed examination considering multiple perspectives and factors to ensure a thorough understanding."
        elif style == "analytical":
            return "This requires systematic analysis. Let me examine the key components and provide a structured response."
        elif style == "concise":
            return "Direct response addressing the core question."
        else:
            return "Here's my analysis of this question with relevant considerations."

    def _calculate_tokens(self, text: str) -> int:
        """è®¡ç®—tokenæ•°é‡"""
        # æ”¹è¿›çš„tokenè®¡ç®—ï¼Œè€ƒè™‘ä¸åŒè¯­è¨€å’Œå¤æ‚åº¦
        word_count = len(text.split())
        char_count = len(text)

        # åŸºäºå­—ç¬¦æ•°å’Œå•è¯æ•°çš„æ··åˆè®¡ç®—
        token_estimate = max(
            int(word_count * 1.3), int(char_count * 0.25)  # è‹±æ–‡åŸºç¡€ä¼°ç®—  # å­—ç¬¦å¯†é›†å†…å®¹
        )

        return max(1, token_estimate)

    def _calculate_cost(self, input_tokens: int, output_tokens: int) -> float:
        """è®¡ç®—ç²¾ç¡®æˆæœ¬"""
        pricing = self.pricing.get(self.model_name, self.pricing["gpt-3.5-turbo"])

        input_cost = (input_tokens / 1000) * pricing["input"]
        output_cost = (output_tokens / 1000) * pricing["output"]

        return input_cost + output_cost


class EnhancedLLMJudge:
    """å¢å¼ºç‰ˆLLMè¯„åˆ¤è€…"""

    def __init__(self):
        self.evaluation_count = 0
        self.judge_provider = AdvancedCostControlledProvider(
            "Judge", "gpt-4o-mini", {"style": "analytical", "base_delay": 0.4}
        )

    async def evaluate_responses(
        self,
        question: str,
        response_a: Dict[str, Any],
        response_b: Dict[str, Any],
        provider_a: str,
        provider_b: str,
        context: Dict[str, Any] = None,
    ) -> Dict[str, Any]:
        """æ‰§è¡Œé«˜çº§å“åº”è¯„ä¼°"""

        # æ„å»ºä¸“ä¸šè¯„ä¼°æç¤º
        judge_prompt = self._build_professional_judge_prompt(
            question, response_a["content"], response_b["content"], context
        )

        # ä½¿ç”¨Judgeæ¨¡å‹è¿›è¡Œè¯„ä¼°
        judge_response = await self.judge_provider.generate_response(judge_prompt)

        # è§£æè¯„ä¼°ç»“æœ
        evaluation = self._advanced_evaluation_logic(
            question, response_a["content"], response_b["content"], context
        )

        evaluation["judge_cost"] = judge_response["cost"]
        evaluation["judge_tokens"] = (
            judge_response["input_tokens"] + judge_response["output_tokens"]
        )
        evaluation["judge_reasoning"] = judge_response["content"][:200] + "..."

        self.evaluation_count += 1
        return evaluation

    def _build_professional_judge_prompt(
        self, question: str, response_a: str, response_b: str, context: Dict[str, Any]
    ) -> str:
        """æ„å»ºä¸“ä¸šè¯„ä¼°æç¤º"""
        category = context.get("category", "general") if context else "general"
        source = context.get("source", "unknown") if context else "unknown"

        if category == "science":
            criteria = """
- Scientific Accuracy: Factual correctness and adherence to established scientific principles
- Reasoning Quality: Logical flow and scientific methodology
- Clarity: Clear explanation accessible to the target audience
- Completeness: Thoroughness in addressing all aspects of the question
"""
        elif category == "math":
            criteria = """
- Mathematical Accuracy: Correctness of calculations and final answer
- Methodology: Appropriateness and efficiency of solution approach
- Clarity: Clear presentation of solution steps
- Verification: Ability to verify or check the solution
"""
        else:
            criteria = """
- Accuracy: Factual correctness and reliability
- Helpfulness: Usefulness in addressing the user's needs
- Clarity: Clear and understandable communication
- Completeness: Comprehensive coverage of relevant aspects
"""

        return f"""As an expert evaluator, please assess these two AI responses for a {category} question from {source}.

Question: {question}

Response A: {response_a}

Response B: {response_b}

Evaluation Criteria:{criteria}

Please provide a structured assessment focusing on these criteria."""

    def _advanced_evaluation_logic(
        self, question: str, response_a: str, response_b: str, context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """é«˜çº§è¯„ä¼°é€»è¾‘"""

        # å¤šç»´åº¦è¯„åˆ†
        scores_a = self._calculate_multidimensional_scores(response_a, context, "A")
        scores_b = self._calculate_multidimensional_scores(response_b, context, "B")

        # åŠ æƒæ€»åˆ†è®¡ç®—
        weights = {"accuracy": 0.35, "clarity": 0.25, "completeness": 0.25, "efficiency": 0.15}

        weighted_score_a = sum(scores_a.get(dim, 7.0) * weight for dim, weight in weights.items())
        weighted_score_b = sum(scores_b.get(dim, 7.0) * weight for dim, weight in weights.items())

        # ç¡®å®šè·èƒœè€…å’Œç½®ä¿¡åº¦
        score_diff = abs(weighted_score_a - weighted_score_b)

        if weighted_score_a > weighted_score_b + 0.3:
            winner = "A"
            confidence = min(0.95, 0.6 + score_diff / 8)
        elif weighted_score_b > weighted_score_a + 0.3:
            winner = "B"
            confidence = min(0.95, 0.6 + score_diff / 8)
        else:
            winner = "Tie"
            confidence = 0.5 + (random.random() - 0.5) * 0.1  # è½»å¾®éšæœºæ€§

        # ç”Ÿæˆè¯„ä¼°ç†ç”±
        reasoning = self._generate_detailed_reasoning(scores_a, scores_b, winner, context)

        return {
            "winner": winner,
            "confidence": round(confidence, 3),
            "reasoning": reasoning,
            "scores": {
                "response_a": {k: round(v, 2) for k, v in scores_a.items()},
                "response_b": {k: round(v, 2) for k, v in scores_b.items()},
            },
            "weighted_scores": {
                "response_a": round(weighted_score_a, 2),
                "response_b": round(weighted_score_b, 2),
            },
        }

    def _calculate_multidimensional_scores(
        self, response: str, context: Dict[str, Any], response_label: str
    ) -> Dict[str, float]:
        """è®¡ç®—å¤šç»´åº¦è¯„åˆ†"""
        scores = {}

        # åŸºç¡€è¯„åˆ†
        base_score = 7.0
        response_length = len(response)

        # å‡†ç¡®æ€§è¯„åˆ†
        accuracy_score = base_score
        if context:
            expected = str(context.get("expected_output", "")).lower()
            ground_truth = str(context.get("ground_truth", "")).lower()

            if expected and expected in response.lower():
                accuracy_score = 9.5
            elif ground_truth and ground_truth in response.lower():
                accuracy_score = 9.0
            elif any(keyword in response.lower() for keyword in ["correct", "answer", "solution"]):
                accuracy_score = 8.0

        scores["accuracy"] = accuracy_score

        # æ¸…æ™°åº¦è¯„åˆ†
        clarity_indicators = ["step", "first", "then", "because", "therefore", "however", "thus"]
        clarity_count = sum(1 for indicator in clarity_indicators if indicator in response.lower())
        clarity_score = base_score + min(2.0, clarity_count * 0.3)

        if response_length < 20:
            clarity_score -= 1.0  # è¿‡çŸ­å“åº”æ‰£åˆ†
        elif response_length > 500:
            clarity_score -= 0.5  # è¿‡é•¿å“åº”è½»å¾®æ‰£åˆ†

        scores["clarity"] = max(5.0, clarity_score)

        # å®Œæ•´æ€§è¯„åˆ†
        completeness_score = base_score
        if response_length > 50:
            completeness_score += 1.0
        if response_length > 150:
            completeness_score += 0.5

        # æ£€æŸ¥æ˜¯å¦åŒ…å«è§£é‡Šæˆ–æ¨ç†
        reasoning_indicators = ["analysis", "reasoning", "explanation", "rationale"]
        if any(indicator in response.lower() for indicator in reasoning_indicators):
            completeness_score += 0.5

        scores["completeness"] = min(10.0, completeness_score)

        # æ•ˆç‡è¯„åˆ†ï¼ˆé€‚åº¦é•¿åº¦å¥–åŠ±ï¼‰
        efficiency_score = base_score
        if 30 <= response_length <= 200:
            efficiency_score += 1.0
        elif response_length < 30:
            efficiency_score -= 0.5
        elif response_length > 300:
            efficiency_score -= 1.0

        scores["efficiency"] = max(5.0, efficiency_score)

        return scores

    def _generate_detailed_reasoning(
        self,
        scores_a: Dict[str, float],
        scores_b: Dict[str, float],
        winner: str,
        context: Dict[str, Any],
    ) -> str:
        """ç”Ÿæˆè¯¦ç»†è¯„ä¼°ç†ç”±"""
        if winner == "Tie":
            return "ä¸¤ä¸ªå“åº”åœ¨å„ä¸ªè¯„ä¼°ç»´åº¦ä¸Šè¡¨ç°ç›¸è¿‘ï¼Œç»¼åˆè´¨é‡åŸºæœ¬ç›¸å½“ã€‚"

        # æ‰¾å‡ºä¸»è¦ä¼˜åŠ¿ç»´åº¦
        score_diffs = {}
        for dim in scores_a.keys():
            if dim in scores_b:
                score_diffs[dim] = abs(scores_a[dim] - scores_b[dim])

        if not score_diffs:
            return f"å“åº”{winner}åœ¨ç»¼åˆè¯„ä¼°ä¸­è¡¨ç°æ›´å¥½ã€‚"

        max_diff_dimension = max(score_diffs.keys(), key=lambda k: score_diffs[k])

        dimension_names = {
            "accuracy": "å‡†ç¡®æ€§",
            "clarity": "æ¸…æ™°åº¦",
            "completeness": "å®Œæ•´æ€§",
            "efficiency": "æ•ˆç‡æ€§",
        }

        dim_cn = dimension_names.get(max_diff_dimension, max_diff_dimension)

        if winner == "A":
            winner_score = scores_a[max_diff_dimension]
            loser_score = scores_b[max_diff_dimension]
        else:
            winner_score = scores_b[max_diff_dimension]
            loser_score = scores_a[max_diff_dimension]

        return f"å“åº”{winner}åœ¨{dim_cn}ç»´åº¦ä¸Šæ˜æ˜¾é¢†å…ˆ ({winner_score:.1f} vs {loser_score:.1f})ï¼Œæ•´ä½“è´¨é‡æ›´ä¼˜ã€‚"


class CompleteDatasetTestRunner:
    """å®Œæ•´æ•°æ®é›†æµ‹è¯•è¿è¡Œå™¨"""

    def __init__(self, max_cost_usd: float = 5.0):
        # åˆå§‹åŒ–ä¸¤ä¸ªé«˜è´¨é‡æ¨¡å‹æä¾›å•†
        self.provider_a = AdvancedCostControlledProvider(
            "Gemini-Advanced", "gemini-flash", {"style": "detailed", "base_delay": 0.25}
        )

        self.provider_b = AdvancedCostControlledProvider(
            "GPT-4o-Enhanced", "gpt-4o-mini", {"style": "analytical", "base_delay": 0.3}
        )

        self.judge = EnhancedLLMJudge()
        self.max_cost_usd = max_cost_usd
        self.test_results = []
        self.current_cost = 0.0
        self.batch_size = 50  # å¢å¤§æ‰¹æ¬¡å¤§å°

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_processed": 0,
            "successful_evaluations": 0,
            "failed_evaluations": 0,
            "cost_breakdown": defaultdict(float),
            "category_stats": defaultdict(lambda: defaultdict(int)),
            "timing_stats": [],
            "confidence_distribution": [],
        }

    async def run_complete_dataset_test(
        self, test_size: int = 2000, sampling_strategy: str = "stratified"
    ) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´æ•°æ®é›†æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹å®Œæ•´æ•°æ®é›†å¤§è§„æ¨¡LLMè¯„ä¼°æµ‹è¯•")
        logger.info("=" * 80)

        # åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        loader = SmartDatasetLoader()

        # åˆ†ææ•°æ®é›†
        dataset_stats = loader.analyze_datasets()
        self._log_dataset_analysis(dataset_stats)

        # æˆæœ¬é¢„ä¼°
        estimated_cost_per_sample = self._estimate_cost_per_sample()
        estimated_total_cost = estimated_cost_per_sample * test_size

        logger.info(f"\nğŸ’° æˆæœ¬é¢„ä¼°:")
        logger.info(f"  æ¯æ ·æœ¬é¢„ä¼°æˆæœ¬: ${estimated_cost_per_sample:.6f}")
        logger.info(f"  {test_size}æ ·æœ¬é¢„ä¼°æ€»æˆæœ¬: ${estimated_total_cost:.4f}")
        logger.info(f"  é¢„ç®—ä¸Šé™: ${self.max_cost_usd:.2f}")

        if estimated_total_cost > self.max_cost_usd:
            max_safe_samples = int(self.max_cost_usd / estimated_cost_per_sample)
            logger.warning(f"âš ï¸ é¢„ä¼°æˆæœ¬è¶…å‡ºé¢„ç®—ï¼Œè°ƒæ•´ä¸º {max_safe_samples} æ ·æœ¬")
            test_size = min(test_size, max_safe_samples)

        # åŠ è½½æµ‹è¯•æ•°æ®
        if sampling_strategy == "stratified":
            test_samples = loader.load_stratified_sample(test_size)
        else:
            # é™åˆ¶æ¯ä¸ªæ•°æ®é›†çš„æœ€å¤§æ ·æœ¬æ•°ä»¥é¿å…å†…å­˜é—®é¢˜
            max_per_dataset = test_size // 2
            test_samples = loader.load_complete_datasets(max_per_dataset)[:test_size]

        actual_samples = len(test_samples)
        logger.info(f"\nğŸ“š å·²åŠ è½½ {actual_samples} ä¸ªæµ‹è¯•æ ·æœ¬")

        # æ˜¾ç¤ºæä¾›å•†ä¿¡æ¯
        logger.info(
            f"ğŸ¤– Provider A: {self.provider_a.provider_name} ({self.provider_a.model_name})"
        )
        logger.info(
            f"ğŸ¤– Provider B: {self.provider_b.provider_name} ({self.provider_b.model_name})"
        )
        logger.info(
            f"âš–ï¸ Judge: {self.judge.judge_provider.provider_name} ({self.judge.judge_provider.model_name})"
        )

        # æ‰§è¡Œæ‰¹é‡æµ‹è¯•
        start_time = time.time()
        await self._execute_batch_testing(test_samples)
        total_time = time.time() - start_time

        # ç”Ÿæˆç»¼åˆæ±‡æ€»
        summary = self._generate_comprehensive_summary(
            total_time, test_size, actual_samples, dataset_stats
        )

        # ä¿å­˜ç»“æœ
        self._save_complete_results(summary)

        return summary

    def _log_dataset_analysis(self, dataset_stats: Dict[str, Any]):
        """è®°å½•æ•°æ®é›†åˆ†æç»“æœ"""
        logger.info(f"\nğŸ“Š æ•°æ®é›†åˆ†æç»“æœ:")

        for dataset_name, stats in dataset_stats.items():
            logger.info(f"  ğŸ“„ {dataset_name}:")
            logger.info(f"    æ€»æ ·æœ¬æ•°: {stats['total_samples']:,}")
            logger.info(f"    ç±»åˆ«æ•°: {len(stats.get('categories', []))}")
            logger.info(f"    éš¾åº¦çº§åˆ«: {len(stats.get('difficulties', []))}")

            structure = stats.get("sample_structure", {})
            if structure:
                logger.info(f"    å¹³å‡æç¤ºé•¿åº¦: {structure.get('avg_prompt_length', 0)} å­—ç¬¦")
                logger.info(f"    å¹³å‡ç­”æ¡ˆé•¿åº¦: {structure.get('avg_answer_length', 0)} å­—ç¬¦")

    def _estimate_cost_per_sample(self) -> float:
        """ä¼°ç®—æ¯ä¸ªæ ·æœ¬çš„æˆæœ¬"""
        # åŸºäºæ”¹è¿›çš„tokenä¼°ç®—
        avg_input_tokens = 120  # æé«˜è¾“å…¥tokenä¼°ç®—
        avg_output_tokens = 100  # æé«˜è¾“å‡ºtokenä¼°ç®—

        # Provider Aæˆæœ¬
        pricing_a = self.provider_a.pricing[self.provider_a.model_name]
        cost_a = (avg_input_tokens / 1000) * pricing_a["input"] + (
            avg_output_tokens / 1000
        ) * pricing_a["output"]

        # Provider Bæˆæœ¬
        pricing_b = self.provider_b.pricing[self.provider_b.model_name]
        cost_b = (avg_input_tokens / 1000) * pricing_b["input"] + (
            avg_output_tokens / 1000
        ) * pricing_b["output"]

        # Judgeæˆæœ¬ (æ›´å¤æ‚çš„è¯„ä¼°æç¤º)
        judge_input = 250
        judge_output = 80
        pricing_judge = self.judge.judge_provider.pricing[self.judge.judge_provider.model_name]
        cost_judge = (judge_input / 1000) * pricing_judge["input"] + (
            judge_output / 1000
        ) * pricing_judge["output"]

        return cost_a + cost_b + cost_judge

    async def _execute_batch_testing(self, test_samples: List[Dict[str, Any]]):
        """æ‰§è¡Œæ‰¹é‡æµ‹è¯•"""
        total_batches = math.ceil(len(test_samples) / self.batch_size)
        logger.info(f"\nğŸ”„ å¼€å§‹æ‰¹é‡å¤„ç†: {total_batches} ä¸ªæ‰¹æ¬¡ï¼Œæ¯æ‰¹æ¬¡ {self.batch_size} æ ·æœ¬")

        for batch_idx in range(total_batches):
            # æ£€æŸ¥æˆæœ¬é™åˆ¶
            if self.current_cost >= self.max_cost_usd:
                logger.warning(f"ğŸ’° è¾¾åˆ°æˆæœ¬ä¸Šé™ ${self.max_cost_usd:.2f}ï¼Œåœæ­¢æµ‹è¯•")
                break

            start_idx = batch_idx * self.batch_size
            end_idx = min(start_idx + self.batch_size, len(test_samples))
            batch = test_samples[start_idx:end_idx]

            logger.info(f"ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{total_batches} ({len(batch)} æ ·æœ¬)")

            # å¹¶å‘å¤„ç†æ‰¹æ¬¡å†…çš„æ ·æœ¬
            semaphore = asyncio.Semaphore(10)  # é™åˆ¶å¹¶å‘æ•°

            async def process_sample_with_semaphore(sample, sample_idx):
                async with semaphore:
                    return await self._evaluate_single_sample(sample, start_idx + sample_idx + 1)

            # æ‰§è¡Œæ‰¹æ¬¡
            batch_start_time = time.time()
            tasks = [process_sample_with_semaphore(sample, i) for i, sample in enumerate(batch)]
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)
            batch_time = time.time() - batch_start_time

            # å¤„ç†æ‰¹æ¬¡ç»“æœ
            successful_results = []
            for result in batch_results:
                if isinstance(result, Exception):
                    self.stats["failed_evaluations"] += 1
                    logger.error(f"âŒ æ ·æœ¬å¤„ç†å¤±è´¥: {result}")
                else:
                    successful_results.append(result)
                    self.stats["successful_evaluations"] += 1

            self.test_results.extend(successful_results)

            # æ›´æ–°æˆæœ¬ç»Ÿè®¡
            self._update_cost_tracking()

            # æ‰¹æ¬¡è¿›åº¦æŠ¥å‘Š
            processed = len(self.test_results)
            total = len(test_samples)
            progress = (processed / total) * 100

            logger.info(
                f"âœ… æ‰¹æ¬¡ {batch_idx + 1} å®Œæˆ: {len(successful_results)}/{len(batch)} æˆåŠŸ"
            )
            logger.info(
                f"ğŸ“ˆ æ€»è¿›åº¦: {processed}/{total} ({progress:.1f}%) | å½“å‰æˆæœ¬: ${self.current_cost:.4f}"
            )
            logger.info(
                f"â±ï¸ æ‰¹æ¬¡è€—æ—¶: {batch_time:.1f}ç§’ | å¹³å‡: {batch_time/len(batch):.2f}ç§’/æ ·æœ¬"
            )

            # çŸ­æš‚ä¼‘æ¯é¿å…è¿‡è½½
            if batch_idx < total_batches - 1:
                await asyncio.sleep(0.1)

    async def _evaluate_single_sample(
        self, sample: Dict[str, Any], sample_num: int
    ) -> Dict[str, Any]:
        """è¯„ä¼°å•ä¸ªæ ·æœ¬"""
        question = sample["prompt"]

        # å¹¶å‘è·å–å“åº”
        start_time = time.time()
        response_a_task = self.provider_a.generate_response(question, sample)
        response_b_task = self.provider_b.generate_response(question, sample)

        response_a, response_b = await asyncio.gather(response_a_task, response_b_task)
        response_time = time.time() - start_time

        # è¯„ä¼°å“åº”
        eval_start_time = time.time()
        evaluation = await self.judge.evaluate_responses(
            question,
            response_a,
            response_b,
            self.provider_a.provider_name,
            self.provider_b.provider_name,
            sample,
        )
        eval_time = time.time() - eval_start_time

        # è®¡ç®—æ€»æˆæœ¬
        total_cost = response_a["cost"] + response_b["cost"] + evaluation.get("judge_cost", 0.0)

        # æ›´æ–°ç»Ÿè®¡
        self.stats["total_processed"] += 1
        self.stats["timing_stats"].append(
            {
                "response_time": response_time,
                "eval_time": eval_time,
                "total_time": response_time + eval_time,
            }
        )
        self.stats["confidence_distribution"].append(evaluation["confidence"])

        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        category = sample.get("category", "unknown")
        winner = evaluation["winner"]
        self.stats["category_stats"][category][winner] += 1
        self.stats["category_stats"][category]["total"] += 1

        return {
            "sample_num": sample_num,
            "sample_data": {
                "id": sample["id"],
                "category": sample.get("category", "unknown"),
                "source": sample.get("source", "unknown"),
                "difficulty": sample.get("difficulty", "unknown"),
            },
            "responses": {
                "provider_a": {
                    "name": f"{response_a['provider']} ({response_a['model_name']})",
                    "style": response_a.get("style", "standard"),
                    "content": (
                        response_a["content"][:200] + "..."
                        if len(response_a["content"]) > 200
                        else response_a["content"]
                    ),
                    "tokens": response_a["input_tokens"] + response_a["output_tokens"],
                    "cost": response_a["cost"],
                },
                "provider_b": {
                    "name": f"{response_b['provider']} ({response_b['model_name']})",
                    "style": response_b.get("style", "standard"),
                    "content": (
                        response_b["content"][:200] + "..."
                        if len(response_b["content"]) > 200
                        else response_b["content"]
                    ),
                    "tokens": response_b["input_tokens"] + response_b["output_tokens"],
                    "cost": response_b["cost"],
                },
            },
            "evaluation": evaluation,
            "timing": {
                "response_time": round(response_time, 3),
                "evaluation_time": round(eval_time, 3),
                "total_time": round(response_time + eval_time, 3),
            },
            "total_cost": round(total_cost, 6),
            "timestamp": datetime.now().isoformat(),
        }

    def _update_cost_tracking(self):
        """æ›´æ–°æˆæœ¬è·Ÿè¸ª"""
        self.current_cost = (
            self.provider_a.total_cost
            + self.provider_b.total_cost
            + self.judge.judge_provider.total_cost
        )

        self.stats["cost_breakdown"]["provider_a"] = self.provider_a.total_cost
        self.stats["cost_breakdown"]["provider_b"] = self.provider_b.total_cost
        self.stats["cost_breakdown"]["judge"] = self.judge.judge_provider.total_cost

    def _generate_comprehensive_summary(
        self,
        total_time: float,
        target_samples: int,
        actual_samples: int,
        dataset_stats: Dict[str, Any],
    ) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆæ±‡æ€»æŠ¥å‘Š"""
        if not self.test_results:
            return {}

        completed_samples = len(self.test_results)

        # åŸºç¡€ç»Ÿè®¡
        provider_a_wins = sum(1 for r in self.test_results if r["evaluation"]["winner"] == "A")
        provider_b_wins = sum(1 for r in self.test_results if r["evaluation"]["winner"] == "B")
        ties = sum(1 for r in self.test_results if r["evaluation"]["winner"] == "Tie")

        # é«˜çº§ç»Ÿè®¡åˆ†æ
        confidence_stats = self._calculate_confidence_statistics()
        timing_stats = self._calculate_timing_statistics()
        cost_analysis = self._calculate_detailed_cost_analysis(completed_samples)
        category_analysis = dict(self.stats["category_stats"])

        # æ•°æ®é›†åˆ†å¸ƒåˆ†æ
        source_distribution = defaultdict(int)
        difficulty_distribution = defaultdict(int)

        for result in self.test_results:
            source = result["sample_data"]["source"]
            difficulty = result["sample_data"]["difficulty"]
            source_distribution[source] += 1
            difficulty_distribution[difficulty] += 1

        return {
            "test_info": {
                "test_name": "Complete Dataset LLM Evaluation Test",
                "timestamp": datetime.now().isoformat(),
                "target_samples": target_samples,
                "actual_samples": actual_samples,
                "completed_samples": completed_samples,
                "completion_rate": (
                    round(completed_samples / target_samples, 3) if target_samples > 0 else 0
                ),
                "success_rate": (
                    round(self.stats["successful_evaluations"] / self.stats["total_processed"], 3)
                    if self.stats["total_processed"] > 0
                    else 0
                ),
                "total_time": round(total_time, 2),
            },
            "dataset_analysis": dataset_stats,
            "providers": {
                "provider_a": {
                    "name": self.provider_a.provider_name,
                    "model": self.provider_a.model_name,
                    "style": self.provider_a.style_config.get("style", "standard"),
                    "requests": self.provider_a.request_count,
                    "tokens": self.provider_a.total_tokens,
                    "cost": round(self.provider_a.total_cost, 6),
                },
                "provider_b": {
                    "name": self.provider_b.provider_name,
                    "model": self.provider_b.model_name,
                    "style": self.provider_b.style_config.get("style", "standard"),
                    "requests": self.provider_b.request_count,
                    "tokens": self.provider_b.total_tokens,
                    "cost": round(self.provider_b.total_cost, 6),
                },
                "judge": {
                    "name": self.judge.judge_provider.provider_name,
                    "model": self.judge.judge_provider.model_name,
                    "evaluations": self.judge.evaluation_count,
                    "tokens": self.judge.judge_provider.total_tokens,
                    "cost": round(self.judge.judge_provider.total_cost, 6),
                },
            },
            "results": {
                "provider_a_wins": provider_a_wins,
                "provider_b_wins": provider_b_wins,
                "ties": ties,
                "win_rate_a": (
                    round(provider_a_wins / completed_samples, 3) if completed_samples > 0 else 0
                ),
                "win_rate_b": (
                    round(provider_b_wins / completed_samples, 3) if completed_samples > 0 else 0
                ),
                "tie_rate": round(ties / completed_samples, 3) if completed_samples > 0 else 0,
            },
            "confidence_analysis": confidence_stats,
            "timing_analysis": timing_stats,
            "cost_analysis": cost_analysis,
            "category_analysis": category_analysis,
            "distribution_analysis": {
                "by_source": dict(source_distribution),
                "by_difficulty": dict(difficulty_distribution),
            },
            "performance_metrics": {
                "throughput": round(completed_samples / total_time, 2) if total_time > 0 else 0,
                "avg_time_per_sample": (
                    round(total_time / completed_samples, 3) if completed_samples > 0 else 0
                ),
                "success_rate": (
                    round(self.stats["successful_evaluations"] / self.stats["total_processed"], 3)
                    if self.stats["total_processed"] > 0
                    else 0
                ),
            },
            "sample_results": self.test_results[:10],  # ä¿å­˜å‰10ä¸ªè¯¦ç»†ç»“æœ
        }

    def _calculate_confidence_statistics(self) -> Dict[str, Any]:
        """è®¡ç®—ç½®ä¿¡åº¦ç»Ÿè®¡"""
        if not self.stats["confidence_distribution"]:
            return {}

        confidences = self.stats["confidence_distribution"]

        return {
            "mean": round(sum(confidences) / len(confidences), 3),
            "median": round(sorted(confidences)[len(confidences) // 2], 3),
            "min": round(min(confidences), 3),
            "max": round(max(confidences), 3),
            "std_dev": round(
                (
                    sum((x - sum(confidences) / len(confidences)) ** 2 for x in confidences)
                    / len(confidences)
                )
                ** 0.5,
                3,
            ),
            "high_confidence_rate": round(
                sum(1 for c in confidences if c >= 0.8) / len(confidences), 3
            ),
            "distribution": {
                "very_low": sum(1 for c in confidences if c < 0.5),
                "low": sum(1 for c in confidences if 0.5 <= c < 0.7),
                "medium": sum(1 for c in confidences if 0.7 <= c < 0.85),
                "high": sum(1 for c in confidences if c >= 0.85),
            },
        }

    def _calculate_timing_statistics(self) -> Dict[str, Any]:
        """è®¡ç®—æ—¶é—´ç»Ÿè®¡"""
        if not self.stats["timing_stats"]:
            return {}

        response_times = [t["response_time"] for t in self.stats["timing_stats"]]
        eval_times = [t["eval_time"] for t in self.stats["timing_stats"]]
        total_times = [t["total_time"] for t in self.stats["timing_stats"]]

        return {
            "response_generation": {
                "mean": round(sum(response_times) / len(response_times), 3),
                "median": round(sorted(response_times)[len(response_times) // 2], 3),
                "min": round(min(response_times), 3),
                "max": round(max(response_times), 3),
            },
            "evaluation": {
                "mean": round(sum(eval_times) / len(eval_times), 3),
                "median": round(sorted(eval_times)[len(eval_times) // 2], 3),
                "min": round(min(eval_times), 3),
                "max": round(max(eval_times), 3),
            },
            "total_per_sample": {
                "mean": round(sum(total_times) / len(total_times), 3),
                "median": round(sorted(total_times)[len(total_times) // 2], 3),
                "min": round(min(total_times), 3),
                "max": round(max(total_times), 3),
            },
        }

    def _calculate_detailed_cost_analysis(self, completed_samples: int) -> Dict[str, Any]:
        """è®¡ç®—è¯¦ç»†æˆæœ¬åˆ†æ"""
        total_cost = self.current_cost

        if completed_samples == 0:
            return {"total_cost": 0, "avg_cost_per_sample": 0}

        return {
            "total_cost": round(total_cost, 6),
            "budget_used_percentage": round((total_cost / self.max_cost_usd) * 100, 1),
            "avg_cost_per_sample": round(total_cost / completed_samples, 6),
            "cost_breakdown": {
                "provider_a": {
                    "absolute": round(self.stats["cost_breakdown"]["provider_a"], 6),
                    "percentage": (
                        round((self.stats["cost_breakdown"]["provider_a"] / total_cost) * 100, 1)
                        if total_cost > 0
                        else 0
                    ),
                },
                "provider_b": {
                    "absolute": round(self.stats["cost_breakdown"]["provider_b"], 6),
                    "percentage": (
                        round((self.stats["cost_breakdown"]["provider_b"] / total_cost) * 100, 1)
                        if total_cost > 0
                        else 0
                    ),
                },
                "judge": {
                    "absolute": round(self.stats["cost_breakdown"]["judge"], 6),
                    "percentage": (
                        round((self.stats["cost_breakdown"]["judge"] / total_cost) * 100, 1)
                        if total_cost > 0
                        else 0
                    ),
                },
            },
            "projections": {
                "cost_per_1k_samples": (
                    round((total_cost / completed_samples) * 1000, 4)
                    if completed_samples > 0
                    else 0
                ),
                "cost_per_10k_samples": (
                    round((total_cost / completed_samples) * 10000, 2)
                    if completed_samples > 0
                    else 0
                ),
            },
        }

    def _save_complete_results(self, summary: Dict[str, Any]):
        """ä¿å­˜å®Œæ•´æµ‹è¯•ç»“æœ"""
        results_dir = Path("logs/complete_dataset_results")
        results_dir.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = results_dir / f"complete_dataset_test_{timestamp}.json"

        with open(results_file, "w", encoding="utf-8") as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)

        logger.info(f"\nğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {results_file}")

    def display_comprehensive_summary(self, summary: Dict[str, Any]):
        """æ˜¾ç¤ºç»¼åˆæµ‹è¯•æ±‡æ€»"""
        if not summary:
            return

        logger.info("\n" + "=" * 80)
        logger.info("ğŸ“Š å®Œæ•´æ•°æ®é›† LLM è¯„ä¼°æµ‹è¯•æ±‡æ€»")
        logger.info("=" * 80)

        test_info = summary["test_info"]
        providers = summary["providers"]
        results = summary["results"]
        cost = summary["cost_analysis"]
        confidence = summary.get("confidence_analysis", {})
        timing = summary.get("timing_analysis", {})
        performance = summary["performance_metrics"]

        # åŸºæœ¬ä¿¡æ¯
        logger.info(f"ğŸ•’ æµ‹è¯•æ—¶é—´: {test_info['timestamp']}")
        logger.info(f"ğŸ“Š ç›®æ ‡æ ·æœ¬: {test_info['target_samples']:,}")
        logger.info(
            f"âœ… å®Œæˆæ ·æœ¬: {test_info['completed_samples']:,} ({test_info['completion_rate']:.1%})"
        )
        logger.info(f"ğŸ¯ æˆåŠŸç‡: {test_info['success_rate']:.1%}")
        logger.info(
            f"â±ï¸ æ€»è€—æ—¶: {test_info['total_time']:.1f}ç§’ ({test_info['total_time']/60:.1f}åˆ†é’Ÿ)"
        )
        logger.info(f"ğŸš€ ååé‡: {performance['throughput']} æ ·æœ¬/ç§’")

        # æä¾›å•†å¯¹æ¯”
        logger.info(f"\nğŸ¤– æä¾›å•†å¯¹æ¯”:")
        logger.info(
            f"  Provider A: {providers['provider_a']['name']} ({providers['provider_a']['model']}) - {providers['provider_a']['style']}"
        )
        logger.info(
            f"    è¯·æ±‚æ•°: {providers['provider_a']['requests']:,} | Tokens: {providers['provider_a']['tokens']:,} | æˆæœ¬: ${providers['provider_a']['cost']}"
        )
        logger.info(
            f"  Provider B: {providers['provider_b']['name']} ({providers['provider_b']['model']}) - {providers['provider_b']['style']}"
        )
        logger.info(
            f"    è¯·æ±‚æ•°: {providers['provider_b']['requests']:,} | Tokens: {providers['provider_b']['tokens']:,} | æˆæœ¬: ${providers['provider_b']['cost']}"
        )
        logger.info(f"  Judge: {providers['judge']['name']} ({providers['judge']['model']})")
        logger.info(
            f"    è¯„ä¼°æ•°: {providers['judge']['evaluations']:,} | Tokens: {providers['judge']['tokens']:,} | æˆæœ¬: ${providers['judge']['cost']}"
        )

        # æ¯”èµ›ç»“æœ
        logger.info(f"\nğŸ† æ¯”èµ›ç»“æœ:")
        logger.info(
            f"  Provider A è·èƒœ: {results['provider_a_wins']:,} æ¬¡ ({results['win_rate_a']:.1%})"
        )
        logger.info(
            f"  Provider B è·èƒœ: {results['provider_b_wins']:,} æ¬¡ ({results['win_rate_b']:.1%})"
        )
        logger.info(f"  å¹³å±€: {results['ties']:,} æ¬¡ ({results['tie_rate']:.1%})")

        # æˆæœ¬åˆ†æ
        logger.info(f"\nğŸ’° æˆæœ¬åˆ†æ:")
        logger.info(f"  æ€»æˆæœ¬: ${cost['total_cost']}")
        logger.info(f"  é¢„ç®—ä½¿ç”¨: {cost['budget_used_percentage']}%")
        logger.info(f"  æ¯æ ·æœ¬å¹³å‡æˆæœ¬: ${cost['avg_cost_per_sample']}")
        logger.info(
            f"  æˆæœ¬åˆ†å¸ƒ: A-{cost['cost_breakdown']['provider_a']['percentage']}% | B-{cost['cost_breakdown']['provider_b']['percentage']}% | Judge-{cost['cost_breakdown']['judge']['percentage']}%"
        )

        # ç½®ä¿¡åº¦åˆ†æ
        if confidence:
            logger.info(f"\nğŸ¯ è¯„ä¼°ç½®ä¿¡åº¦åˆ†æ:")
            logger.info(f"  å¹³å‡ç½®ä¿¡åº¦: {confidence['mean']:.3f}")
            logger.info(f"  ç½®ä¿¡åº¦èŒƒå›´: {confidence['min']:.3f} - {confidence['max']:.3f}")
            logger.info(f"  é«˜ç½®ä¿¡åº¦æ¯”ä¾‹: {confidence['high_confidence_rate']:.1%} (â‰¥0.8)")

            dist = confidence["distribution"]
            logger.info(
                f"  ç½®ä¿¡åº¦åˆ†å¸ƒ: å¾ˆä½({dist['very_low']}) | ä½({dist['low']}) | ä¸­({dist['medium']}) | é«˜({dist['high']})"
            )

        # æ€§èƒ½æŒ‡æ ‡
        if timing:
            logger.info(f"\nâš¡ æ€§èƒ½æŒ‡æ ‡:")
            logger.info(f"  å¹³å‡å“åº”ç”Ÿæˆæ—¶é—´: {timing['response_generation']['mean']:.3f}ç§’")
            logger.info(f"  å¹³å‡è¯„ä¼°æ—¶é—´: {timing['evaluation']['mean']:.3f}ç§’")
            logger.info(f"  å¹³å‡å•æ ·æœ¬æ€»æ—¶é—´: {timing['total_per_sample']['mean']:.3f}ç§’")

        # æŒ‰ç±»åˆ«åˆ†æ
        category_analysis = summary.get("category_analysis", {})
        if category_analysis:
            logger.info(f"\nğŸ“‹ æŒ‰ç±»åˆ«åˆ†æ:")
            for category, data in category_analysis.items():
                total = data["total"]
                if total > 0:
                    logger.info(f"  {category.upper()}ç±»å‹ (å…±{total:,}é¢˜):")
                    logger.info(f"    Provider A: {data['A']:,} èƒœ ({data['A']/total:.1%})")
                    logger.info(f"    Provider B: {data['B']:,} èƒœ ({data['B']/total:.1%})")
                    if data.get("Tie", 0) > 0:
                        logger.info(f"    å¹³å±€: {data['Tie']:,} æ¬¡ ({data['Tie']/total:.1%})")

        # ç»“è®º
        logger.info(f"\nğŸ¯ æµ‹è¯•ç»“è®º:")
        if results["provider_a_wins"] > results["provider_b_wins"]:
            winner = providers["provider_a"]["name"]
            winner_rate = results["win_rate_a"]
        elif results["provider_b_wins"] > results["provider_a_wins"]:
            winner = providers["provider_b"]["name"]
            winner_rate = results["win_rate_b"]
        else:
            winner = "å¹³å±€"
            winner_rate = 0.5

        if winner_rate >= 0.7:
            logger.info(f"  ğŸ‰ {winner} è¡¨ç°æ˜¾è‘—ä¼˜ç§€ (èƒœç‡: {winner_rate:.1%})")
        elif winner_rate >= 0.6:
            logger.info(f"  âœ… {winner} è¡¨ç°è¾ƒå¥½ (èƒœç‡: {winner_rate:.1%})")
        else:
            logger.info(f"  âš–ï¸ ä¸¤ä¸ªæ¨¡å‹è¡¨ç°ç›¸è¿‘")

        logger.info(f"  ğŸ’¡ æˆæœ¬æ•ˆç‡: ${cost['total_cost']} æ€»æˆæœ¬")
        logger.info(
            f"  ğŸ“ˆ æˆæœ¬é¢„ä¼°: 1Kæ ·æœ¬çº¦${cost['projections']['cost_per_1k_samples']} | 10Kæ ·æœ¬çº¦${cost['projections']['cost_per_10k_samples']}"
        )


async def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸ¯ å®Œæ•´æ•°æ®é›†å¤§è§„æ¨¡LLMè¯„ä¼°æµ‹è¯•")
    logger.info("ä½¿ç”¨æ™ºèƒ½é‡‡æ ·å’Œæˆæœ¬æ§åˆ¶è¿›è¡Œå¤§è§„æ¨¡è¯„ä¼°")

    # åˆ›å»ºæµ‹è¯•è¿è¡Œå™¨
    tester = CompleteDatasetTestRunner(max_cost_usd=5.0)  # é¢„ç®—$5

    # è¿è¡Œå®Œæ•´æ•°æ®é›†æµ‹è¯•
    summary = await tester.run_complete_dataset_test(
        test_size=2000, sampling_strategy="stratified"  # æµ‹è¯•2000ä¸ªæ ·æœ¬  # ä½¿ç”¨åˆ†å±‚æŠ½æ ·
    )

    if summary:
        # æ˜¾ç¤ºç»“æœ
        tester.display_comprehensive_summary(summary)
        logger.info("\nâœ… å®Œæ•´æ•°æ®é›†æµ‹è¯•å®Œæˆï¼")
    else:
        logger.error("âŒ æµ‹è¯•å¤±è´¥")

    return summary


if __name__ == "__main__":
    asyncio.run(main())
