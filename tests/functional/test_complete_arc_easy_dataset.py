#!/usr/bin/env python3
"""
Complete ARC-Easy Dataset Test with Three Models
ARC-Easyå®Œæ•´æ•°æ®é›†æµ‹è¯• - OpenAI + Anthropic + Google Geminiä¸‰æ¨¡å‹å¯¹æ¯”
åŒ…å«æ™ºèƒ½é€€é¿æœºåˆ¶å’Œé€Ÿç‡é™åˆ¶ä¿æŠ¤
"""

import asyncio
import json
import logging
import os
import random
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

from test_real_api_integration import RealAPIProvider
from test_real_multi_model_comparison import RealLLMJudge

# Configure logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)


class CompleteARCEasyTest:
    """ARC-Easyå®Œæ•´æ•°æ®é›†æµ‹è¯•"""

    def __init__(self):
        self.judge = RealLLMJudge()
        self.results = []
        self.api_call_counts = {"openai": 0, "anthropic": 0, "google": 0}
        self.last_api_call = {"openai": 0, "anthropic": 0, "google": 0}

    def load_arc_easy_dataset(self) -> List[Dict[str, Any]]:
        """åŠ è½½å®Œæ•´ARC-Easyæ•°æ®é›†"""

        dataset_path = Path("data/processed/arc_easy.json")
        if not dataset_path.exists():
            raise FileNotFoundError(f"ARC-Easy dataset not found: {dataset_path}")

        with open(dataset_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        # éšæœºæ‰“ä¹±æ•°æ®ç¡®ä¿éšæœºæ€§
        random.shuffle(data)
        logger.info(f"åŠ è½½ARC-Easyå®Œæ•´æ•°æ®é›†: {len(data)}ä¸ªæ ·æœ¬")

        return data

    async def smart_backoff(self, provider: str, base_delay: float = 1.0) -> None:
        """æ™ºèƒ½é€€é¿æœºåˆ¶ - æ ¹æ®æä¾›å•†å’Œè°ƒç”¨é¢‘ç‡åŠ¨æ€è°ƒæ•´å»¶è¿Ÿ"""

        current_time = time.time()
        time_since_last = current_time - self.last_api_call[provider]
        call_count = self.api_call_counts[provider]

        # åŠ¨æ€è®¡ç®—é€€é¿å»¶è¿Ÿ
        if provider == "google":
            # Google Geminiä»˜è´¹å±‚çº§ä½†ä»éœ€è°¨æ…
            if call_count % 10 == 0 and call_count > 0:  # æ¯10æ¬¡è°ƒç”¨
                delay = base_delay * 3.0  # 3ç§’å»¶è¿Ÿ
            elif time_since_last < 0.5:
                delay = base_delay * 2.0  # 2ç§’å»¶è¿Ÿ
            else:
                delay = base_delay  # 1ç§’æ ‡å‡†å»¶è¿Ÿ

        elif provider == "anthropic":
            # Anthropicç›¸å¯¹ç¨³å®šä½†ä¹Ÿéœ€è¦é€‚åº¦å»¶è¿Ÿ
            if call_count % 20 == 0 and call_count > 0:  # æ¯20æ¬¡è°ƒç”¨
                delay = base_delay * 2.0  # 2ç§’å»¶è¿Ÿ
            elif time_since_last < 0.3:
                delay = base_delay * 1.5  # 1.5ç§’å»¶è¿Ÿ
            else:
                delay = base_delay * 0.5  # 0.5ç§’å»¶è¿Ÿ

        else:  # openai
            # OpenAIæœ€ç¨³å®šï¼Œæœ€å°å»¶è¿Ÿ
            if call_count % 30 == 0 and call_count > 0:  # æ¯30æ¬¡è°ƒç”¨
                delay = base_delay * 1.5  # 1.5ç§’å»¶è¿Ÿ
            else:
                delay = base_delay * 0.3  # 0.3ç§’å»¶è¿Ÿ

        # æ‰§è¡Œå»¶è¿Ÿ
        if delay > 0:
            logger.debug(f"  {provider} æ™ºèƒ½é€€é¿: {delay:.1f}s (è°ƒç”¨#{call_count})")
            await asyncio.sleep(delay)

        # æ›´æ–°è®¡æ•°å™¨
        self.api_call_counts[provider] += 1
        self.last_api_call[provider] = time.time()

    async def run_complete_arc_easy_test(self, budget_limit: float = 10.0) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´ARC-Easyæ•°æ®é›†æµ‹è¯•"""

        logger.info(f"ğŸš€ å¼€å§‹ARC-Easyå®Œæ•´æ•°æ®é›†æµ‹è¯•")
        logger.info(f"æ•°æ®é›†: ARC-Easy (ç§‘å­¦çŸ¥è¯†)")
        logger.info(f"é¢„ç®—é™åˆ¶: ${budget_limit}")
        logger.info(f"æµ‹è¯•æ¨¡å‹: OpenAI + Anthropic + Googleä¸‰æ¨¡å‹éšæœºå¯¹æ¯”")

        start_time = time.time()

        # åŠ è½½å®Œæ•´æ•°æ®é›†
        try:
            test_data = self.load_arc_easy_dataset()
            total_samples = len(test_data)
        except Exception as e:
            logger.error(f"åŠ è½½æ•°æ®é›†å¤±è´¥: {str(e)}")
            return {"error": f"Failed to load dataset: {str(e)}"}

        results = {
            "test_info": {
                "test_name": "Complete ARC-Easy Dataset Test",
                "timestamp": datetime.now().isoformat(),
                "dataset_name": "arc_easy",
                "total_samples": total_samples,
                "budget_limit": budget_limit,
                "test_type": "Three-Model Random Comparison",
                "models": [
                    "OpenAI GPT-4o-mini",
                    "Anthropic Claude-3-haiku",
                    "Google Gemini-1.5-flash",
                ],
            },
            "dataset_analysis": {
                "by_source": {"ARC-Easy": total_samples},
                "by_category": {"science": total_samples},
                "by_difficulty": {"easy": total_samples},
            },
            "model_configs": {},
            "test_results": [],
            "summary": {},
            "performance_metrics": {
                "api_call_counts": {},
                "backoff_stats": {},
                "error_recovery": {},
            },
        }

        async with RealAPIProvider() as provider:
            # ä½¿ç”¨æ‰€æœ‰ä¸‰ä¸ªæä¾›å•†
            available_providers = ["openai", "anthropic", "google"]
            logger.info(f"å¯ç”¨APIæä¾›å•†: {available_providers}")

            # è®°å½•æ¨¡å‹é…ç½®
            for provider_name in available_providers:
                config = provider.api_configs[provider_name]
                results["model_configs"][provider_name] = {
                    "provider": config.provider,
                    "model": config.model,
                    "pricing": config.pricing,
                }

            # åˆå§‹åŒ–ç»Ÿè®¡
            total_cost = 0.0
            model_stats = {
                provider: {
                    "wins": 0,
                    "requests": 0,
                    "tokens": 0,
                    "cost": 0.0,
                    "avg_response_time": 0.0,
                    "response_times": [],
                    "error_count": 0,
                }
                for provider in available_providers
            }
            ties = 0
            skipped_samples = 0

            # æ‰¹é‡å¤„ç†è®¾ç½® - å‡å°æ‰¹é‡å¤§å°ä»¥æ›´å¥½æ§åˆ¶è¿›åº¦
            batch_size = 5  # å‡å°‘æ‰¹é‡å¤§å°ï¼Œæ›´é¢‘ç¹çš„è¿›åº¦æ›´æ–°
            total_batches = (total_samples + batch_size - 1) // batch_size

            logger.info(f"å¼€å§‹æ‰¹é‡å¤„ç†ï¼Œå…±{total_batches}æ‰¹ï¼Œæ¯æ‰¹{batch_size}ä¸ªæ ·æœ¬")
            logger.info(f"é¢„è®¡å¤„ç†æ—¶é—´: {total_batches * batch_size * 15 / 3600:.1f}å°æ—¶")

            # æ¯100ä¸ªæ ·æœ¬ä¿å­˜ä¸€æ¬¡ä¸­é—´ç»“æœ
            save_interval = 100
            last_save = 0

            for batch_idx in range(total_batches):
                if total_cost >= budget_limit:
                    logger.warning(f"è¾¾åˆ°é¢„ç®—é™åˆ¶ ${budget_limit:.2f}ï¼Œåœæ­¢æµ‹è¯•")
                    break

                batch_start = batch_idx * batch_size
                batch_end = min((batch_idx + 1) * batch_size, total_samples)
                batch_data = test_data[batch_start:batch_end]

                logger.info(
                    f"æ‰¹æ¬¡ {batch_idx + 1}/{total_batches} (æ ·æœ¬ {batch_start + 1}-{batch_end})"
                )

                # å¤„ç†å½“å‰æ‰¹æ¬¡
                for i, sample in enumerate(batch_data):
                    global_idx = batch_start + i + 1

                    if total_cost >= budget_limit:
                        logger.warning(f"è¾¾åˆ°é¢„ç®—é™åˆ¶ï¼Œåœæ­¢åœ¨æ ·æœ¬{global_idx}")
                        break

                    logger.info(
                        f"  æ ·æœ¬ {global_idx}/{total_samples}: {sample.get('id', f'sample_{global_idx}')}"
                    )

                    # éšæœºé€‰æ‹©ä¸¤ä¸ªä¸åŒçš„æä¾›å•†è¿›è¡Œå¯¹æ¯”
                    model_pair = random.sample(available_providers, 2)
                    model_a, model_b = model_pair

                    prompt = sample.get("prompt", "")
                    if not prompt:
                        logger.warning(f"    æ ·æœ¬æ— æ•ˆpromptï¼Œè·³è¿‡")
                        skipped_samples += 1
                        continue

                    try:
                        # æ¨¡å‹Aå“åº” - å¸¦æ™ºèƒ½é€€é¿
                        await self.smart_backoff(model_a)
                        response_start = time.time()
                        response_a = await provider.generate_response(prompt, model_a)
                        response_a_time = time.time() - response_start

                        # æ¨¡å‹Bå“åº” - å¸¦æ™ºèƒ½é€€é¿
                        await self.smart_backoff(model_b)
                        response_start = time.time()
                        response_b = await provider.generate_response(prompt, model_b)
                        response_b_time = time.time() - response_start

                        if not response_a["success"] or not response_b["success"]:
                            logger.error(f"    APIè°ƒç”¨å¤±è´¥ï¼Œè·³è¿‡æ ·æœ¬ {global_idx}")
                            if not response_a["success"]:
                                logger.error(
                                    f"      {model_a}: {response_a.get('error', 'Unknown error')}"
                                )
                                model_stats[model_a]["error_count"] += 1
                            if not response_b["success"]:
                                logger.error(
                                    f"      {model_b}: {response_b.get('error', 'Unknown error')}"
                                )
                                model_stats[model_b]["error_count"] += 1
                            skipped_samples += 1
                            continue

                        # æ›´æ–°ç»Ÿè®¡
                        model_stats[model_a]["requests"] += 1
                        model_stats[model_a]["tokens"] += response_a["total_tokens"]
                        model_stats[model_a]["cost"] += response_a["cost"]
                        model_stats[model_a]["response_times"].append(response_a_time)

                        model_stats[model_b]["requests"] += 1
                        model_stats[model_b]["tokens"] += response_b["total_tokens"]
                        model_stats[model_b]["cost"] += response_b["cost"]
                        model_stats[model_b]["response_times"].append(response_b_time)

                        total_cost += response_a["cost"] + response_b["cost"]

                        # è¿›è¡Œè¯„åˆ¤ - ä½¿ç”¨OpenAIä½œä¸ºè¯„åˆ¤è€…
                        await self.smart_backoff("openai", 0.5)  # è¯„åˆ¤ç”¨è¾ƒçŸ­å»¶è¿Ÿ
                        responses_for_judge = {"model_a": response_a, "model_b": response_b}

                        evaluation = await self.judge.evaluate_responses(
                            prompt, responses_for_judge, provider
                        )

                        if not evaluation["success"]:
                            logger.error(f"    è¯„åˆ¤å¤±è´¥ï¼Œè®°å½•ä¸ºå¹³å±€")
                            evaluation = {
                                "winner": "tie",
                                "confidence": 0.5,
                                "judge_cost": 0.0,
                                "judge_tokens": 0,
                            }
                        else:
                            total_cost += evaluation["judge_cost"]

                        # æ›´æ–°èƒœè´Ÿç»Ÿè®¡
                        winner = evaluation["winner"]
                        if winner == "model_a":
                            model_stats[model_a]["wins"] += 1
                        elif winner == "model_b":
                            model_stats[model_b]["wins"] += 1
                        else:
                            ties += 1

                        # è®°å½•ç»“æœ
                        test_result = {
                            "sample_num": global_idx,
                            "sample_data": {
                                "id": sample.get("id", f"sample_{global_idx}"),
                                "source": sample.get("source", "ARC-Easy"),
                                "category": sample.get("category", "science"),
                                "difficulty": sample.get("difficulty", "easy"),
                                "prompt_length": len(prompt),
                            },
                            "model_pair": {
                                "model_a": f"{model_a} ({response_a['model']})",
                                "model_b": f"{model_b} ({response_b['model']})",
                            },
                            "responses": {
                                "model_a": {
                                    "provider": model_a,
                                    "model": response_a["model"],
                                    "tokens": response_a["total_tokens"],
                                    "cost": response_a["cost"],
                                    "response_time": response_a_time,
                                },
                                "model_b": {
                                    "provider": model_b,
                                    "model": response_b["model"],
                                    "tokens": response_b["total_tokens"],
                                    "cost": response_b["cost"],
                                    "response_time": response_b_time,
                                },
                            },
                            "evaluation": {
                                "winner": winner,
                                "confidence": evaluation["confidence"],
                                "judge_cost": evaluation["judge_cost"],
                                "judge_tokens": evaluation["judge_tokens"],
                            },
                            "timestamp": datetime.now().isoformat(),
                        }

                        results["test_results"].append(test_result)

                        logger.info(
                            f"    ç»“æœ: {winner}, æˆæœ¬: ${response_a['cost'] + response_b['cost'] + evaluation.get('judge_cost', 0):.6f}"
                        )

                    except Exception as e:
                        logger.error(f"    å¤„ç†æ ·æœ¬ {global_idx} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                        skipped_samples += 1
                        continue

                # æ‰¹æ¬¡å®Œæˆåæ˜¾ç¤ºè¿›åº¦å’Œä¸­é—´ä¿å­˜
                completed_tests = len(results["test_results"])
                progress = (completed_tests / total_samples) * 100 if total_samples > 0 else 0
                elapsed_time = time.time() - start_time
                estimated_total = elapsed_time / progress * 100 if progress > 0 else 0
                remaining_time = estimated_total - elapsed_time

                logger.info(f"æ‰¹æ¬¡ {batch_idx + 1} å®Œæˆ")
                logger.info(f"  æ€»è¿›åº¦: {completed_tests}/{total_samples} ({progress:.1f}%)")
                logger.info(
                    f"  å½“å‰æˆæœ¬: ${total_cost:.6f} ({total_cost/budget_limit*100:.1f}%é¢„ç®—)"
                )
                logger.info(f"  å·²ç”¨æ—¶: {elapsed_time/60:.1f}åˆ†é’Ÿ")
                logger.info(f"  é¢„è®¡å‰©ä½™: {remaining_time/60:.1f}åˆ†é’Ÿ")

                # å®šæœŸä¿å­˜ä¸­é—´ç»“æœ
                if completed_tests - last_save >= save_interval:
                    interim_results = results.copy()
                    interim_results["interim_save"] = True
                    interim_results["save_point"] = completed_tests
                    self.save_interim_results(interim_results, f"interim_{completed_tests}")
                    last_save = completed_tests
                    logger.info(f"  å·²ä¿å­˜ä¸­é—´ç»“æœ (æ ·æœ¬#{completed_tests})")

        end_time = time.time()
        total_time = end_time - start_time

        # è®¡ç®—å¹³å‡å“åº”æ—¶é—´
        for provider, stats in model_stats.items():
            if stats["response_times"]:
                stats["avg_response_time"] = sum(stats["response_times"]) / len(
                    stats["response_times"]
                )
                del stats["response_times"]  # åˆ é™¤åŸå§‹æ•°æ®èŠ‚çœç©ºé—´

        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        completed_tests = len(results["test_results"])

        results["summary"] = {
            "total_time": total_time,
            "completed_tests": completed_tests,
            "skipped_samples": skipped_samples,
            "completion_rate": completed_tests / total_samples if total_samples > 0 else 0,
            "total_cost": total_cost,
            "budget_used_percentage": (total_cost / budget_limit * 100) if budget_limit > 0 else 0,
            "avg_cost_per_test": total_cost / completed_tests if completed_tests > 0 else 0,
            "throughput": completed_tests / total_time if total_time > 0 else 0,
            "model_stats": model_stats,
            "ties": ties,
            "judge_stats": {
                "total_evaluations": self.judge.total_evaluations,
                "total_cost": self.judge.total_cost,
            },
        }

        # æ€§èƒ½æŒ‡æ ‡
        results["performance_metrics"] = {
            "api_call_counts": dict(self.api_call_counts),
            "total_api_calls": sum(self.api_call_counts.values()),
            "avg_processing_time": total_time / completed_tests if completed_tests > 0 else 0,
            "cost_efficiency": completed_tests / total_cost if total_cost > 0 else 0,
        }

        return results

    def save_interim_results(self, results: Dict[str, Any], suffix: str = ""):
        """ä¿å­˜ä¸­é—´ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"arc_easy_complete_test_{suffix}_{timestamp}.json"

        results_dir = Path("logs/complete_arc_easy_results")
        results_dir.mkdir(parents=True, exist_ok=True)

        filepath = results_dir / filename

        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        logger.info(f"ä¸­é—´ç»“æœå·²ä¿å­˜åˆ°: {filepath}")
        return filepath

    def save_results(self, results: Dict[str, Any], filename: str = None):
        """ä¿å­˜æœ€ç»ˆæµ‹è¯•ç»“æœ"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            completed = results["summary"]["completed_tests"]
            filename = f"arc_easy_complete_dataset_{completed}samples_{timestamp}.json"

        results_dir = Path("logs/complete_arc_easy_results")
        results_dir.mkdir(parents=True, exist_ok=True)

        filepath = results_dir / filename

        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        logger.info(f"æœ€ç»ˆæµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {filepath}")
        return filepath


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ARC-Easyå®Œæ•´æ•°æ®é›†æµ‹è¯•")
    print("=" * 60)

    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    api_keys = {
        "OpenAI": os.getenv("OPENAI_API_KEY"),
        "Anthropic": os.getenv("ANTHROPIC_API_KEY"),
        "Google": os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY"),
    }

    print("ğŸ“‹ APIå¯†é’¥çŠ¶æ€:")
    available_count = 0
    for provider, key in api_keys.items():
        status = "âœ… å·²é…ç½®" if key else "âŒ æœªé…ç½®"
        if key:
            available_count += 1
        print(f"  {provider}: {status}")

    if available_count < 3:
        print(f"\nâš ï¸  è­¦å‘Š: åªæœ‰{available_count}ä¸ªAPIå¯†é’¥é…ç½®ï¼Œå°†è¿›è¡Œå¯ç”¨æ¨¡å‹çš„å¯¹æ¯”æµ‹è¯•")
    else:
        print(f"\nâœ… å¯ä»¥è¿›è¡Œå®Œæ•´ä¸‰æ¨¡å‹å¯¹æ¯”æµ‹è¯•")

    print(f"\nğŸ”¬ æµ‹è¯•é…ç½®:")
    print(f"  æ•°æ®é›†: ARC-Easyå®Œæ•´æ•°æ®é›† (5,197æ ·æœ¬)")
    print(f"  æ¨¡å‹: OpenAI + Anthropic + Googleä¸‰æ¨¡å‹éšæœºå¯¹æ¯”")
    print(f"  é¢„ç®—é™åˆ¶: $10.00")
    print(f"  æ™ºèƒ½é€€é¿: å·²å¯ç”¨ (é¿å…é€Ÿç‡é™åˆ¶)")
    print(f"  ä¸­é—´ä¿å­˜: æ¯100æ ·æœ¬ä¿å­˜ä¸€æ¬¡")

    # åˆ›å»ºæµ‹è¯•å®ä¾‹
    tester = CompleteARCEasyTest()

    print(f"\nğŸš€ å¼€å§‹å®Œæ•´æ•°æ®é›†æµ‹è¯•...")

    # è¿è¡Œæµ‹è¯•
    results = await tester.run_complete_arc_easy_test(budget_limit=10.0)

    if "error" in results:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {results['error']}")
        return

    # ä¿å­˜ç»“æœ
    filepath = tester.save_results(results)

    # æ˜¾ç¤ºç»“æœæ‘˜è¦
    summary = results["summary"]

    print(f"\nğŸ“Š æµ‹è¯•å®Œæˆæ‘˜è¦:")
    print(f"  æ€»è€—æ—¶: {summary['total_time']:.1f}ç§’ ({summary['total_time']/3600:.2f}å°æ—¶)")
    print(f"  å®Œæˆæµ‹è¯•: {summary['completed_tests']}")
    print(f"  è·³è¿‡æ ·æœ¬: {summary['skipped_samples']}")
    print(f"  å®Œæˆç‡: {summary['completion_rate']:.1%}")
    print(f"  æ€»æˆæœ¬: ${summary['total_cost']:.6f}")
    print(f"  é¢„ç®—ä½¿ç”¨: {summary['budget_used_percentage']:.1f}%")
    print(f"  å¹³å‡æˆæœ¬/æµ‹è¯•: ${summary['avg_cost_per_test']:.6f}")
    print(f"  ååé‡: {summary['throughput']:.3f} æµ‹è¯•/ç§’")
    print(f"  å¹³å±€æ¬¡æ•°: {summary['ties']}")

    print(f"\nğŸ† æ¨¡å‹è¡¨ç°:")
    for model, stats in summary["model_stats"].items():
        if stats["requests"] > 0:
            win_rate = stats["wins"] / stats["requests"] * 100 if stats["requests"] > 0 else 0
            print(f"  {model.upper()}:")
            print(f"    å‚ä¸æµ‹è¯•: {stats['requests']}æ¬¡")
            print(f"    è·èƒœ: {stats['wins']}æ¬¡ ({win_rate:.1f}%)")
            print(f"    Tokenä½¿ç”¨: {stats['tokens']:,}")
            print(f"    æˆæœ¬: ${stats['cost']:.6f}")
            print(f"    å¹³å‡å“åº”æ—¶é—´: {stats['avg_response_time']:.3f}ç§’")
            print(f"    é”™è¯¯æ¬¡æ•°: {stats['error_count']}")

    print(f"\nğŸ“ˆ æ€§èƒ½æŒ‡æ ‡:")
    perf = results["performance_metrics"]
    print(f"  æ€»APIè°ƒç”¨: {perf['total_api_calls']}")
    print(f"  å¹³å‡å¤„ç†æ—¶é—´: {perf['avg_processing_time']:.1f}ç§’/æ ·æœ¬")
    print(f"  æˆæœ¬æ•ˆç‡: {perf['cost_efficiency']:.1f} æ ·æœ¬/$")

    print(f"\nâœ… ARC-Easyå®Œæ•´æ•°æ®é›†æµ‹è¯•å®Œæˆï¼")
    print(f"ğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {filepath}")


if __name__ == "__main__":
    asyncio.run(main())
