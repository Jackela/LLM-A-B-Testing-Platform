#!/usr/bin/env python3
"""
LLM as a Judge Test - Small Sample Evaluation
è¿è¡Œå°æ ·æœ¬æµ‹è¯•å¹¶ä½¿ç”¨LLMè¯„ä¼°ç»“æœ
"""

import asyncio
import json
import logging
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

import httpx

# Configure logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)


class MockLLMProvider:
    """æ¨¡æ‹ŸLLMæä¾›å•†ï¼Œç”¨äºç”Ÿæˆæµ‹è¯•å“åº”"""

    def __init__(self, provider_name: str, style: str):
        self.provider_name = provider_name
        self.style = style

    async def generate_response(self, prompt: str) -> str:
        """æ ¹æ®ä¸åŒé£æ ¼ç”Ÿæˆæ¨¡æ‹Ÿå“åº”"""
        await asyncio.sleep(0.1)  # æ¨¡æ‹ŸAPIè°ƒç”¨å»¶è¿Ÿ

        if "capital of France" in prompt.lower():
            if self.style == "detailed":
                return "The capital of France is Paris. Paris is located in the north-central part of France and has been the country's capital since 1789. It is known for landmarks like the Eiffel Tower and the Louvre Museum."
            else:
                return "Paris is the capital of France."

        elif "solve" in prompt.lower() and "math" in prompt.lower():
            if "2 + 3" in prompt:
                if self.style == "detailed":
                    return "To solve 2 + 3, I add the two numbers together: 2 + 3 = 5. The answer is 5."
                else:
                    return "2 + 3 = 5"
            elif "10 * 6" in prompt:
                if self.style == "detailed":
                    return "To calculate 10 Ã— 6, I multiply: 10 Ã— 6 = 60. The final answer is 60."
                else:
                    return "10 * 6 = 60"

        elif "explain" in prompt.lower() and "photosynthesis" in prompt.lower():
            if self.style == "detailed":
                return "Photosynthesis is the process by which plants convert sunlight, carbon dioxide, and water into glucose and oxygen. This process occurs in chloroplasts and is essential for plant survival and oxygen production on Earth."
            else:
                return "Photosynthesis is how plants make food using sunlight, water, and CO2."

        # Default responses
        if self.style == "detailed":
            return f"This is a detailed response from {self.provider_name}. I aim to provide comprehensive and thorough answers to help users understand the topic fully."
        else:
            return f"This is a concise response from {self.provider_name}."


class LLMJudge:
    """LLMä½œä¸ºè¯„åˆ¤è€…ï¼Œè¯„ä¼°ä¸¤ä¸ªå“åº”çš„è´¨é‡"""

    def __init__(self):
        self.judge_prompt_template = """
ä½œä¸ºä¸€ä¸ªä¸“ä¸šçš„AIè¯„ä¼°ä¸“å®¶ï¼Œè¯·è¯„ä¼°ä»¥ä¸‹ä¸¤ä¸ªAIå“åº”çš„è´¨é‡ã€‚

é—®é¢˜ï¼š{question}

å“åº”A ({provider_a}):
{response_a}

å“åº”B ({provider_b}):
{response_b}

è¯·ä»ä»¥ä¸‹ç»´åº¦è¯„ä¼°ï¼ˆæ¯ä¸ªç»´åº¦1-10åˆ†ï¼‰ï¼š
1. å‡†ç¡®æ€§ (Accuracy) - ä¿¡æ¯æ˜¯å¦æ­£ç¡®
2. å®Œæ•´æ€§ (Completeness) - æ˜¯å¦å……åˆ†å›ç­”äº†é—®é¢˜  
3. æ¸…æ™°åº¦ (Clarity) - è¡¨è¾¾æ˜¯å¦æ¸…æ™°æ˜“æ‡‚
4. æœ‰ç”¨æ€§ (Helpfulness) - å¯¹ç”¨æˆ·æ˜¯å¦æœ‰å¸®åŠ©

è¯·æä¾›ï¼š
1. å„ç»´åº¦çš„è¯„åˆ†
2. æ€»ä½“è¯„åˆ† (1-10)
3. ç®€è¦è¯´æ˜ç†ç”±
4. æ¨èå“ªä¸ªå“åº”æ›´å¥½

è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼š
{{
    "scores": {{
        "response_a": {{
            "accuracy": åˆ†æ•°,
            "completeness": åˆ†æ•°,
            "clarity": åˆ†æ•°,
            "helpfulness": åˆ†æ•°,
            "overall": åˆ†æ•°
        }},
        "response_b": {{
            "accuracy": åˆ†æ•°,
            "completeness": åˆ†æ•°,
            "clarity": åˆ†æ•°,
            "helpfulness": åˆ†æ•°,
            "overall": åˆ†æ•°
        }}
    }},
    "winner": "A" æˆ– "B" æˆ– "Tie",
    "confidence": ç½®ä¿¡åº¦(0-1),
    "reasoning": "è¯„ä¼°ç†ç”±",
    "judge_provider": "Claude"
}}
"""

    async def evaluate_responses(
        self, question: str, response_a: str, response_b: str, provider_a: str, provider_b: str
    ) -> Dict[str, Any]:
        """ä½¿ç”¨LLMè¯„ä¼°ä¸¤ä¸ªå“åº”"""

        # æ„å»ºè¯„ä¼°æç¤º
        judge_prompt = self.judge_prompt_template.format(
            question=question,
            response_a=response_a,
            response_b=response_b,
            provider_a=provider_a,
            provider_b=provider_b,
        )

        # æ¨¡æ‹ŸLLMè¯„ä¼°ï¼ˆå®é™…åº”ç”¨ä¸­è¿™é‡Œä¼šè°ƒç”¨çœŸå®çš„LLM APIï¼‰
        await asyncio.sleep(0.5)  # æ¨¡æ‹ŸAPIè°ƒç”¨æ—¶é—´

        # åŸºäºå¯å‘å¼è§„åˆ™çš„æ¨¡æ‹Ÿè¯„ä¼°
        eval_result = self._simulate_evaluation(
            question, response_a, response_b, provider_a, provider_b
        )

        return eval_result

    def _simulate_evaluation(
        self, question: str, response_a: str, response_b: str, provider_a: str, provider_b: str
    ) -> Dict[str, Any]:
        """æ¨¡æ‹ŸLLMè¯„ä¼°é€»è¾‘"""

        # è®¡ç®—å“åº”é•¿åº¦å’Œä¿¡æ¯å¯†åº¦
        len_a, len_b = len(response_a), len(response_b)

        # åŸºæœ¬è¯„åˆ†é€»è¾‘
        def score_response(response: str, is_detailed: bool) -> Dict[str, float]:
            base_score = 7.0

            # å‡†ç¡®æ€§è¯„ä¼°
            accuracy = (
                8.0
                if any(
                    keyword in response.lower()
                    for keyword in ["paris", "5", "60", "photosynthesis"]
                )
                else 7.0
            )

            # å®Œæ•´æ€§è¯„ä¼°
            completeness = 8.5 if len(response) > 50 else 6.5

            # æ¸…æ™°åº¦è¯„ä¼°
            clarity = 8.0 if not response.lower().startswith("this is") else 7.0

            # æœ‰ç”¨æ€§è¯„ä¼°
            helpfulness = 8.5 if is_detailed else 7.5

            overall = (accuracy + completeness + clarity + helpfulness) / 4

            return {
                "accuracy": round(accuracy, 1),
                "completeness": round(completeness, 1),
                "clarity": round(clarity, 1),
                "helpfulness": round(helpfulness, 1),
                "overall": round(overall, 1),
            }

        # è¯„ä¼°ä¸¤ä¸ªå“åº”
        scores_a = score_response(response_a, "detailed" in provider_a.lower())
        scores_b = score_response(response_b, "detailed" in provider_b.lower())

        # ç¡®å®šè·èƒœè€…
        if scores_a["overall"] > scores_b["overall"]:
            winner = "A"
            confidence = min(0.9, (scores_a["overall"] - scores_b["overall"]) / 2 + 0.6)
        elif scores_b["overall"] > scores_a["overall"]:
            winner = "B"
            confidence = min(0.9, (scores_b["overall"] - scores_a["overall"]) / 2 + 0.6)
        else:
            winner = "Tie"
            confidence = 0.5

        # ç”Ÿæˆè¯„ä¼°ç†ç”±
        if winner == "A":
            reasoning = f"å“åº”Aåœ¨å¤šä¸ªç»´åº¦ä¸Šè¡¨ç°æ›´å¥½ï¼Œç‰¹åˆ«æ˜¯åœ¨{'å®Œæ•´æ€§' if scores_a['completeness'] > scores_b['completeness'] else 'æœ‰ç”¨æ€§'}æ–¹é¢ã€‚"
        elif winner == "B":
            reasoning = f"å“åº”Båœ¨å¤šä¸ªç»´åº¦ä¸Šè¡¨ç°æ›´å¥½ï¼Œç‰¹åˆ«æ˜¯åœ¨{'å®Œæ•´æ€§' if scores_b['completeness'] > scores_a['completeness'] else 'æœ‰ç”¨æ€§'}æ–¹é¢ã€‚"
        else:
            reasoning = "ä¸¤ä¸ªå“åº”è´¨é‡ç›¸è¿‘ï¼Œå„æœ‰ä¼˜åŠ£ã€‚"

        return {
            "scores": {"response_a": scores_a, "response_b": scores_b},
            "winner": winner,
            "confidence": round(confidence, 2),
            "reasoning": reasoning,
            "judge_provider": "Claude (Simulated)",
        }


class SmallSampleTest:
    """å°æ ·æœ¬æµ‹è¯•è¿è¡Œå™¨"""

    def __init__(self):
        self.provider_a = MockLLMProvider("GPT-4-Detailed", "detailed")
        self.provider_b = MockLLMProvider("Claude-Concise", "concise")
        self.judge = LLMJudge()
        self.test_results = []

    def get_test_questions(self) -> List[Dict[str, Any]]:
        """è·å–æµ‹è¯•é—®é¢˜é›†"""
        return [
            {
                "id": "q1",
                "question": "What is the capital of France?",
                "category": "factual",
                "expected_answer": "Paris",
            },
            {
                "id": "q2",
                "question": "Solve this math problem: 2 + 3 = ?",
                "category": "math",
                "expected_answer": "5",
            },
            {
                "id": "q3",
                "question": "Calculate: 10 * 6",
                "category": "math",
                "expected_answer": "60",
            },
            {
                "id": "q4",
                "question": "Explain photosynthesis in simple terms",
                "category": "science",
                "expected_answer": "Process by which plants make food using sunlight",
            },
        ]

    async def run_evaluation(self, question_data: Dict[str, Any]) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªé—®é¢˜çš„è¯„ä¼°"""
        question = question_data["question"]

        logger.info(f"ğŸ” è¯„ä¼°é—®é¢˜: {question}")

        # è·å–ä¸¤ä¸ªæä¾›å•†çš„å“åº”
        start_time = time.time()
        response_a = await self.provider_a.generate_response(question)
        response_b = await self.provider_b.generate_response(question)
        response_time = time.time() - start_time

        logger.info(f"ğŸ“ Provider A ({self.provider_a.provider_name}): {response_a}")
        logger.info(f"ğŸ“ Provider B ({self.provider_b.provider_name}): {response_b}")

        # LLMè¯„ä¼°
        start_time = time.time()
        evaluation = await self.judge.evaluate_responses(
            question,
            response_a,
            response_b,
            self.provider_a.provider_name,
            self.provider_b.provider_name,
        )
        eval_time = time.time() - start_time

        # ç¼–è¯‘ç»“æœ
        result = {
            "question_data": question_data,
            "responses": {
                "provider_a": {"name": self.provider_a.provider_name, "response": response_a},
                "provider_b": {"name": self.provider_b.provider_name, "response": response_b},
            },
            "evaluation": evaluation,
            "timing": {
                "response_time": round(response_time, 3),
                "evaluation_time": round(eval_time, 3),
            },
            "timestamp": datetime.now().isoformat(),
        }

        return result

    async def run_small_sample_test(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„å°æ ·æœ¬æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹LLM as a Judgeå°æ ·æœ¬æµ‹è¯•")
        logger.info("=" * 60)

        test_questions = self.get_test_questions()
        overall_start_time = time.time()

        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        for i, question_data in enumerate(test_questions, 1):
            logger.info(f"\nğŸ“‹ æµ‹è¯• {i}/{len(test_questions)}")
            result = await self.run_evaluation(question_data)
            self.test_results.append(result)

            # æ˜¾ç¤ºè¯„ä¼°ç»“æœ
            eval_data = result["evaluation"]
            winner = eval_data["winner"]
            confidence = eval_data["confidence"]

            logger.info(f"ğŸ† è·èƒœè€…: {winner} (ç½®ä¿¡åº¦: {confidence})")
            logger.info(f"ğŸ’­ è¯„ä¼°ç†ç”±: {eval_data['reasoning']}")

        total_time = time.time() - overall_start_time

        # ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡
        summary = self._generate_summary(total_time)

        # ä¿å­˜ç»“æœ
        self._save_results(summary)

        return summary

    def _generate_summary(self, total_time: float) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æ±‡æ€»"""
        if not self.test_results:
            return {}

        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        provider_a_wins = sum(1 for r in self.test_results if r["evaluation"]["winner"] == "A")
        provider_b_wins = sum(1 for r in self.test_results if r["evaluation"]["winner"] == "B")
        ties = sum(1 for r in self.test_results if r["evaluation"]["winner"] == "Tie")

        # å¹³å‡è¯„åˆ†
        avg_scores_a = {}
        avg_scores_b = {}
        for dimension in ["accuracy", "completeness", "clarity", "helpfulness", "overall"]:
            avg_scores_a[dimension] = round(
                sum(r["evaluation"]["scores"]["response_a"][dimension] for r in self.test_results)
                / len(self.test_results),
                2,
            )
            avg_scores_b[dimension] = round(
                sum(r["evaluation"]["scores"]["response_b"][dimension] for r in self.test_results)
                / len(self.test_results),
                2,
            )

        # å¹³å‡å“åº”æ—¶é—´
        avg_response_time = round(
            sum(r["timing"]["response_time"] for r in self.test_results) / len(self.test_results), 3
        )
        avg_eval_time = round(
            sum(r["timing"]["evaluation_time"] for r in self.test_results) / len(self.test_results),
            3,
        )

        summary = {
            "test_info": {
                "test_name": "LLM as a Judge - Small Sample Test",
                "timestamp": datetime.now().isoformat(),
                "total_questions": len(self.test_results),
                "total_time": round(total_time, 2),
            },
            "providers": {
                "provider_a": self.provider_a.provider_name,
                "provider_b": self.provider_b.provider_name,
            },
            "results": {
                "provider_a_wins": provider_a_wins,
                "provider_b_wins": provider_b_wins,
                "ties": ties,
                "win_rate_a": round(provider_a_wins / len(self.test_results), 2),
                "win_rate_b": round(provider_b_wins / len(self.test_results), 2),
            },
            "average_scores": {"provider_a": avg_scores_a, "provider_b": avg_scores_b},
            "performance": {
                "avg_response_time": avg_response_time,
                "avg_evaluation_time": avg_eval_time,
                "total_time": round(total_time, 2),
            },
            "detailed_results": self.test_results,
        }

        return summary

    def _save_results(self, summary: Dict[str, Any]):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        # åˆ›å»ºç»“æœç›®å½•
        results_dir = Path("logs/llm_judge_results")
        results_dir.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜è¯¦ç»†ç»“æœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = results_dir / f"llm_judge_test_{timestamp}.json"

        with open(results_file, "w", encoding="utf-8") as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)

        logger.info(f"\nğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {results_file}")

    def display_summary(self, summary: Dict[str, Any]):
        """æ˜¾ç¤ºæµ‹è¯•æ±‡æ€»"""
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“Š LLM AS A JUDGE æµ‹è¯•æ±‡æ€»")
        logger.info("=" * 60)

        test_info = summary["test_info"]
        providers = summary["providers"]
        results = summary["results"]
        avg_scores = summary["average_scores"]
        performance = summary["performance"]

        logger.info(f"ğŸ•’ æµ‹è¯•æ—¶é—´: {test_info['timestamp']}")
        logger.info(f"ğŸ“ æµ‹è¯•é—®é¢˜æ•°: {test_info['total_questions']}")
        logger.info(f"â±ï¸ æ€»è€—æ—¶: {test_info['total_time']}ç§’")

        logger.info(f"\nğŸ¤– æµ‹è¯•æä¾›å•†:")
        logger.info(f"  Provider A: {providers['provider_a']}")
        logger.info(f"  Provider B: {providers['provider_b']}")

        logger.info(f"\nğŸ† æ¯”èµ›ç»“æœ:")
        logger.info(
            f"  Provider A è·èƒœ: {results['provider_a_wins']} æ¬¡ ({results['win_rate_a']:.0%})"
        )
        logger.info(
            f"  Provider B è·èƒœ: {results['provider_b_wins']} æ¬¡ ({results['win_rate_b']:.0%})"
        )
        logger.info(f"  å¹³å±€: {results['ties']} æ¬¡")

        logger.info(f"\nğŸ“ˆ å¹³å‡è¯„åˆ†å¯¹æ¯”:")
        logger.info(f"{'ç»´åº¦':<12} {'Provider A':<12} {'Provider B':<12} {'å·®å€¼'}")
        logger.info("-" * 50)

        for dimension in ["accuracy", "completeness", "clarity", "helpfulness", "overall"]:
            score_a = avg_scores["provider_a"][dimension]
            score_b = avg_scores["provider_b"][dimension]
            diff = round(score_a - score_b, 2)

            dim_cn = {
                "accuracy": "å‡†ç¡®æ€§",
                "completeness": "å®Œæ•´æ€§",
                "clarity": "æ¸…æ™°åº¦",
                "helpfulness": "æœ‰ç”¨æ€§",
                "overall": "æ€»ä½“è¯„åˆ†",
            }

            logger.info(f"{dim_cn[dimension]:<10} {score_a:<12} {score_b:<12} {diff:+.2f}")

        logger.info(f"\nâš¡ æ€§èƒ½æŒ‡æ ‡:")
        logger.info(f"  å¹³å‡å“åº”æ—¶é—´: {performance['avg_response_time']}ç§’")
        logger.info(f"  å¹³å‡è¯„ä¼°æ—¶é—´: {performance['avg_evaluation_time']}ç§’")

        # ç»“è®º
        if results["provider_a_wins"] > results["provider_b_wins"]:
            winner = providers["provider_a"]
        elif results["provider_b_wins"] > results["provider_a_wins"]:
            winner = providers["provider_b"]
        else:
            winner = "å¹³å±€"

        logger.info(f"\nğŸ¯ æµ‹è¯•ç»“è®º:")
        logger.info(f"  åœ¨æœ¬æ¬¡å°æ ·æœ¬æµ‹è¯•ä¸­ï¼Œ{winner} è¡¨ç°æ›´ä¼˜ç§€")
        logger.info(f"  ä¸¤ä¸ªæ¨¡å‹åœ¨ä¸åŒç»´åº¦å„æœ‰ä¼˜åŠ¿ï¼Œå»ºè®®æ ¹æ®å…·ä½“éœ€æ±‚é€‰æ‹©")


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    logger.info("ğŸ¯ LLM as a Judge - å°æ ·æœ¬è¯„ä¼°æµ‹è¯•")
    logger.info("æœ¬æµ‹è¯•å°†æ¨¡æ‹Ÿä¸¤ä¸ªLLMæä¾›å•†çš„å“åº”ï¼Œå¹¶ä½¿ç”¨LLMä½œä¸ºè¯„åˆ¤è€…è¿›è¡Œè¯„ä¼°")

    # åˆ›å»ºå¹¶è¿è¡Œæµ‹è¯•
    tester = SmallSampleTest()
    summary = await tester.run_small_sample_test()

    # æ˜¾ç¤ºæ±‡æ€»ç»“æœ
    tester.display_summary(summary)

    logger.info("\nâœ… LLM as a Judge æµ‹è¯•å®Œæˆï¼")
    return summary


if __name__ == "__main__":
    asyncio.run(main())
