#!/usr/bin/env python3
"""
Large Scale LLM Evaluation Test - 100 Samples
ä½¿ç”¨ä¾¿å®œæ¨¡å‹è¿›è¡Œå¤§è§„æ¨¡LLM as a Judgeè¯„ä¼°
æ”¯æŒ: Gemini Flash, Claude Sonnet, GPT-3.5 Turbo
"""

import asyncio
import json
import logging
import os
import random
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

import httpx

# Configure logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)


class APILLMProvider:
    """çœŸå®API LLMæä¾›å•†"""

    def __init__(self, provider_name: str, model_name: str, api_key: str = None):
        self.provider_name = provider_name
        self.model_name = model_name
        self.api_key = api_key or os.getenv(f"{provider_name.upper()}_API_KEY")
        self.request_count = 0
        self.total_cost = 0.0

        # ä»·æ ¼é…ç½® (æ¯1K tokensçš„ä»·æ ¼ï¼ŒUSD)
        self.pricing = {
            "gemini-flash": {"input": 0.000075, "output": 0.0003},  # Gemini 1.5 Flash
            "claude-sonnet": {"input": 0.003, "output": 0.015},  # Claude 3.5 Sonnet
            "gpt-3.5-turbo": {"input": 0.0005, "output": 0.0015},  # GPT-3.5 Turbo
        }

    async def generate_response(
        self, prompt: str, context: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """ç”Ÿæˆå“åº”å¹¶è®¡ç®—æˆæœ¬"""
        if not self.api_key:
            # å¦‚æœæ²¡æœ‰API keyï¼Œä½¿ç”¨æ¨¡æ‹Ÿå“åº”
            return await self._simulate_response(prompt, context)

        try:
            if "gemini" in self.model_name.lower():
                return await self._call_gemini_api(prompt)
            elif "claude" in self.model_name.lower():
                return await self._call_claude_api(prompt)
            elif "gpt" in self.model_name.lower():
                return await self._call_openai_api(prompt)
            else:
                return await self._simulate_response(prompt, context)
        except Exception as e:
            logger.warning(f"APIè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿå“åº”: {e}")
            return await self._simulate_response(prompt, context)

    async def _call_gemini_api(self, prompt: str) -> Dict[str, Any]:
        """è°ƒç”¨Gemini API"""
        url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key={self.api_key}"

        payload = {
            "contents": [{"parts": [{"text": prompt}]}],
            "generationConfig": {"temperature": 0.7, "maxOutputTokens": 500},
        }

        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(url, json=payload)
            response.raise_for_status()

            data = response.json()
            content = data["candidates"][0]["content"]["parts"][0]["text"]

            # ä¼°ç®—tokenä½¿ç”¨é‡
            input_tokens = len(prompt.split()) * 1.3  # ç²—ç•¥ä¼°ç®—
            output_tokens = len(content.split()) * 1.3

            cost = self._calculate_cost("gemini-flash", input_tokens, output_tokens)
            self.total_cost += cost
            self.request_count += 1

            return {
                "content": content,
                "input_tokens": int(input_tokens),
                "output_tokens": int(output_tokens),
                "cost": cost,
            }

    async def _call_claude_api(self, prompt: str) -> Dict[str, Any]:
        """è°ƒç”¨Claude API"""
        url = "https://api.anthropic.com/v1/messages"

        headers = {
            "x-api-key": self.api_key,
            "Content-Type": "application/json",
            "anthropic-version": "2023-06-01",
        }

        payload = {
            "model": "claude-3-5-sonnet-20241022",
            "max_tokens": 500,
            "temperature": 0.7,
            "messages": [{"role": "user", "content": prompt}],
        }

        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(url, headers=headers, json=payload)
            response.raise_for_status()

            data = response.json()
            content = data["content"][0]["text"]
            input_tokens = data["usage"]["input_tokens"]
            output_tokens = data["usage"]["output_tokens"]

            cost = self._calculate_cost("claude-sonnet", input_tokens, output_tokens)
            self.total_cost += cost
            self.request_count += 1

            return {
                "content": content,
                "input_tokens": input_tokens,
                "output_tokens": output_tokens,
                "cost": cost,
            }

    async def _call_openai_api(self, prompt: str) -> Dict[str, Any]:
        """è°ƒç”¨OpenAI API"""
        url = "https://api.openai.com/v1/chat/completions"

        headers = {"Authorization": f"Bearer {self.api_key}", "Content-Type": "application/json"}

        payload = {
            "model": "gpt-3.5-turbo",
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": 500,
            "temperature": 0.7,
        }

        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(url, headers=headers, json=payload)
            response.raise_for_status()

            data = response.json()
            content = data["choices"][0]["message"]["content"]
            input_tokens = data["usage"]["prompt_tokens"]
            output_tokens = data["usage"]["completion_tokens"]

            cost = self._calculate_cost("gpt-3.5-turbo", input_tokens, output_tokens)
            self.total_cost += cost
            self.request_count += 1

            return {
                "content": content,
                "input_tokens": input_tokens,
                "output_tokens": output_tokens,
                "cost": cost,
            }

    async def _simulate_response(
        self, prompt: str, context: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """æ¨¡æ‹ŸAPIå“åº”ï¼ˆå½“æ²¡æœ‰API keyæ—¶ä½¿ç”¨ï¼‰"""
        await asyncio.sleep(random.uniform(0.2, 0.8))  # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ

        # åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆæ¨¡æ‹Ÿå“åº”
        if context:
            if context.get("source") == "ARC-Easy":
                content = self._generate_science_response(prompt, context)
            elif context.get("source") == "GSM8K":
                content = self._generate_math_response(prompt, context)
            else:
                content = f"Based on the question, {context.get('expected_output', 'this requires careful analysis.')}"
        else:
            content = f"This is a simulated response from {self.model_name}."

        # ä¼°ç®—æ¨¡æ‹Ÿçš„tokenä½¿ç”¨é‡å’Œæˆæœ¬
        input_tokens = len(prompt.split()) * 1.3
        output_tokens = len(content.split()) * 1.3
        cost = self._calculate_cost(self.model_name.lower(), input_tokens, output_tokens)

        self.total_cost += cost
        self.request_count += 1

        return {
            "content": content,
            "input_tokens": int(input_tokens),
            "output_tokens": int(output_tokens),
            "cost": cost,
        }

    def _generate_science_response(self, prompt: str, context: Dict[str, Any]) -> str:
        """ç”Ÿæˆç§‘å­¦é—®é¢˜å“åº”"""
        if "gemini" in self.model_name.lower():
            return f"Based on scientific principles, {context.get('expected_output', 'the answer requires understanding fundamental concepts.')} This conclusion is supported by established scientific knowledge."
        elif "claude" in self.model_name.lower():
            return f"Looking at this scientifically: {context.get('expected_output', 'the answer follows from basic principles.')} The reasoning is straightforward when we consider the underlying mechanisms."
        else:
            return f"From a scientific perspective, {context.get('expected_output', 'this can be explained through established theories.')}"

    def _generate_math_response(self, prompt: str, context: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ•°å­¦é—®é¢˜å“åº”"""
        if "gemini" in self.model_name.lower():
            return f"Let me solve this step-by-step: First, I'll identify the known values. Then I'll apply the appropriate mathematical operations. The final answer is {context.get('ground_truth', 'calculated result')}."
        elif "claude" in self.model_name.lower():
            return f"To solve this problem: I need to work through the calculations systematically. The answer is {context.get('ground_truth', 'the computed value')}."
        else:
            return f"Solving this mathematically: {context.get('ground_truth', 'result obtained through calculation')}."

    def _calculate_cost(self, model_key: str, input_tokens: float, output_tokens: float) -> float:
        """è®¡ç®—APIè°ƒç”¨æˆæœ¬"""
        if model_key not in self.pricing:
            model_key = "gpt-3.5-turbo"  # é»˜è®¤ä»·æ ¼

        pricing = self.pricing[model_key]
        input_cost = (input_tokens / 1000) * pricing["input"]
        output_cost = (output_tokens / 1000) * pricing["output"]

        return input_cost + output_cost


class ScalableLLMJudge:
    """å¯æ‰©å±•çš„LLMè¯„åˆ¤è€…ï¼Œæ”¯æŒæ‰¹é‡è¯„ä¼°"""

    def __init__(self, judge_provider: APILLMProvider = None):
        self.judge_provider = judge_provider
        self.evaluation_count = 0

    async def evaluate_responses(
        self,
        question: str,
        response_a: Dict[str, Any],
        response_b: Dict[str, Any],
        provider_a: str,
        provider_b: str,
        context: Dict[str, Any] = None,
    ) -> Dict[str, Any]:
        """è¯„ä¼°ä¸¤ä¸ªå“åº”"""

        # æ„å»ºè¯„ä¼°æç¤º
        judge_prompt = self._build_judge_prompt(
            question, response_a["content"], response_b["content"], provider_a, provider_b, context
        )

        # ä½¿ç”¨LLMè¿›è¡Œè¯„ä¼°
        if self.judge_provider and self.judge_provider.api_key:
            try:
                judge_response = await self.judge_provider.generate_response(judge_prompt)
                evaluation = self._parse_judge_response(judge_response["content"])
                evaluation["judge_cost"] = judge_response["cost"]
            except Exception as e:
                logger.warning(f"Judge APIè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨å¯å‘å¼è¯„ä¼°: {e}")
                evaluation = self._heuristic_evaluation(
                    question, response_a["content"], response_b["content"], context
                )
                evaluation["judge_cost"] = 0.0
        else:
            # ä½¿ç”¨å¯å‘å¼è¯„ä¼°
            evaluation = self._heuristic_evaluation(
                question, response_a["content"], response_b["content"], context
            )
            evaluation["judge_cost"] = 0.0

        self.evaluation_count += 1
        return evaluation

    def _build_judge_prompt(
        self,
        question: str,
        response_a: str,
        response_b: str,
        provider_a: str,
        provider_b: str,
        context: Dict[str, Any],
    ) -> str:
        """æ„å»ºè¯„åˆ¤æç¤º"""
        question_type = context.get("category", "general") if context else "general"

        if question_type == "science":
            criteria = "accuracy (scientific correctness), reasoning (quality of explanation), clarity (how well explained)"
        elif question_type == "math":
            criteria = "accuracy (mathematical correctness), methodology (solution approach), clarity (explanation quality)"
        else:
            criteria = "accuracy (factual correctness), helpfulness (usefulness), clarity (communication quality)"

        return f"""
Please evaluate these two AI responses to determine which is better.

Question: {question}

Response A ({provider_a}):
{response_a}

Response B ({provider_b}):
{response_b}

Evaluate based on: {criteria}

Please respond with a JSON object containing:
{{
    "winner": "A" or "B" or "Tie",
    "confidence": number between 0.5 and 1.0,
    "reasoning": "brief explanation of your choice",
    "scores": {{
        "response_a": number between 1-10,
        "response_b": number between 1-10
    }}
}}
"""

    def _parse_judge_response(self, response: str) -> Dict[str, Any]:
        """è§£æJudgeå“åº”"""
        try:
            # å°è¯•æå–JSON
            import re

            json_match = re.search(r"\{.*\}", response, re.DOTALL)
            if json_match:
                eval_data = json.loads(json_match.group())
                return eval_data
        except:
            pass

        # å¦‚æœJSONè§£æå¤±è´¥ï¼Œä½¿ç”¨å¯å‘å¼è§£æ
        if "response a" in response.lower() or "a is better" in response.lower():
            winner = "A"
        elif "response b" in response.lower() or "b is better" in response.lower():
            winner = "B"
        else:
            winner = "Tie"

        return {
            "winner": winner,
            "confidence": 0.7,
            "reasoning": "Parsed from judge response",
            "scores": {"response_a": 7, "response_b": 7},
        }

    def _heuristic_evaluation(
        self, question: str, response_a: str, response_b: str, context: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """å¯å‘å¼è¯„ä¼°æ–¹æ³•"""

        # åŸºç¡€è¯„åˆ†å› å­
        len_a, len_b = len(response_a), len(response_b)

        # æ£€æŸ¥æ˜¯å¦åŒ…å«é¢„æœŸç­”æ¡ˆ
        score_a = 7.0
        score_b = 7.0

        if context:
            expected = context.get("expected_output", "").lower()
            ground_truth = str(context.get("ground_truth", "")).lower()

            if expected in response_a.lower() or ground_truth in response_a.lower():
                score_a += 1.5
            if expected in response_b.lower() or ground_truth in response_b.lower():
                score_b += 1.5

        # é•¿åº¦å’Œè¯¦ç»†ç¨‹åº¦è¯„ä¼°
        if len_a > len_b * 1.5:
            score_a += 0.5  # å¥–åŠ±è¯¦ç»†å›ç­”
        elif len_b > len_a * 1.5:
            score_b += 0.5

        # ç»“æ„åŒ–å›ç­”å¥–åŠ±
        if any(word in response_a.lower() for word in ["step", "first", "then", "because"]):
            score_a += 0.5
        if any(word in response_b.lower() for word in ["step", "first", "then", "because"]):
            score_b += 0.5

        # ç¡®å®šè·èƒœè€…
        if score_a > score_b:
            winner = "A"
            confidence = min(0.9, 0.6 + (score_a - score_b) / 10)
        elif score_b > score_a:
            winner = "B"
            confidence = min(0.9, 0.6 + (score_b - score_a) / 10)
        else:
            winner = "Tie"
            confidence = 0.5

        return {
            "winner": winner,
            "confidence": round(confidence, 2),
            "reasoning": f"Score A: {score_a:.1f}, Score B: {score_b:.1f}",
            "scores": {"response_a": round(score_a, 1), "response_b": round(score_b, 1)},
        }


class LargeScaleTestRunner:
    """å¤§è§„æ¨¡æµ‹è¯•è¿è¡Œå™¨"""

    def __init__(self):
        # åˆå§‹åŒ–ä¾¿å®œçš„æ¨¡å‹æä¾›å•†
        self.provider_a = APILLMProvider("Gemini", "gemini-flash")
        self.provider_b = APILLMProvider("Claude", "claude-sonnet")

        # åˆå§‹åŒ–è¯„åˆ¤è€…ï¼ˆä¹Ÿä½¿ç”¨ä¾¿å®œçš„æ¨¡å‹ï¼‰
        self.judge = ScalableLLMJudge(APILLMProvider("GPT", "gpt-3.5-turbo"))

        self.test_results = []
        self.batch_size = 10  # æ‰¹å¤„ç†å¤§å°

    def load_test_samples(self, sample_count: int = 100) -> List[Dict[str, Any]]:
        """åŠ è½½æµ‹è¯•æ ·æœ¬"""
        datasets_dir = Path("data/processed")
        all_samples = []

        # ä»ARCæ•°æ®é›†åŠ è½½
        arc_file = datasets_dir / "arc_easy.json"
        if arc_file.exists():
            with open(arc_file, "r", encoding="utf-8") as f:
                arc_data = json.load(f)
            all_samples.extend(arc_data)

        # ä»GSM8Kæ•°æ®é›†åŠ è½½
        gsm8k_file = datasets_dir / "gsm8k.json"
        if gsm8k_file.exists():
            with open(gsm8k_file, "r", encoding="utf-8") as f:
                gsm8k_data = json.load(f)
            all_samples.extend(gsm8k_data)

        # éšæœºé€‰æ‹©æŒ‡å®šæ•°é‡çš„æ ·æœ¬
        if len(all_samples) > sample_count:
            return random.sample(all_samples, sample_count)
        else:
            return all_samples

    async def process_batch(
        self, batch: List[Dict[str, Any]], batch_num: int
    ) -> List[Dict[str, Any]]:
        """å¤„ç†ä¸€ä¸ªæ‰¹æ¬¡çš„æ ·æœ¬"""
        logger.info(f"ğŸ”„ å¤„ç†æ‰¹æ¬¡ {batch_num}: {len(batch)} ä¸ªæ ·æœ¬")

        batch_results = []
        for i, sample in enumerate(batch):
            try:
                result = await self.evaluate_single_sample(
                    sample, batch_num * self.batch_size + i + 1
                )
                batch_results.append(result)

                # æ˜¾ç¤ºè¿›åº¦
                if (i + 1) % 5 == 0:
                    logger.info(f"  âœ… æ‰¹æ¬¡ {batch_num} è¿›åº¦: {i + 1}/{len(batch)}")

            except Exception as e:
                logger.error(f"âŒ æ ·æœ¬è¯„ä¼°å¤±è´¥: {e}")
                continue

        return batch_results

    async def evaluate_single_sample(
        self, sample: Dict[str, Any], sample_num: int
    ) -> Dict[str, Any]:
        """è¯„ä¼°å•ä¸ªæ ·æœ¬"""
        question = sample["prompt"]

        # è·å–ä¸¤ä¸ªæä¾›å•†çš„å“åº”
        start_time = time.time()

        # å¹¶å‘è·å–å“åº”
        response_a_task = self.provider_a.generate_response(question, sample)
        response_b_task = self.provider_b.generate_response(question, sample)

        response_a, response_b = await asyncio.gather(response_a_task, response_b_task)

        response_time = time.time() - start_time

        # LLMè¯„ä¼°
        eval_start_time = time.time()
        evaluation = await self.judge.evaluate_responses(
            question,
            response_a,
            response_b,
            self.provider_a.provider_name,
            self.provider_b.provider_name,
            sample,
        )
        eval_time = time.time() - eval_start_time

        # è®¡ç®—æ€»æˆæœ¬
        total_cost = response_a["cost"] + response_b["cost"] + evaluation.get("judge_cost", 0.0)

        return {
            "sample_num": sample_num,
            "sample_data": {
                "id": sample["id"],
                "category": sample.get("category", "unknown"),
                "source": sample.get("source", "unknown"),
            },
            "responses": {
                "provider_a": {
                    "name": self.provider_a.provider_name,
                    "model": self.provider_a.model_name,
                    "content": (
                        response_a["content"][:200] + "..."
                        if len(response_a["content"]) > 200
                        else response_a["content"]
                    ),
                    "tokens": response_a["input_tokens"] + response_a["output_tokens"],
                    "cost": response_a["cost"],
                },
                "provider_b": {
                    "name": self.provider_b.provider_name,
                    "model": self.provider_b.model_name,
                    "content": (
                        response_b["content"][:200] + "..."
                        if len(response_b["content"]) > 200
                        else response_b["content"]
                    ),
                    "tokens": response_b["input_tokens"] + response_b["output_tokens"],
                    "cost": response_b["cost"],
                },
            },
            "evaluation": evaluation,
            "timing": {
                "response_time": round(response_time, 3),
                "evaluation_time": round(eval_time, 3),
            },
            "total_cost": round(total_cost, 6),
            "timestamp": datetime.now().isoformat(),
        }

    async def run_large_scale_test(self, sample_count: int = 100) -> Dict[str, Any]:
        """è¿è¡Œå¤§è§„æ¨¡æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹å¤§è§„æ¨¡LLM as a Judgeæµ‹è¯•")
        logger.info(f"ğŸ“Š ç›®æ ‡æ ·æœ¬æ•°: {sample_count}")
        logger.info(
            f"ğŸ¤– Provider A: {self.provider_a.provider_name} ({self.provider_a.model_name})"
        )
        logger.info(
            f"ğŸ¤– Provider B: {self.provider_b.provider_name} ({self.provider_b.model_name})"
        )
        logger.info("=" * 80)

        # åŠ è½½æµ‹è¯•æ•°æ®
        test_samples = self.load_test_samples(sample_count)
        actual_count = len(test_samples)
        logger.info(f"ğŸ“š å·²åŠ è½½ {actual_count} ä¸ªæµ‹è¯•æ ·æœ¬")

        if actual_count == 0:
            logger.error("âŒ æ²¡æœ‰æ‰¾åˆ°æµ‹è¯•æ ·æœ¬")
            return {}

        # åˆ†æ‰¹å¤„ç†
        batches = [
            test_samples[i : i + self.batch_size]
            for i in range(0, len(test_samples), self.batch_size)
        ]

        overall_start_time = time.time()

        # å¹¶å‘å¤„ç†æ‰¹æ¬¡ï¼ˆé™åˆ¶å¹¶å‘æ•°ï¼‰
        semaphore = asyncio.Semaphore(3)  # æœ€å¤š3ä¸ªå¹¶å‘æ‰¹æ¬¡

        async def process_batch_with_semaphore(batch, batch_num):
            async with semaphore:
                return await self.process_batch(batch, batch_num)

        # æ‰§è¡Œæ‰€æœ‰æ‰¹æ¬¡
        batch_tasks = [process_batch_with_semaphore(batch, i) for i, batch in enumerate(batches)]
        batch_results = await asyncio.gather(*batch_tasks)

        # åˆå¹¶ç»“æœ
        for batch_result in batch_results:
            self.test_results.extend(batch_result)

        total_time = time.time() - overall_start_time

        # ç”Ÿæˆæ±‡æ€»
        summary = self._generate_large_scale_summary(total_time, actual_count)

        # ä¿å­˜ç»“æœ
        self._save_large_scale_results(summary)

        return summary

    def _generate_large_scale_summary(
        self, total_time: float, expected_count: int
    ) -> Dict[str, Any]:
        """ç”Ÿæˆå¤§è§„æ¨¡æµ‹è¯•æ±‡æ€»"""
        if not self.test_results:
            return {}

        actual_count = len(self.test_results)

        # åŸºç¡€ç»Ÿè®¡
        provider_a_wins = sum(1 for r in self.test_results if r["evaluation"]["winner"] == "A")
        provider_b_wins = sum(1 for r in self.test_results if r["evaluation"]["winner"] == "B")
        ties = sum(1 for r in self.test_results if r["evaluation"]["winner"] == "Tie")

        # æˆæœ¬åˆ†æ
        total_cost = sum(r["total_cost"] for r in self.test_results)
        avg_cost_per_sample = total_cost / actual_count if actual_count > 0 else 0

        provider_a_cost = self.provider_a.total_cost
        provider_b_cost = self.provider_b.total_cost
        judge_cost = sum(r["evaluation"].get("judge_cost", 0) for r in self.test_results)

        # æ€§èƒ½æŒ‡æ ‡
        avg_response_time = (
            sum(r["timing"]["response_time"] for r in self.test_results) / actual_count
        )
        avg_eval_time = (
            sum(r["timing"]["evaluation_time"] for r in self.test_results) / actual_count
        )

        # æŒ‰ç±»å‹åˆ†æ
        type_analysis = {}
        source_analysis = {}

        for result in self.test_results:
            # æŒ‰ç±»åˆ«åˆ†æ
            category = result["sample_data"]["category"]
            if category not in type_analysis:
                type_analysis[category] = {"A": 0, "B": 0, "Tie": 0, "total": 0}

            winner = result["evaluation"]["winner"]
            type_analysis[category][winner] += 1
            type_analysis[category]["total"] += 1

            # æŒ‰æ•°æ®æºåˆ†æ
            source = result["sample_data"]["source"]
            if source not in source_analysis:
                source_analysis[source] = {"A": 0, "B": 0, "Tie": 0, "total": 0}

            source_analysis[source][winner] += 1
            source_analysis[source]["total"] += 1

        # æˆåŠŸç‡åˆ†æ
        success_rate = actual_count / expected_count if expected_count > 0 else 1.0

        return {
            "test_info": {
                "test_name": "Large Scale LLM as a Judge Test",
                "timestamp": datetime.now().isoformat(),
                "target_samples": expected_count,
                "actual_samples": actual_count,
                "success_rate": round(success_rate, 3),
                "total_time": round(total_time, 2),
            },
            "providers": {
                "provider_a": {
                    "name": self.provider_a.provider_name,
                    "model": self.provider_a.model_name,
                    "requests": self.provider_a.request_count,
                    "total_cost": round(provider_a_cost, 4),
                },
                "provider_b": {
                    "name": self.provider_b.provider_name,
                    "model": self.provider_b.model_name,
                    "requests": self.provider_b.request_count,
                    "total_cost": round(provider_b_cost, 4),
                },
            },
            "results": {
                "provider_a_wins": provider_a_wins,
                "provider_b_wins": provider_b_wins,
                "ties": ties,
                "win_rate_a": round(provider_a_wins / actual_count, 3),
                "win_rate_b": round(provider_b_wins / actual_count, 3),
            },
            "cost_analysis": {
                "total_cost": round(total_cost, 4),
                "avg_cost_per_sample": round(avg_cost_per_sample, 5),
                "provider_a_cost": round(provider_a_cost, 4),
                "provider_b_cost": round(provider_b_cost, 4),
                "judge_cost": round(judge_cost, 4),
                "cost_breakdown": {
                    "responses": (
                        round((provider_a_cost + provider_b_cost) / total_cost * 100, 1)
                        if total_cost > 0
                        else 0
                    ),
                    "evaluation": round(judge_cost / total_cost * 100, 1) if total_cost > 0 else 0,
                },
            },
            "performance": {
                "avg_response_time": round(avg_response_time, 3),
                "avg_evaluation_time": round(avg_eval_time, 3),
                "throughput": round(actual_count / total_time, 2),  # samples per second
                "total_time": round(total_time, 2),
            },
            "analysis": {"by_category": type_analysis, "by_source": source_analysis},
            "sample_results": self.test_results[:10],  # åªä¿å­˜å‰10ä¸ªè¯¦ç»†ç»“æœ
        }

    def _save_large_scale_results(self, summary: Dict[str, Any]):
        """ä¿å­˜å¤§è§„æ¨¡æµ‹è¯•ç»“æœ"""
        results_dir = Path("logs/large_scale_results")
        results_dir.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = results_dir / f"large_scale_test_{timestamp}.json"

        with open(results_file, "w", encoding="utf-8") as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)

        logger.info(f"\nğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {results_file}")

    def display_large_scale_summary(self, summary: Dict[str, Any]):
        """æ˜¾ç¤ºå¤§è§„æ¨¡æµ‹è¯•æ±‡æ€»"""
        if not summary:
            return

        logger.info("\n" + "=" * 80)
        logger.info("ğŸ“Š å¤§è§„æ¨¡ LLM AS A JUDGE æµ‹è¯•æ±‡æ€»")
        logger.info("=" * 80)

        test_info = summary["test_info"]
        providers = summary["providers"]
        results = summary["results"]
        cost = summary["cost_analysis"]
        performance = summary["performance"]
        analysis = summary["analysis"]

        # åŸºæœ¬ä¿¡æ¯
        logger.info(f"ğŸ•’ æµ‹è¯•æ—¶é—´: {test_info['timestamp']}")
        logger.info(f"ğŸ“Š ç›®æ ‡æ ·æœ¬: {test_info['target_samples']}")
        logger.info(f"âœ… å®é™…å®Œæˆ: {test_info['actual_samples']} ({test_info['success_rate']:.1%})")
        logger.info(f"â±ï¸ æ€»è€—æ—¶: {test_info['total_time']}ç§’")
        logger.info(f"ğŸš€ ååé‡: {performance['throughput']} æ ·æœ¬/ç§’")

        # æä¾›å•†ä¿¡æ¯
        logger.info(f"\nğŸ¤– æµ‹è¯•æä¾›å•†:")
        logger.info(
            f"  Provider A: {providers['provider_a']['name']} ({providers['provider_a']['model']})"
        )
        logger.info(
            f"    è¯·æ±‚æ•°: {providers['provider_a']['requests']}, æˆæœ¬: ${providers['provider_a']['total_cost']}"
        )
        logger.info(
            f"  Provider B: {providers['provider_b']['name']} ({providers['provider_b']['model']})"
        )
        logger.info(
            f"    è¯·æ±‚æ•°: {providers['provider_b']['requests']}, æˆæœ¬: ${providers['provider_b']['total_cost']}"
        )

        # æ¯”èµ›ç»“æœ
        logger.info(f"\nğŸ† æ¯”èµ›ç»“æœ:")
        logger.info(
            f"  Provider A è·èƒœ: {results['provider_a_wins']} æ¬¡ ({results['win_rate_a']:.1%})"
        )
        logger.info(
            f"  Provider B è·èƒœ: {results['provider_b_wins']} æ¬¡ ({results['win_rate_b']:.1%})"
        )
        logger.info(f"  å¹³å±€: {results['ties']} æ¬¡")

        # æˆæœ¬åˆ†æ
        logger.info(f"\nğŸ’° æˆæœ¬åˆ†æ:")
        logger.info(f"  æ€»æˆæœ¬: ${cost['total_cost']}")
        logger.info(f"  æ¯æ ·æœ¬å¹³å‡æˆæœ¬: ${cost['avg_cost_per_sample']}")
        logger.info(
            f"  æˆæœ¬åˆ†å¸ƒ: å“åº”ç”Ÿæˆ {cost['cost_breakdown']['responses']}%, è¯„ä¼° {cost['cost_breakdown']['evaluation']}%"
        )

        # æŒ‰ç±»å‹åˆ†æ
        logger.info(f"\nğŸ“‹ æŒ‰ç±»å‹åˆ†æ:")
        for category, data in analysis["by_category"].items():
            total = data["total"]
            logger.info(f"  {category.upper()}ç±»å‹ (å…±{total}é¢˜):")
            logger.info(f"    Provider A: {data['A']} èƒœ ({data['A']/total:.1%})")
            logger.info(f"    Provider B: {data['B']} èƒœ ({data['B']/total:.1%})")
            if data.get("Tie", 0) > 0:
                logger.info(f"    å¹³å±€: {data['Tie']} æ¬¡")

        # æ€§èƒ½æŒ‡æ ‡
        logger.info(f"\nâš¡ æ€§èƒ½æŒ‡æ ‡:")
        logger.info(f"  å¹³å‡å“åº”æ—¶é—´: {performance['avg_response_time']}ç§’")
        logger.info(f"  å¹³å‡è¯„ä¼°æ—¶é—´: {performance['avg_evaluation_time']}ç§’")

        # ç»“è®º
        logger.info(f"\nğŸ¯ æµ‹è¯•ç»“è®º:")
        if results["provider_a_wins"] > results["provider_b_wins"]:
            winner = providers["provider_a"]["name"]
            winner_rate = results["win_rate_a"]
        elif results["provider_b_wins"] > results["provider_a_wins"]:
            winner = providers["provider_b"]["name"]
            winner_rate = results["win_rate_b"]
        else:
            winner = "å¹³å±€"
            winner_rate = 0.5

        if winner_rate >= 0.6:
            logger.info(f"  ğŸ‰ {winner} è¡¨ç°æ˜¾è‘—æ›´å¥½ (èƒœç‡: {winner_rate:.1%})")
        else:
            logger.info(f"  âš–ï¸ ä¸¤ä¸ªæ¨¡å‹è¡¨ç°ç›¸è¿‘")

        logger.info(
            f"  ğŸ’¡ æ€»æˆæœ¬æ§åˆ¶è‰¯å¥½: ${cost['total_cost']} (å¹³å‡æ¯æ ·æœ¬ ${cost['avg_cost_per_sample']})"
        )


async def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸ¯ å¤§è§„æ¨¡LLM as a Judgeè¯„ä¼°æµ‹è¯• (100æ ·æœ¬)")
    logger.info("ä½¿ç”¨ä¾¿å®œæ¨¡å‹: Gemini Flash vs Claude Sonnet")
    logger.info("è¯„åˆ¤è€…: GPT-3.5 Turbo")

    # æ£€æŸ¥APIå¯†é’¥
    api_keys_info = []
    if os.getenv("GEMINI_API_KEY"):
        api_keys_info.append("âœ… Gemini API")
    else:
        api_keys_info.append("âš ï¸ Gemini API (å°†ä½¿ç”¨æ¨¡æ‹Ÿ)")

    if os.getenv("ANTHROPIC_API_KEY"):
        api_keys_info.append("âœ… Claude API")
    else:
        api_keys_info.append("âš ï¸ Claude API (å°†ä½¿ç”¨æ¨¡æ‹Ÿ)")

    if os.getenv("OPENAI_API_KEY"):
        api_keys_info.append("âœ… OpenAI API")
    else:
        api_keys_info.append("âš ï¸ OpenAI API (å°†ä½¿ç”¨æ¨¡æ‹Ÿ)")

    logger.info("ğŸ”‘ APIå¯†é’¥çŠ¶æ€: " + " | ".join(api_keys_info))

    # åˆ›å»ºå¹¶è¿è¡Œæµ‹è¯•
    tester = LargeScaleTestRunner()
    summary = await tester.run_large_scale_test(sample_count=100)

    if summary:
        # æ˜¾ç¤ºæ±‡æ€»ç»“æœ
        tester.display_large_scale_summary(summary)
        logger.info("\nâœ… å¤§è§„æ¨¡æµ‹è¯•å®Œæˆï¼")
    else:
        logger.error("âŒ æµ‹è¯•å¤±è´¥")

    return summary


if __name__ == "__main__":
    asyncio.run(main())
