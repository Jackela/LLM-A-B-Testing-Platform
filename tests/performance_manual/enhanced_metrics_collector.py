#!/usr/bin/env python3
"""
Enhanced Performance Metrics Collector
å¢å¼ºçš„æ€§èƒ½æŒ‡æ ‡æ”¶é›†å™¨ï¼Œä¸“ä¸ºLLM A/Bæµ‹è¯•å¹³å°ä¼˜åŒ–
"""

import asyncio
import json
import logging
import statistics
import threading
import time
from collections import defaultdict, deque
from concurrent.futures import ThreadPoolExecutor
from dataclasses import asdict, dataclass, field
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional, Union

import numpy as np
import psutil

# Configure logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)


@dataclass
class PerformanceSnapshot:
    """æ€§èƒ½å¿«ç…§æ•°æ®ç±»"""

    timestamp: datetime

    # å“åº”æ—¶é—´æŒ‡æ ‡
    response_time_ms: float
    request_id: str
    endpoint: str
    method: str
    status_code: int

    # ç³»ç»Ÿèµ„æºæŒ‡æ ‡
    cpu_percent: float
    memory_mb: float
    memory_percent: float

    # ç½‘ç»œæŒ‡æ ‡
    request_size_bytes: int = 0
    response_size_bytes: int = 0

    # åº”ç”¨ç‰¹å®šæŒ‡æ ‡
    provider: Optional[str] = None
    model: Optional[str] = None
    tokens_used: Optional[int] = None
    cost: Optional[float] = None

    # ç¼“å­˜æŒ‡æ ‡
    cache_hit: Optional[bool] = None
    cache_key: Optional[str] = None

    # è‡ªå®šä¹‰æŒ‡æ ‡
    custom_metrics: Dict[str, Any] = field(default_factory=dict)


@dataclass
class PerformanceBaseline:
    """æ€§èƒ½åŸºå‡†æ•°æ®ç±»"""

    name: str
    created_at: datetime

    # å“åº”æ—¶é—´åŸºå‡†
    avg_response_time_ms: float
    p50_response_time_ms: float
    p95_response_time_ms: float
    p99_response_time_ms: float

    # ååé‡åŸºå‡†
    max_throughput_rps: float
    sustainable_throughput_rps: float

    # èµ„æºä½¿ç”¨åŸºå‡†
    baseline_cpu_percent: float
    baseline_memory_mb: float

    # æˆæœ¬åŸºå‡†
    avg_cost_per_request: float
    cost_efficiency_score: float

    # è´¨é‡åŸºå‡†
    error_rate_threshold: float
    success_rate_target: float

    # å…ƒæ•°æ®
    test_conditions: Dict[str, Any] = field(default_factory=dict)
    validation_data: Dict[str, Any] = field(default_factory=dict)


class EnhancedMetricsCollector:
    """å¢å¼ºçš„æ€§èƒ½æŒ‡æ ‡æ”¶é›†å™¨"""

    def __init__(
        self,
        collection_interval: float = 1.0,
        max_snapshots: int = 10000,
        baseline_file: Optional[Path] = None,
    ):

        self.collection_interval = collection_interval
        self.max_snapshots = max_snapshots
        self.baseline_file = baseline_file or Path("performance_baselines.json")

        # æ•°æ®å­˜å‚¨
        self.snapshots: deque = deque(maxlen=max_snapshots)
        self.baselines: Dict[str, PerformanceBaseline] = {}

        # å®æ—¶ç»Ÿè®¡
        self.current_stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "total_response_time": 0.0,
            "total_cost": 0.0,
            "cache_hits": 0,
            "cache_misses": 0,
        }

        # æ»‘åŠ¨çª—å£ç»Ÿè®¡
        self.window_size = 100
        self.response_times_window = deque(maxlen=self.window_size)
        self.throughput_window = deque(maxlen=60)  # 1åˆ†é’Ÿçª—å£

        # ç›‘æ§æ§åˆ¶
        self._monitoring = False
        self._monitor_thread = None
        self._stop_event = threading.Event()

        # å›è°ƒå‡½æ•°
        self.alert_callbacks: List[Callable] = []
        self.real_time_callbacks: List[Callable] = []

        # æ€§èƒ½é˜ˆå€¼
        self.thresholds = {
            "response_time_p95_ms": 2000,
            "error_rate_percent": 5.0,
            "cpu_percent": 80.0,
            "memory_percent": 85.0,
            "cost_per_request": 0.01,
        }

        # åŠ è½½ç°æœ‰åŸºå‡†
        self._load_baselines()

    def start_monitoring(self):
        """å¯åŠ¨æ€§èƒ½ç›‘æ§"""
        if self._monitoring:
            return

        self._monitoring = True
        self._stop_event.clear()

        self._monitor_thread = threading.Thread(target=self._monitor_system_resources)
        self._monitor_thread.daemon = True
        self._monitor_thread.start()

        logger.info("ğŸ” å¢å¼ºæ€§èƒ½ç›‘æ§å·²å¯åŠ¨")

    def stop_monitoring(self):
        """åœæ­¢æ€§èƒ½ç›‘æ§"""
        if not self._monitoring:
            return

        self._monitoring = False
        self._stop_event.set()

        if self._monitor_thread:
            self._monitor_thread.join(timeout=2.0)

        logger.info("ğŸ“Š å¢å¼ºæ€§èƒ½ç›‘æ§å·²åœæ­¢")

    def _monitor_system_resources(self):
        """ç³»ç»Ÿèµ„æºç›‘æ§çº¿ç¨‹"""
        while not self._stop_event.is_set():
            try:
                # æ”¶é›†ç³»ç»ŸæŒ‡æ ‡
                cpu_percent = psutil.cpu_percent(interval=None)
                memory = psutil.virtual_memory()

                # æ›´æ–°æ»‘åŠ¨çª—å£
                current_time = datetime.now()
                requests_in_window = len(
                    [s for s in self.snapshots if (current_time - s.timestamp).total_seconds() < 60]
                )
                self.throughput_window.append(requests_in_window)

                # æ£€æŸ¥é˜ˆå€¼å¹¶è§¦å‘å‘Šè­¦
                self._check_thresholds(cpu_percent, memory.percent)

                # è°ƒç”¨å®æ—¶å›è°ƒ
                for callback in self.real_time_callbacks:
                    try:
                        callback(
                            {
                                "timestamp": current_time,
                                "cpu_percent": cpu_percent,
                                "memory_percent": memory.percent,
                                "throughput_rps": (
                                    statistics.mean(self.throughput_window)
                                    if self.throughput_window
                                    else 0
                                ),
                                "avg_response_time": self._get_current_avg_response_time(),
                            }
                        )
                    except Exception as e:
                        logger.warning(f"å®æ—¶å›è°ƒæ‰§è¡Œå¤±è´¥: {e}")

                self._stop_event.wait(self.collection_interval)

            except Exception as e:
                logger.error(f"ç³»ç»Ÿèµ„æºç›‘æ§é”™è¯¯: {e}")
                self._stop_event.wait(self.collection_interval)

    def record_request(
        self,
        endpoint: str,
        method: str,
        response_time_ms: float,
        status_code: int,
        request_size: int = 0,
        response_size: int = 0,
        provider: Optional[str] = None,
        model: Optional[str] = None,
        tokens_used: Optional[int] = None,
        cost: Optional[float] = None,
        cache_hit: Optional[bool] = None,
        cache_key: Optional[str] = None,
        custom_metrics: Optional[Dict[str, Any]] = None,
        request_id: Optional[str] = None,
    ) -> PerformanceSnapshot:
        """è®°å½•å•ä¸ªè¯·æ±‚çš„æ€§èƒ½æ•°æ®"""

        # æ”¶é›†ç³»ç»ŸæŒ‡æ ‡
        cpu_percent = psutil.cpu_percent(interval=None)
        memory = psutil.virtual_memory()

        # åˆ›å»ºæ€§èƒ½å¿«ç…§
        snapshot = PerformanceSnapshot(
            timestamp=datetime.now(),
            response_time_ms=response_time_ms,
            request_id=request_id or f"req_{int(time.time() * 1000)}",
            endpoint=endpoint,
            method=method,
            status_code=status_code,
            cpu_percent=cpu_percent,
            memory_mb=memory.used / 1024 / 1024,
            memory_percent=memory.percent,
            request_size_bytes=request_size,
            response_size_bytes=response_size,
            provider=provider,
            model=model,
            tokens_used=tokens_used,
            cost=cost,
            cache_hit=cache_hit,
            cache_key=cache_key,
            custom_metrics=custom_metrics or {},
        )

        # å­˜å‚¨å¿«ç…§
        self.snapshots.append(snapshot)

        # æ›´æ–°å®æ—¶ç»Ÿè®¡
        self._update_real_time_stats(snapshot)

        # æ›´æ–°æ»‘åŠ¨çª—å£
        self.response_times_window.append(response_time_ms)

        return snapshot

    def _update_real_time_stats(self, snapshot: PerformanceSnapshot):
        """æ›´æ–°å®æ—¶ç»Ÿè®¡æ•°æ®"""
        self.current_stats["total_requests"] += 1

        if 200 <= snapshot.status_code < 400:
            self.current_stats["successful_requests"] += 1
        else:
            self.current_stats["failed_requests"] += 1

        self.current_stats["total_response_time"] += snapshot.response_time_ms

        if snapshot.cost:
            self.current_stats["total_cost"] += snapshot.cost

        if snapshot.cache_hit is not None:
            if snapshot.cache_hit:
                self.current_stats["cache_hits"] += 1
            else:
                self.current_stats["cache_misses"] += 1

    def _check_thresholds(self, cpu_percent: float, memory_percent: float):
        """æ£€æŸ¥æ€§èƒ½é˜ˆå€¼å¹¶è§¦å‘å‘Šè­¦"""
        alerts = []

        # CPUå‘Šè­¦
        if cpu_percent > self.thresholds["cpu_percent"]:
            alerts.append(
                {
                    "type": "cpu_high",
                    "value": cpu_percent,
                    "threshold": self.thresholds["cpu_percent"],
                    "severity": "warning",
                }
            )

        # å†…å­˜å‘Šè­¦
        if memory_percent > self.thresholds["memory_percent"]:
            alerts.append(
                {
                    "type": "memory_high",
                    "value": memory_percent,
                    "threshold": self.thresholds["memory_percent"],
                    "severity": "warning",
                }
            )

        # å“åº”æ—¶é—´å‘Šè­¦
        if len(self.response_times_window) >= 10:
            p95_time = np.percentile(list(self.response_times_window), 95)
            if p95_time > self.thresholds["response_time_p95_ms"]:
                alerts.append(
                    {
                        "type": "response_time_high",
                        "value": p95_time,
                        "threshold": self.thresholds["response_time_p95_ms"],
                        "severity": "critical",
                    }
                )

        # é”™è¯¯ç‡å‘Šè­¦
        if self.current_stats["total_requests"] >= 10:
            error_rate = (
                self.current_stats["failed_requests"] / self.current_stats["total_requests"]
            ) * 100
            if error_rate > self.thresholds["error_rate_percent"]:
                alerts.append(
                    {
                        "type": "error_rate_high",
                        "value": error_rate,
                        "threshold": self.thresholds["error_rate_percent"],
                        "severity": "critical",
                    }
                )

        # è§¦å‘å‘Šè­¦å›è°ƒ
        for alert in alerts:
            for callback in self.alert_callbacks:
                try:
                    callback(alert)
                except Exception as e:
                    logger.warning(f"å‘Šè­¦å›è°ƒæ‰§è¡Œå¤±è´¥: {e}")

    def _get_current_avg_response_time(self) -> float:
        """è·å–å½“å‰å¹³å‡å“åº”æ—¶é—´"""
        if not self.response_times_window:
            return 0.0
        return statistics.mean(self.response_times_window)

    def create_baseline(
        self,
        name: str,
        test_duration_minutes: int = 10,
        target_rps: int = 100,
        test_conditions: Optional[Dict[str, Any]] = None,
    ) -> PerformanceBaseline:
        """åˆ›å»ºæ€§èƒ½åŸºå‡†"""

        logger.info(f"ğŸ¯ å¼€å§‹åˆ›å»ºæ€§èƒ½åŸºå‡†: {name}")

        # æ¸…ç©ºç°æœ‰æ•°æ®
        initial_snapshots = len(self.snapshots)

        # ç­‰å¾…æ”¶é›†è¶³å¤Ÿæ•°æ®
        end_time = datetime.now() + timedelta(minutes=test_duration_minutes)

        while datetime.now() < end_time:
            await asyncio.sleep(1)

            if len(self.snapshots) - initial_snapshots >= target_rps * test_duration_minutes:
                break

        # åˆ†ææ”¶é›†çš„æ•°æ®
        recent_snapshots = list(self.snapshots)[initial_snapshots:]

        if not recent_snapshots:
            raise ValueError("æ²¡æœ‰æ”¶é›†åˆ°è¶³å¤Ÿçš„æ€§èƒ½æ•°æ®æ¥åˆ›å»ºåŸºå‡†")

        # è®¡ç®—åŸºå‡†æŒ‡æ ‡
        response_times = [
            s.response_time_ms for s in recent_snapshots if 200 <= s.status_code < 400
        ]

        if not response_times:
            raise ValueError("æ²¡æœ‰æˆåŠŸçš„è¯·æ±‚æ•°æ®æ¥åˆ›å»ºåŸºå‡†")

        avg_response_time = statistics.mean(response_times)
        p50_response_time = np.percentile(response_times, 50)
        p95_response_time = np.percentile(response_times, 95)
        p99_response_time = np.percentile(response_times, 99)

        # è®¡ç®—ååé‡
        duration_seconds = test_duration_minutes * 60
        max_throughput = len(recent_snapshots) / duration_seconds
        sustainable_throughput = len(response_times) / duration_seconds

        # è®¡ç®—èµ„æºä½¿ç”¨åŸºå‡†
        cpu_values = [s.cpu_percent for s in recent_snapshots]
        memory_values = [s.memory_mb for s in recent_snapshots]

        baseline_cpu = statistics.mean(cpu_values)
        baseline_memory = statistics.mean(memory_values)

        # è®¡ç®—æˆæœ¬åŸºå‡†
        costs = [s.cost for s in recent_snapshots if s.cost is not None]
        avg_cost = statistics.mean(costs) if costs else 0.0

        # è®¡ç®—æˆæœ¬æ•ˆç‡å¾—åˆ† (å“åº”æ—¶é—´ vs æˆæœ¬)
        cost_efficiency = (
            (1000 / avg_response_time) / max(avg_cost, 0.001) if avg_cost > 0 else 1000
        )

        # è®¡ç®—è´¨é‡æŒ‡æ ‡
        total_requests = len(recent_snapshots)
        successful_requests = len(response_times)
        error_rate = ((total_requests - successful_requests) / total_requests) * 100
        success_rate = (successful_requests / total_requests) * 100

        # åˆ›å»ºåŸºå‡†å¯¹è±¡
        baseline = PerformanceBaseline(
            name=name,
            created_at=datetime.now(),
            avg_response_time_ms=avg_response_time,
            p50_response_time_ms=p50_response_time,
            p95_response_time_ms=p95_response_time,
            p99_response_time_ms=p99_response_time,
            max_throughput_rps=max_throughput,
            sustainable_throughput_rps=sustainable_throughput,
            baseline_cpu_percent=baseline_cpu,
            baseline_memory_mb=baseline_memory,
            avg_cost_per_request=avg_cost,
            cost_efficiency_score=cost_efficiency,
            error_rate_threshold=max(error_rate, 1.0),  # è‡³å°‘1%çš„é˜ˆå€¼
            success_rate_target=min(success_rate, 99.0),  # æœ€å¤š99%çš„ç›®æ ‡
            test_conditions=test_conditions or {},
            validation_data={
                "total_requests": total_requests,
                "test_duration_minutes": test_duration_minutes,
                "data_points": len(recent_snapshots),
            },
        )

        # ä¿å­˜åŸºå‡†
        self.baselines[name] = baseline
        self._save_baselines()

        logger.info(f"âœ… æ€§èƒ½åŸºå‡†åˆ›å»ºå®Œæˆ: {name}")
        logger.info(f"   å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.1f}ms")
        logger.info(f"   P95å“åº”æ—¶é—´: {p95_response_time:.1f}ms")
        logger.info(f"   å¯æŒç»­ååé‡: {sustainable_throughput:.1f} RPS")
        logger.info(f"   æˆåŠŸç‡: {success_rate:.1f}%")

        return baseline

    def compare_to_baseline(
        self, baseline_name: str, analysis_window_minutes: int = 5
    ) -> Dict[str, Any]:
        """ä¸åŸºå‡†è¿›è¡Œå¯¹æ¯”åˆ†æ"""

        if baseline_name not in self.baselines:
            raise ValueError(f"åŸºå‡† '{baseline_name}' ä¸å­˜åœ¨")

        baseline = self.baselines[baseline_name]

        # è·å–æœ€è¿‘çš„æ€§èƒ½æ•°æ®
        cutoff_time = datetime.now() - timedelta(minutes=analysis_window_minutes)
        recent_snapshots = [s for s in self.snapshots if s.timestamp > cutoff_time]

        if not recent_snapshots:
            raise ValueError("æ²¡æœ‰è¶³å¤Ÿçš„æœ€è¿‘æ€§èƒ½æ•°æ®è¿›è¡Œå¯¹æ¯”")

        # è®¡ç®—å½“å‰æŒ‡æ ‡
        successful_snapshots = [s for s in recent_snapshots if 200 <= s.status_code < 400]
        response_times = [s.response_time_ms for s in successful_snapshots]

        if not response_times:
            raise ValueError("æ²¡æœ‰æˆåŠŸçš„è¯·æ±‚æ•°æ®è¿›è¡Œå¯¹æ¯”")

        current_avg_response = statistics.mean(response_times)
        current_p95_response = np.percentile(response_times, 95)
        current_p99_response = np.percentile(response_times, 99)

        duration_seconds = analysis_window_minutes * 60
        current_throughput = len(successful_snapshots) / duration_seconds

        # è®¡ç®—å¯¹æ¯”ç»“æœ
        comparison = {
            "baseline_name": baseline_name,
            "analysis_period_minutes": analysis_window_minutes,
            "timestamp": datetime.now().isoformat(),
            "response_time_comparison": {
                "avg_response_time": {
                    "current": current_avg_response,
                    "baseline": baseline.avg_response_time_ms,
                    "change_percent": (
                        (current_avg_response - baseline.avg_response_time_ms)
                        / baseline.avg_response_time_ms
                    )
                    * 100,
                    "status": (
                        "improved"
                        if current_avg_response < baseline.avg_response_time_ms
                        else "degraded"
                    ),
                },
                "p95_response_time": {
                    "current": current_p95_response,
                    "baseline": baseline.p95_response_time_ms,
                    "change_percent": (
                        (current_p95_response - baseline.p95_response_time_ms)
                        / baseline.p95_response_time_ms
                    )
                    * 100,
                    "status": (
                        "improved"
                        if current_p95_response < baseline.p95_response_time_ms
                        else "degraded"
                    ),
                },
            },
            "throughput_comparison": {
                "current_rps": current_throughput,
                "baseline_rps": baseline.sustainable_throughput_rps,
                "change_percent": (
                    (current_throughput - baseline.sustainable_throughput_rps)
                    / baseline.sustainable_throughput_rps
                )
                * 100,
                "status": (
                    "improved"
                    if current_throughput > baseline.sustainable_throughput_rps
                    else "degraded"
                ),
            },
            "overall_assessment": self._assess_performance_change(
                current_avg_response,
                baseline.avg_response_time_ms,
                current_throughput,
                baseline.sustainable_throughput_rps,
            ),
        }

        return comparison

    def _assess_performance_change(
        self,
        current_response_time: float,
        baseline_response_time: float,
        current_throughput: float,
        baseline_throughput: float,
    ) -> Dict[str, Any]:
        """è¯„ä¼°æ•´ä½“æ€§èƒ½å˜åŒ–"""

        response_change = (
            (current_response_time - baseline_response_time) / baseline_response_time
        ) * 100
        throughput_change = ((current_throughput - baseline_throughput) / baseline_throughput) * 100

        # è®¡ç®—ç»¼åˆå¾—åˆ†
        performance_score = 0

        # å“åº”æ—¶é—´æƒé‡50%
        if response_change <= -10:  # æ”¹å–„è¶…è¿‡10%
            performance_score += 50
        elif response_change <= 0:  # æœ‰æ”¹å–„
            performance_score += 25
        elif response_change <= 10:  # è½»å¾®é€€åŒ–
            performance_score += 10
        # é€€åŒ–è¶…è¿‡10%å¾—0åˆ†

        # ååé‡æƒé‡50%
        if throughput_change >= 10:  # æ”¹å–„è¶…è¿‡10%
            performance_score += 50
        elif throughput_change >= 0:  # æœ‰æ”¹å–„
            performance_score += 25
        elif throughput_change >= -10:  # è½»å¾®é€€åŒ–
            performance_score += 10
        # é€€åŒ–è¶…è¿‡10%å¾—0åˆ†

        # ç¡®å®šæ•´ä½“çŠ¶æ€
        if performance_score >= 75:
            status = "significantly_improved"
        elif performance_score >= 50:
            status = "improved"
        elif performance_score >= 25:
            status = "stable"
        else:
            status = "degraded"

        return {
            "performance_score": performance_score,
            "status": status,
            "response_time_change_percent": round(response_change, 2),
            "throughput_change_percent": round(throughput_change, 2),
            "recommendation": self._get_performance_recommendation(
                status, response_change, throughput_change
            ),
        }

    def _get_performance_recommendation(
        self, status: str, response_change: float, throughput_change: float
    ) -> str:
        """æ ¹æ®æ€§èƒ½å˜åŒ–ç”Ÿæˆå»ºè®®"""

        if status == "significantly_improved":
            return "æ€§èƒ½æ˜¾è‘—æ”¹å–„ï¼Œå¯è€ƒè™‘è¿›ä¸€æ­¥ä¼˜åŒ–æˆ–å¢åŠ è´Ÿè½½"
        elif status == "improved":
            return "æ€§èƒ½æœ‰æ‰€æ”¹å–„ï¼Œç»§ç»­ç›‘æ§ä»¥ç¡®ä¿ç¨³å®šæ€§"
        elif status == "stable":
            return "æ€§èƒ½åŸºæœ¬ç¨³å®šï¼Œæ³¨æ„ç›‘æ§èµ„æºä½¿ç”¨æƒ…å†µ"
        else:
            recommendations = ["æ€§èƒ½å‡ºç°é€€åŒ–ï¼Œå»ºè®®ï¼š"]

            if response_change > 10:
                recommendations.append("- æ£€æŸ¥å“åº”æ—¶é—´é€€åŒ–çš„åŸå› ")
                recommendations.append("- ä¼˜åŒ–æ…¢æŸ¥è¯¢æˆ–APIè°ƒç”¨")

            if throughput_change < -10:
                recommendations.append("- æ£€æŸ¥ç³»ç»Ÿç“¶é¢ˆ")
                recommendations.append("- è€ƒè™‘å¢åŠ èµ„æºæˆ–ä¼˜åŒ–å¹¶å‘å¤„ç†")

            return " ".join(recommendations)

    def _save_baselines(self):
        """ä¿å­˜åŸºå‡†åˆ°æ–‡ä»¶"""
        try:
            baselines_data = {}
            for name, baseline in self.baselines.items():
                baselines_data[name] = asdict(baseline)
                # è½¬æ¢datetimeä¸ºISOæ ¼å¼
                baselines_data[name]["created_at"] = baseline.created_at.isoformat()

            with open(self.baseline_file, "w", encoding="utf-8") as f:
                json.dump(baselines_data, f, ensure_ascii=False, indent=2)

        except Exception as e:
            logger.error(f"ä¿å­˜åŸºå‡†å¤±è´¥: {e}")

    def _load_baselines(self):
        """ä»æ–‡ä»¶åŠ è½½åŸºå‡†"""
        try:
            if self.baseline_file.exists():
                with open(self.baseline_file, "r", encoding="utf-8") as f:
                    baselines_data = json.load(f)

                for name, data in baselines_data.items():
                    # è½¬æ¢ISOæ ¼å¼ä¸ºdatetime
                    data["created_at"] = datetime.fromisoformat(data["created_at"])
                    self.baselines[name] = PerformanceBaseline(**data)

                logger.info(f"âœ… åŠ è½½äº† {len(self.baselines)} ä¸ªæ€§èƒ½åŸºå‡†")

        except Exception as e:
            logger.warning(f"åŠ è½½åŸºå‡†å¤±è´¥: {e}")

    def add_alert_callback(self, callback: Callable[[Dict[str, Any]], None]):
        """æ·»åŠ å‘Šè­¦å›è°ƒå‡½æ•°"""
        self.alert_callbacks.append(callback)

    def add_real_time_callback(self, callback: Callable[[Dict[str, Any]], None]):
        """æ·»åŠ å®æ—¶ç›‘æ§å›è°ƒå‡½æ•°"""
        self.real_time_callbacks.append(callback)

    def generate_performance_report(self, time_range_hours: int = 24) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""

        cutoff_time = datetime.now() - timedelta(hours=time_range_hours)
        relevant_snapshots = [s for s in self.snapshots if s.timestamp > cutoff_time]

        if not relevant_snapshots:
            return {"error": "æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®ç”ŸæˆæŠ¥å‘Š"}

        # åŸºç¡€ç»Ÿè®¡
        total_requests = len(relevant_snapshots)
        successful_requests = [s for s in relevant_snapshots if 200 <= s.status_code < 400]
        failed_requests = [s for s in relevant_snapshots if not (200 <= s.status_code < 400)]

        response_times = [s.response_time_ms for s in successful_requests]

        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        report = {
            "report_period": {
                "hours": time_range_hours,
                "start_time": cutoff_time.isoformat(),
                "end_time": datetime.now().isoformat(),
            },
            "request_statistics": {
                "total_requests": total_requests,
                "successful_requests": len(successful_requests),
                "failed_requests": len(failed_requests),
                "success_rate": (
                    len(successful_requests) / total_requests if total_requests > 0 else 0
                ),
                "error_rate": len(failed_requests) / total_requests if total_requests > 0 else 0,
            },
            "response_time_analysis": {},
            "throughput_analysis": {},
            "resource_usage_analysis": {},
            "cost_analysis": {},
            "cache_analysis": {},
            "baseline_comparisons": {},
            "recommendations": [],
        }

        # å“åº”æ—¶é—´åˆ†æ
        if response_times:
            report["response_time_analysis"] = {
                "average_ms": statistics.mean(response_times),
                "median_ms": statistics.median(response_times),
                "p95_ms": np.percentile(response_times, 95),
                "p99_ms": np.percentile(response_times, 99),
                "min_ms": min(response_times),
                "max_ms": max(response_times),
                "std_dev_ms": statistics.stdev(response_times) if len(response_times) > 1 else 0,
            }

        # ååé‡åˆ†æ
        if time_range_hours > 0:
            rps = total_requests / (time_range_hours * 3600)
            successful_rps = len(successful_requests) / (time_range_hours * 3600)

            report["throughput_analysis"] = {
                "total_rps": rps,
                "successful_rps": successful_rps,
                "peak_hour_rps": self._calculate_peak_hour_rps(relevant_snapshots),
            }

        # èµ„æºä½¿ç”¨åˆ†æ
        cpu_values = [s.cpu_percent for s in relevant_snapshots]
        memory_values = [s.memory_mb for s in relevant_snapshots]

        if cpu_values:
            report["resource_usage_analysis"] = {
                "cpu_usage": {
                    "average_percent": statistics.mean(cpu_values),
                    "max_percent": max(cpu_values),
                    "p95_percent": np.percentile(cpu_values, 95),
                },
                "memory_usage": {
                    "average_mb": statistics.mean(memory_values),
                    "max_mb": max(memory_values),
                    "p95_mb": np.percentile(memory_values, 95),
                },
            }

        # æˆæœ¬åˆ†æ
        costs = [s.cost for s in relevant_snapshots if s.cost is not None]
        if costs:
            report["cost_analysis"] = {
                "total_cost": sum(costs),
                "average_cost_per_request": statistics.mean(costs),
                "cost_distribution": {
                    "min": min(costs),
                    "max": max(costs),
                    "p95": np.percentile(costs, 95),
                },
            }

        # ç¼“å­˜åˆ†æ
        cache_hits = [s for s in relevant_snapshots if s.cache_hit is True]
        cache_misses = [s for s in relevant_snapshots if s.cache_hit is False]

        if cache_hits or cache_misses:
            total_cache_ops = len(cache_hits) + len(cache_misses)
            report["cache_analysis"] = {
                "hit_rate": len(cache_hits) / total_cache_ops if total_cache_ops > 0 else 0,
                "total_cache_operations": total_cache_ops,
                "cache_hits": len(cache_hits),
                "cache_misses": len(cache_misses),
            }

        # åŸºå‡†å¯¹æ¯”
        for baseline_name in self.baselines:
            try:
                comparison = self.compare_to_baseline(baseline_name, time_range_hours * 60)
                report["baseline_comparisons"][baseline_name] = comparison
            except Exception as e:
                report["baseline_comparisons"][baseline_name] = {"error": str(e)}

        # ç”Ÿæˆå»ºè®®
        report["recommendations"] = self._generate_report_recommendations(report)

        return report

    def _calculate_peak_hour_rps(self, snapshots: List[PerformanceSnapshot]) -> float:
        """è®¡ç®—å³°å€¼å°æ—¶RPS"""
        if not snapshots:
            return 0.0

        # æŒ‰å°æ—¶åˆ†ç»„è®¡ç®—RPS
        hourly_counts = defaultdict(int)

        for snapshot in snapshots:
            hour_key = snapshot.timestamp.replace(minute=0, second=0, microsecond=0)
            hourly_counts[hour_key] += 1

        return max(hourly_counts.values()) if hourly_counts else 0.0

    def _generate_report_recommendations(self, report: Dict[str, Any]) -> List[str]:
        """æ ¹æ®æŠ¥å‘Šæ•°æ®ç”Ÿæˆå»ºè®®"""
        recommendations = []

        # æˆåŠŸç‡å»ºè®®
        success_rate = report["request_statistics"]["success_rate"]
        if success_rate < 0.95:
            recommendations.append(f"æˆåŠŸç‡ä»…ä¸º {success_rate:.1%}ï¼Œéœ€è¦æ”¹å–„é”™è¯¯å¤„ç†")

        # å“åº”æ—¶é—´å»ºè®®
        response_analysis = report.get("response_time_analysis", {})
        if response_analysis:
            p95_time = response_analysis.get("p95_ms", 0)
            if p95_time > 2000:
                recommendations.append(f"P95å“åº”æ—¶é—´ {p95_time:.0f}ms è¿‡é«˜ï¼Œéœ€è¦æ€§èƒ½ä¼˜åŒ–")

        # èµ„æºä½¿ç”¨å»ºè®®
        resource_analysis = report.get("resource_usage_analysis", {})
        if resource_analysis:
            cpu_p95 = resource_analysis.get("cpu_usage", {}).get("p95_percent", 0)
            if cpu_p95 > 80:
                recommendations.append(f"CPUä½¿ç”¨ç‡P95è¾¾åˆ° {cpu_p95:.1f}%ï¼Œè€ƒè™‘æ‰©å®¹æˆ–ä¼˜åŒ–")

        # ç¼“å­˜å»ºè®®
        cache_analysis = report.get("cache_analysis", {})
        if cache_analysis:
            hit_rate = cache_analysis.get("hit_rate", 0)
            if hit_rate < 0.5:
                recommendations.append(f"ç¼“å­˜å‘½ä¸­ç‡ä»… {hit_rate:.1%}ï¼Œè€ƒè™‘ä¼˜åŒ–ç¼“å­˜ç­–ç•¥")

        # åŸºå‡†å¯¹æ¯”å»ºè®®
        baseline_comparisons = report.get("baseline_comparisons", {})
        for baseline_name, comparison in baseline_comparisons.items():
            if isinstance(comparison, dict) and "overall_assessment" in comparison:
                status = comparison["overall_assessment"]["status"]
                if status == "degraded":
                    recommendations.append(f"ç›¸æ¯”åŸºå‡† '{baseline_name}' æ€§èƒ½æœ‰æ‰€é€€åŒ–")

        if not recommendations:
            recommendations.append("ç³»ç»Ÿæ€§èƒ½è¡¨ç°è‰¯å¥½ï¼Œç»§ç»­ä¿æŒ")

        return recommendations


# ä½¿ç”¨ç¤ºä¾‹
async def example_enhanced_metrics():
    """å¢å¼ºæŒ‡æ ‡æ”¶é›†å™¨ä½¿ç”¨ç¤ºä¾‹"""

    collector = EnhancedMetricsCollector(
        collection_interval=1.0,
        max_snapshots=5000,
        baseline_file=Path("enhanced_performance_baselines.json"),
    )

    # è®¾ç½®å‘Šè­¦å›è°ƒ
    def alert_handler(alert):
        logger.warning(
            f"ğŸš¨ æ€§èƒ½å‘Šè­¦: {alert['type']} = {alert['value']:.2f} (é˜ˆå€¼: {alert['threshold']})"
        )

    def real_time_handler(metrics):
        logger.info(
            f"ğŸ“Š å®æ—¶æŒ‡æ ‡: CPU={metrics['cpu_percent']:.1f}% å†…å­˜={metrics['memory_percent']:.1f}% RPS={metrics['throughput_rps']:.1f}"
        )

    collector.add_alert_callback(alert_handler)
    collector.add_real_time_callback(real_time_handler)

    try:
        # å¯åŠ¨ç›‘æ§
        collector.start_monitoring()

        # æ¨¡æ‹Ÿè¯·æ±‚è®°å½•
        logger.info("ğŸ”„ å¼€å§‹æ¨¡æ‹Ÿè¯·æ±‚è®°å½•...")

        for i in range(100):
            # æ¨¡æ‹Ÿä¸åŒç±»å‹çš„è¯·æ±‚
            response_time = 100 + (i % 50) * 10  # 100-590ms
            status_code = 200 if i % 10 != 0 else 500  # 10%é”™è¯¯ç‡

            collector.record_request(
                endpoint="/api/v1/tests",
                method="GET",
                response_time_ms=response_time,
                status_code=status_code,
                request_size=1024,
                response_size=2048,
                provider="openai" if i % 2 == 0 else "anthropic",
                model="gpt-4" if i % 2 == 0 else "claude-3",
                tokens_used=100 + i,
                cost=0.001 * (1 + i * 0.01),
                cache_hit=i % 3 == 0,
                cache_key=f"cache_key_{i % 10}",
                custom_metrics={"test_metric": i},
            )

            await asyncio.sleep(0.1)

        # åˆ›å»ºæ€§èƒ½åŸºå‡†
        logger.info("ğŸ¯ åˆ›å»ºæ€§èƒ½åŸºå‡†...")
        await asyncio.sleep(2)  # ç­‰å¾…æ›´å¤šæ•°æ®

        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ä¿®æ”¹ä¸ºå¼‚æ­¥ç‰ˆæœ¬
        # baseline = await collector.create_baseline(
        #     name="test_baseline_v1",
        #     test_duration_minutes=1,
        #     target_rps=50,
        #     test_conditions={"load_type": "synthetic", "environment": "test"}
        # )

        # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
        logger.info("ğŸ“Š ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š...")
        report = collector.generate_performance_report(time_range_hours=1)

        # ä¿å­˜æŠ¥å‘Š
        report_file = Path("enhanced_performance_report.json")
        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        logger.info(f"âœ… å¢å¼ºæ€§èƒ½æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

        # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡
        req_stats = report["request_statistics"]
        logger.info(
            f"ğŸ“ˆ è¯·æ±‚ç»Ÿè®¡: æ€»è®¡{req_stats['total_requests']} æˆåŠŸç‡{req_stats['success_rate']:.1%}"
        )

        if "response_time_analysis" in report:
            rt_stats = report["response_time_analysis"]
            logger.info(
                f"â±ï¸ å“åº”æ—¶é—´: å¹³å‡{rt_stats['average_ms']:.1f}ms P95{rt_stats['p95_ms']:.1f}ms"
            )

        # æ˜¾ç¤ºå»ºè®®
        logger.info("ğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®:")
        for rec in report["recommendations"]:
            logger.info(f"  â€¢ {rec}")

    finally:
        collector.stop_monitoring()


if __name__ == "__main__":
    asyncio.run(example_enhanced_metrics())
