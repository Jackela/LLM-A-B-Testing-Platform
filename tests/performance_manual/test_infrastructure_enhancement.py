#!/usr/bin/env python3
"""
Test Infrastructure Enhancement
æµ‹è¯•åŸºç¡€è®¾æ–½å¢å¼ºï¼Œä¸“æ³¨äºç¯å¢ƒè®¾ç½®ã€æ•°æ®ç®¡ç†ã€ç›‘æ§å’Œæ‰§è¡Œæµç¨‹ä¼˜åŒ–
"""

import asyncio
import json
import logging
import threading
import time
from concurrent.futures import ThreadPoolExecutor
from contextlib import asynccontextmanager
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional, Union

import aiofiles
import docker
import psutil
import pytest
import yaml

# Configure logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)


@dataclass
class TestEnvironmentConfig:
    """æµ‹è¯•ç¯å¢ƒé…ç½®"""

    name: str
    description: str

    # åŸºç¡€è®¾ç½®
    max_concurrent_tests: int = 50
    test_timeout_seconds: int = 300
    retry_attempts: int = 3

    # èµ„æºé…ç½®
    memory_limit_mb: int = 2048
    cpu_limit_percent: int = 80
    disk_space_limit_gb: int = 10

    # ç½‘ç»œé…ç½®
    api_rate_limit_per_second: int = 100
    connection_pool_size: int = 100
    request_timeout_seconds: int = 30

    # æ•°æ®é…ç½®
    test_data_cache_size_mb: int = 500
    result_retention_days: int = 30
    enable_test_data_warming: bool = True

    # ç›‘æ§é…ç½®
    enable_real_time_monitoring: bool = True
    metrics_collection_interval: float = 1.0
    performance_alert_thresholds: Dict[str, float] = field(
        default_factory=lambda: {
            "response_time_p95_ms": 2000,
            "error_rate_percent": 5.0,
            "memory_usage_percent": 85.0,
            "cpu_usage_percent": 80.0,
        }
    )

    # æ•°æ®åº“é…ç½®
    database_config: Dict[str, Any] = field(
        default_factory=lambda: {
            "pool_size": 20,
            "max_overflow": 30,
            "pool_timeout": 10,
            "enable_query_optimization": True,
        }
    )

    # ç¼“å­˜é…ç½®
    cache_config: Dict[str, Any] = field(
        default_factory=lambda: {
            "redis_url": "redis://localhost:6379",
            "max_memory": "256mb",
            "eviction_policy": "allkeys-lru",
            "enable_persistence": False,
        }
    )


@dataclass
class TestRunMetrics:
    """æµ‹è¯•è¿è¡ŒæŒ‡æ ‡"""

    test_name: str
    start_time: datetime
    end_time: Optional[datetime] = None

    # æ‰§è¡Œç»Ÿè®¡
    total_samples: int = 0
    processed_samples: int = 0
    successful_samples: int = 0
    failed_samples: int = 0

    # æ€§èƒ½æŒ‡æ ‡
    average_response_time_ms: float = 0.0
    p95_response_time_ms: float = 0.0
    throughput_samples_per_second: float = 0.0

    # èµ„æºä½¿ç”¨
    peak_memory_mb: float = 0.0
    average_cpu_percent: float = 0.0
    network_io_mb: float = 0.0

    # æˆæœ¬æŒ‡æ ‡
    total_cost_usd: float = 0.0
    cost_per_sample_usd: float = 0.0

    # è´¨é‡æŒ‡æ ‡
    data_quality_score: float = 0.0
    test_coverage_percent: float = 0.0

    # é”™è¯¯åˆ†æ
    error_breakdown: Dict[str, int] = field(default_factory=dict)
    warning_count: int = 0


class TestDataManager:
    """æµ‹è¯•æ•°æ®ç®¡ç†å™¨"""

    def __init__(self, data_dir: Path, cache_size_mb: int = 500, enable_compression: bool = True):

        self.data_dir = data_dir
        self.data_dir.mkdir(parents=True, exist_ok=True)

        self.cache_size_mb = cache_size_mb
        self.enable_compression = enable_compression

        # å†…å­˜ç¼“å­˜
        self._memory_cache: Dict[str, Any] = {}
        self._cache_access_times: Dict[str, datetime] = {}
        self._current_cache_size = 0

        # æ•°æ®ç»Ÿè®¡
        self.data_stats = {
            "datasets_loaded": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "disk_reads": 0,
            "compression_ratio": 0.0,
        }

    async def load_dataset(
        self, dataset_name: str, max_samples: Optional[int] = None, force_reload: bool = False
    ) -> List[Dict[str, Any]]:
        """åŠ è½½æ•°æ®é›†"""

        cache_key = f"{dataset_name}_{max_samples or 'all'}"

        # æ£€æŸ¥å†…å­˜ç¼“å­˜
        if not force_reload and cache_key in self._memory_cache:
            self._cache_access_times[cache_key] = datetime.now()
            self.data_stats["cache_hits"] += 1
            logger.debug(f"ğŸ“¦ ä»ç¼“å­˜åŠ è½½æ•°æ®é›†: {cache_key}")
            return self._memory_cache[cache_key]

        self.data_stats["cache_misses"] += 1

        # ä»ç£ç›˜åŠ è½½
        dataset_file = self.data_dir / f"{dataset_name}.json"
        if not dataset_file.exists():
            raise FileNotFoundError(f"æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {dataset_file}")

        logger.info(f"ğŸ’¾ ä»ç£ç›˜åŠ è½½æ•°æ®é›†: {dataset_name}")
        start_time = time.time()

        async with aiofiles.open(dataset_file, "r", encoding="utf-8") as f:
            content = await f.read()

        if self.enable_compression:
            # æ£€æŸ¥æ˜¯å¦ä¸ºå‹ç¼©æ–‡ä»¶
            try:
                import gzip

                if dataset_file.suffix == ".gz":
                    content = gzip.decompress(content.encode()).decode()
            except Exception:
                pass

        dataset = json.loads(content)
        self.data_stats["disk_reads"] += 1

        # åº”ç”¨æ ·æœ¬é™åˆ¶
        if max_samples and len(dataset) > max_samples:
            dataset = dataset[:max_samples]

        load_time = time.time() - start_time
        logger.info(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆ: {len(dataset)} æ ·æœ¬, è€—æ—¶ {load_time:.2f}s")

        # ç¼“å­˜åˆ°å†…å­˜
        await self._cache_dataset(cache_key, dataset)

        self.data_stats["datasets_loaded"] += 1
        return dataset

    async def _cache_dataset(self, cache_key: str, data: List[Dict[str, Any]]):
        """ç¼“å­˜æ•°æ®é›†åˆ°å†…å­˜"""

        # ä¼°ç®—æ•°æ®å¤§å°
        data_size_mb = len(json.dumps(data)) / 1024 / 1024

        # æ£€æŸ¥ç¼“å­˜å®¹é‡
        while self._current_cache_size + data_size_mb > self.cache_size_mb:
            await self._evict_oldest_cache()

        # æ·»åŠ åˆ°ç¼“å­˜
        self._memory_cache[cache_key] = data
        self._cache_access_times[cache_key] = datetime.now()
        self._current_cache_size += data_size_mb

        logger.debug(f"ğŸ’¾ æ•°æ®é›†å·²ç¼“å­˜: {cache_key} ({data_size_mb:.1f}MB)")

    async def _evict_oldest_cache(self):
        """é©±é€æœ€æ—§çš„ç¼“å­˜"""

        if not self._cache_access_times:
            return

        # æ‰¾åˆ°æœ€æ—§çš„ç¼“å­˜é¡¹
        oldest_key = min(self._cache_access_times.keys(), key=lambda k: self._cache_access_times[k])

        # ä¼°ç®—å¤§å°å¹¶ç§»é™¤
        if oldest_key in self._memory_cache:
            data_size_mb = len(json.dumps(self._memory_cache[oldest_key])) / 1024 / 1024
            del self._memory_cache[oldest_key]
            del self._cache_access_times[oldest_key]
            self._current_cache_size -= data_size_mb

            logger.debug(f"ğŸ—‘ï¸ é©±é€ç¼“å­˜: {oldest_key} ({data_size_mb:.1f}MB)")

    async def prepare_test_datasets(
        self, required_datasets: List[str], sample_limits: Optional[Dict[str, int]] = None
    ) -> Dict[str, List[Dict[str, Any]]]:
        """é¢„åŠ è½½æµ‹è¯•æ‰€éœ€çš„æ•°æ®é›†"""

        logger.info(f"ğŸ”„ é¢„åŠ è½½ {len(required_datasets)} ä¸ªæ•°æ®é›†...")

        datasets = {}
        load_tasks = []

        for dataset_name in required_datasets:
            max_samples = sample_limits.get(dataset_name) if sample_limits else None
            task = self.load_dataset(dataset_name, max_samples)
            load_tasks.append((dataset_name, task))

        # å¹¶è¡ŒåŠ è½½
        for dataset_name, task in load_tasks:
            try:
                data = await task
                datasets[dataset_name] = data
                logger.info(f"âœ… {dataset_name}: {len(data)} æ ·æœ¬")
            except Exception as e:
                logger.error(f"âŒ åŠ è½½æ•°æ®é›†å¤±è´¥ {dataset_name}: {e}")
                datasets[dataset_name] = []

        logger.info(f"ğŸ¯ æ•°æ®é›†é¢„åŠ è½½å®Œæˆ: {len(datasets)} ä¸ªæ•°æ®é›†")
        return datasets

    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""

        total_operations = self.data_stats["cache_hits"] + self.data_stats["cache_misses"]
        hit_rate = self.data_stats["cache_hits"] / total_operations if total_operations > 0 else 0

        return {
            "cache_hit_rate": hit_rate,
            "current_cache_size_mb": self._current_cache_size,
            "max_cache_size_mb": self.cache_size_mb,
            "cached_items": len(self._memory_cache),
            "data_stats": self.data_stats.copy(),
        }


class TestEnvironmentManager:
    """æµ‹è¯•ç¯å¢ƒç®¡ç†å™¨"""

    def __init__(
        self, config: TestEnvironmentConfig, work_dir: Path = Path("tests/performance_workspace")
    ):

        self.config = config
        self.work_dir = work_dir
        self.work_dir.mkdir(parents=True, exist_ok=True)

        # ç»„ä»¶åˆå§‹åŒ–
        self.data_manager = TestDataManager(
            data_dir=self.work_dir / "data", cache_size_mb=config.test_data_cache_size_mb
        )

        # èµ„æºç›‘æ§
        self._monitoring_active = False
        self._monitor_thread = None
        self._stop_monitoring = threading.Event()
        self._resource_history = []

        # å¹¶å‘æ§åˆ¶
        self._semaphore = asyncio.Semaphore(config.max_concurrent_tests)
        self._executor = ThreadPoolExecutor(max_workers=config.max_concurrent_tests)

        # æµ‹è¯•è¿è¡Œè®°å½•
        self.test_runs: List[TestRunMetrics] = []
        self.active_tests: Dict[str, TestRunMetrics] = {}

        # ç¯å¢ƒå¥åº·çŠ¶æ€
        self.health_status = {
            "cpu_usage": 0.0,
            "memory_usage": 0.0,
            "disk_usage": 0.0,
            "network_latency": 0.0,
            "database_connections": 0,
            "cache_health": "ok",
        }

    async def setup_environment(self) -> bool:
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""

        logger.info(f"ğŸš€ è®¾ç½®æµ‹è¯•ç¯å¢ƒ: {self.config.name}")

        try:
            # 1. æ£€æŸ¥ç³»ç»Ÿèµ„æº
            if not await self._check_system_requirements():
                return False

            # 2. åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± 
            await self._setup_database_pool()

            # 3. åˆå§‹åŒ–ç¼“å­˜
            await self._setup_cache()

            # 4. å¯åŠ¨èµ„æºç›‘æ§
            self._start_resource_monitoring()

            # 5. é¢„çƒ­æµ‹è¯•æ•°æ®
            if self.config.enable_test_data_warming:
                await self._warm_test_data()

            # 6. éªŒè¯ç¯å¢ƒå¥åº·çŠ¶æ€
            health_check = await self._comprehensive_health_check()

            if health_check["status"] == "healthy":
                logger.info("âœ… æµ‹è¯•ç¯å¢ƒè®¾ç½®å®Œæˆ")
                return True
            else:
                logger.error(f"âŒ ç¯å¢ƒå¥åº·æ£€æŸ¥å¤±è´¥: {health_check['issues']}")
                return False

        except Exception as e:
            logger.error(f"âŒ ç¯å¢ƒè®¾ç½®å¤±è´¥: {e}")
            return False

    async def _check_system_requirements(self) -> bool:
        """æ£€æŸ¥ç³»ç»Ÿèµ„æºè¦æ±‚"""

        logger.info("ğŸ” æ£€æŸ¥ç³»ç»Ÿèµ„æº...")

        # æ£€æŸ¥å¯ç”¨å†…å­˜
        memory = psutil.virtual_memory()
        available_memory_mb = memory.available / 1024 / 1024

        if available_memory_mb < self.config.memory_limit_mb:
            logger.error(
                f"å†…å­˜ä¸è¶³: éœ€è¦{self.config.memory_limit_mb}MB, å¯ç”¨{available_memory_mb:.0f}MB"
            )
            return False

        # æ£€æŸ¥CPU
        cpu_count = psutil.cpu_count()
        if cpu_count < 2:
            logger.warning("CPUæ ¸å¿ƒæ•°è¾ƒå°‘ï¼Œå¯èƒ½å½±å“å¹¶å‘æ€§èƒ½")

        # æ£€æŸ¥ç£ç›˜ç©ºé—´
        disk = psutil.disk_usage(self.work_dir)
        available_gb = disk.free / 1024 / 1024 / 1024

        if available_gb < self.config.disk_space_limit_gb:
            logger.error(
                f"ç£ç›˜ç©ºé—´ä¸è¶³: éœ€è¦{self.config.disk_space_limit_gb}GB, å¯ç”¨{available_gb:.1f}GB"
            )
            return False

        logger.info("âœ… ç³»ç»Ÿèµ„æºæ£€æŸ¥é€šè¿‡")
        return True

    async def _setup_database_pool(self):
        """è®¾ç½®æ•°æ®åº“è¿æ¥æ± """

        logger.info("ğŸ—„ï¸ åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± ...")

        # è¿™é‡Œå¯ä»¥é›†æˆå®é™…çš„æ•°æ®åº“è¿æ¥æ± è®¾ç½®
        # ä¾‹å¦‚: SQLAlchemy, asyncpg ç­‰

        # æ¨¡æ‹Ÿæ•°æ®åº“æ± è®¾ç½®
        await asyncio.sleep(0.5)

        self.health_status["database_connections"] = self.config.database_config["pool_size"]
        logger.info("âœ… æ•°æ®åº“è¿æ¥æ± åˆå§‹åŒ–å®Œæˆ")

    async def _setup_cache(self):
        """è®¾ç½®ç¼“å­˜"""

        logger.info("ğŸ”§ åˆå§‹åŒ–ç¼“å­˜ç³»ç»Ÿ...")

        # è¿™é‡Œå¯ä»¥é›†æˆå®é™…çš„ç¼“å­˜ç³»ç»Ÿ
        # ä¾‹å¦‚: Redis, Memcached ç­‰

        # æ¨¡æ‹Ÿç¼“å­˜è®¾ç½®
        await asyncio.sleep(0.3)

        self.health_status["cache_health"] = "ok"
        logger.info("âœ… ç¼“å­˜ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")

    def _start_resource_monitoring(self):
        """å¯åŠ¨èµ„æºç›‘æ§"""

        if self._monitoring_active:
            return

        self._monitoring_active = True
        self._stop_monitoring.clear()

        self._monitor_thread = threading.Thread(target=self._resource_monitor_loop)
        self._monitor_thread.daemon = True
        self._monitor_thread.start()

        logger.info("ğŸ“Š èµ„æºç›‘æ§å·²å¯åŠ¨")

    def _resource_monitor_loop(self):
        """èµ„æºç›‘æ§å¾ªç¯"""

        while not self._stop_monitoring.is_set():
            try:
                # æ”¶é›†ç³»ç»Ÿèµ„æºä¿¡æ¯
                cpu_percent = psutil.cpu_percent(interval=None)
                memory = psutil.virtual_memory()
                disk = psutil.disk_usage(self.work_dir)

                # æ›´æ–°å¥åº·çŠ¶æ€
                self.health_status.update(
                    {
                        "cpu_usage": cpu_percent,
                        "memory_usage": memory.percent,
                        "disk_usage": (disk.used / disk.total) * 100,
                    }
                )

                # è®°å½•å†å²æ•°æ®
                resource_snapshot = {
                    "timestamp": datetime.now(),
                    "cpu_percent": cpu_percent,
                    "memory_percent": memory.percent,
                    "memory_mb": memory.used / 1024 / 1024,
                    "disk_percent": self.health_status["disk_usage"],
                }

                self._resource_history.append(resource_snapshot)

                # ä¿æŒå†å²æ•°æ®åœ¨åˆç†èŒƒå›´å†…
                if len(self._resource_history) > 3600:  # 1å°æ—¶çš„æ•°æ®
                    self._resource_history = self._resource_history[-1800:]

                # æ£€æŸ¥å‘Šè­¦é˜ˆå€¼
                self._check_resource_alerts(resource_snapshot)

                self._stop_monitoring.wait(self.config.metrics_collection_interval)

            except Exception as e:
                logger.error(f"èµ„æºç›‘æ§é”™è¯¯: {e}")
                self._stop_monitoring.wait(5.0)

    def _check_resource_alerts(self, snapshot: Dict[str, Any]):
        """æ£€æŸ¥èµ„æºå‘Šè­¦"""

        thresholds = self.config.performance_alert_thresholds

        # CPUå‘Šè­¦
        if snapshot["cpu_percent"] > thresholds.get("cpu_usage_percent", 80):
            logger.warning(f"ğŸš¨ CPUä½¿ç”¨ç‡è¿‡é«˜: {snapshot['cpu_percent']:.1f}%")

        # å†…å­˜å‘Šè­¦
        if snapshot["memory_percent"] > thresholds.get("memory_usage_percent", 85):
            logger.warning(f"ğŸš¨ å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {snapshot['memory_percent']:.1f}%")

        # ç£ç›˜å‘Šè­¦
        if snapshot["disk_percent"] > 90:
            logger.warning(f"ğŸš¨ ç£ç›˜ä½¿ç”¨ç‡è¿‡é«˜: {snapshot['disk_percent']:.1f}%")

    async def _warm_test_data(self):
        """é¢„çƒ­æµ‹è¯•æ•°æ®"""

        logger.info("ğŸ”¥ å¼€å§‹æµ‹è¯•æ•°æ®é¢„çƒ­...")

        # åŠ è½½å¸¸ç”¨æ•°æ®é›†
        common_datasets = ["arc_easy", "gsm8k"]
        available_datasets = []

        for dataset in common_datasets:
            dataset_file = self.data_manager.data_dir / f"{dataset}.json"
            if dataset_file.exists():
                available_datasets.append(dataset)

        if available_datasets:
            await self.data_manager.prepare_test_datasets(
                available_datasets, sample_limits={dataset: 1000 for dataset in available_datasets}
            )
            logger.info(f"âœ… é¢„çƒ­å®Œæˆ: {len(available_datasets)} ä¸ªæ•°æ®é›†")
        else:
            logger.info("â„¹ï¸ æ²¡æœ‰æ‰¾åˆ°å¯é¢„çƒ­çš„æ•°æ®é›†")

    async def _comprehensive_health_check(self) -> Dict[str, Any]:
        """ç»¼åˆå¥åº·æ£€æŸ¥"""

        logger.info("ğŸ¥ æ‰§è¡Œç»¼åˆå¥åº·æ£€æŸ¥...")

        issues = []
        warnings = []

        # ç³»ç»Ÿèµ„æºæ£€æŸ¥
        if self.health_status["cpu_usage"] > 90:
            issues.append("CPUä½¿ç”¨ç‡è¿‡é«˜")
        elif self.health_status["cpu_usage"] > 70:
            warnings.append("CPUä½¿ç”¨ç‡è¾ƒé«˜")

        if self.health_status["memory_usage"] > 90:
            issues.append("å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜")
        elif self.health_status["memory_usage"] > 80:
            warnings.append("å†…å­˜ä½¿ç”¨ç‡è¾ƒé«˜")

        # ç¼“å­˜å¥åº·æ£€æŸ¥
        cache_stats = self.data_manager.get_cache_stats()
        if cache_stats["cache_hit_rate"] < 0.3:
            warnings.append("ç¼“å­˜å‘½ä¸­ç‡è¾ƒä½")

        # ç¡®å®šæ•´ä½“çŠ¶æ€
        if issues:
            status = "unhealthy"
        elif warnings:
            status = "degraded"
        else:
            status = "healthy"

        health_report = {
            "status": status,
            "timestamp": datetime.now().isoformat(),
            "issues": issues,
            "warnings": warnings,
            "system_health": self.health_status.copy(),
            "cache_stats": cache_stats,
            "environment_config": {
                "name": self.config.name,
                "max_concurrent_tests": self.config.max_concurrent_tests,
                "memory_limit_mb": self.config.memory_limit_mb,
            },
        }

        logger.info(f"ğŸ¯ å¥åº·æ£€æŸ¥å®Œæˆ: {status}")
        if warnings:
            logger.warning(f"âš ï¸ è­¦å‘Š: {', '.join(warnings)}")

        return health_report

    @asynccontextmanager
    async def test_session(self, test_name: str):
        """æµ‹è¯•ä¼šè¯ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""

        # åˆ›å»ºæµ‹è¯•è¿è¡Œè®°å½•
        test_metrics = TestRunMetrics(test_name=test_name, start_time=datetime.now())

        self.active_tests[test_name] = test_metrics

        async with self._semaphore:
            try:
                logger.info(f"ğŸš€ å¼€å§‹æµ‹è¯•ä¼šè¯: {test_name}")
                yield test_metrics

            except Exception as e:
                logger.error(f"âŒ æµ‹è¯•ä¼šè¯å¤±è´¥: {test_name} - {e}")
                test_metrics.failed_samples += 1
                raise

            finally:
                # å®Œæˆæµ‹è¯•è®°å½•
                test_metrics.end_time = datetime.now()

                if test_metrics.total_samples > 0:
                    test_metrics.data_quality_score = (
                        test_metrics.successful_samples / test_metrics.total_samples
                    )

                # ç§»åŠ¨åˆ°å®Œæˆåˆ—è¡¨
                if test_name in self.active_tests:
                    del self.active_tests[test_name]

                self.test_runs.append(test_metrics)

                duration = (test_metrics.end_time - test_metrics.start_time).total_seconds()
                logger.info(f"âœ… æµ‹è¯•ä¼šè¯å®Œæˆ: {test_name} (è€—æ—¶ {duration:.1f}s)")

    async def run_performance_test(
        self, test_func: Callable, test_name: str, test_args: Optional[Dict[str, Any]] = None
    ) -> TestRunMetrics:
        """è¿è¡Œæ€§èƒ½æµ‹è¯•"""

        async with self.test_session(test_name) as metrics:

            # å‡†å¤‡æµ‹è¯•å‚æ•°
            args = test_args or {}
            args["test_metrics"] = metrics
            args["data_manager"] = self.data_manager
            args["environment"] = self

            # æ‰§è¡Œæµ‹è¯•
            if asyncio.iscoroutinefunction(test_func):
                result = await test_func(**args)
            else:
                loop = asyncio.get_event_loop()
                result = await loop.run_in_executor(self._executor, lambda: test_func(**args))

            # æ›´æ–°æŒ‡æ ‡
            if isinstance(result, dict):
                if "total_samples" in result:
                    metrics.total_samples = result["total_samples"]
                if "successful_samples" in result:
                    metrics.successful_samples = result["successful_samples"]
                if "total_cost" in result:
                    metrics.total_cost_usd = result["total_cost"]

            return metrics

    def get_environment_status(self) -> Dict[str, Any]:
        """è·å–ç¯å¢ƒçŠ¶æ€"""

        return {
            "config": {
                "name": self.config.name,
                "max_concurrent_tests": self.config.max_concurrent_tests,
                "memory_limit_mb": self.config.memory_limit_mb,
            },
            "health": self.health_status.copy(),
            "active_tests": len(self.active_tests),
            "completed_tests": len(self.test_runs),
            "cache_stats": self.data_manager.get_cache_stats(),
            "resource_usage": self._get_current_resource_usage(),
            "monitoring_active": self._monitoring_active,
        }

    def _get_current_resource_usage(self) -> Dict[str, float]:
        """è·å–å½“å‰èµ„æºä½¿ç”¨æƒ…å†µ"""

        if self._resource_history:
            latest = self._resource_history[-1]
            return {
                "cpu_percent": latest["cpu_percent"],
                "memory_percent": latest["memory_percent"],
                "memory_mb": latest["memory_mb"],
                "disk_percent": latest["disk_percent"],
            }

        return {"cpu_percent": 0.0, "memory_percent": 0.0, "memory_mb": 0.0, "disk_percent": 0.0}

    async def cleanup_environment(self):
        """æ¸…ç†ç¯å¢ƒ"""

        logger.info("ğŸ§¹ å¼€å§‹æ¸…ç†æµ‹è¯•ç¯å¢ƒ...")

        # åœæ­¢ç›‘æ§
        if self._monitoring_active:
            self._monitoring_active = False
            self._stop_monitoring.set()

            if self._monitor_thread:
                self._monitor_thread.join(timeout=2.0)

        # å…³é—­çº¿ç¨‹æ± 
        self._executor.shutdown(wait=True)

        # æ¸…ç†ç¼“å­˜
        self.data_manager._memory_cache.clear()
        self.data_manager._cache_access_times.clear()

        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        final_report = await self._generate_final_report()

        # ä¿å­˜æŠ¥å‘Š
        report_file = (
            self.work_dir
            / f"test_environment_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        )
        async with aiofiles.open(report_file, "w", encoding="utf-8") as f:
            await f.write(json.dumps(final_report, ensure_ascii=False, indent=2))

        logger.info(f"ğŸ“Š ç¯å¢ƒæŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        logger.info("âœ… æµ‹è¯•ç¯å¢ƒæ¸…ç†å®Œæˆ")

    async def _generate_final_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""

        total_tests = len(self.test_runs)
        if total_tests == 0:
            return {"message": "æ²¡æœ‰æ‰§è¡Œä»»ä½•æµ‹è¯•"}

        # è®¡ç®—æ±‡æ€»ç»Ÿè®¡
        total_samples = sum(t.total_samples for t in self.test_runs)
        successful_samples = sum(t.successful_samples for t in self.test_runs)
        total_cost = sum(t.total_cost_usd for t in self.test_runs)

        success_rate = successful_samples / total_samples if total_samples > 0 else 0
        avg_cost_per_sample = total_cost / total_samples if total_samples > 0 else 0

        # æ€§èƒ½ç»Ÿè®¡
        response_times = []
        for test in self.test_runs:
            if test.average_response_time_ms > 0:
                response_times.append(test.average_response_time_ms)

        avg_response_time = sum(response_times) / len(response_times) if response_times else 0

        # èµ„æºä½¿ç”¨ç»Ÿè®¡
        if self._resource_history:
            cpu_values = [r["cpu_percent"] for r in self._resource_history]
            memory_values = [r["memory_percent"] for r in self._resource_history]

            peak_cpu = max(cpu_values)
            peak_memory = max(memory_values)
            avg_cpu = sum(cpu_values) / len(cpu_values)
            avg_memory = sum(memory_values) / len(memory_values)
        else:
            peak_cpu = peak_memory = avg_cpu = avg_memory = 0

        return {
            "environment_summary": {
                "name": self.config.name,
                "test_duration": (
                    (datetime.now() - min(t.start_time for t in self.test_runs)).total_seconds()
                    if self.test_runs
                    else 0
                ),
                "total_tests_executed": total_tests,
            },
            "performance_summary": {
                "total_samples_processed": total_samples,
                "success_rate": success_rate,
                "average_response_time_ms": avg_response_time,
                "total_cost_usd": total_cost,
                "cost_per_sample_usd": avg_cost_per_sample,
            },
            "resource_utilization": {
                "peak_cpu_percent": peak_cpu,
                "peak_memory_percent": peak_memory,
                "average_cpu_percent": avg_cpu,
                "average_memory_percent": avg_memory,
            },
            "cache_performance": self.data_manager.get_cache_stats(),
            "test_results": [
                {
                    "name": t.test_name,
                    "duration_seconds": (
                        (t.end_time - t.start_time).total_seconds() if t.end_time else 0
                    ),
                    "total_samples": t.total_samples,
                    "success_rate": (
                        t.successful_samples / t.total_samples if t.total_samples > 0 else 0
                    ),
                    "cost_usd": t.total_cost_usd,
                }
                for t in self.test_runs
            ],
            "recommendations": self._generate_environment_recommendations(),
        }

    def _generate_environment_recommendations(self) -> List[str]:
        """ç”Ÿæˆç¯å¢ƒä¼˜åŒ–å»ºè®®"""

        recommendations = []

        # ç¼“å­˜å»ºè®®
        cache_stats = self.data_manager.get_cache_stats()
        if cache_stats["cache_hit_rate"] < 0.5:
            recommendations.append("ç¼“å­˜å‘½ä¸­ç‡è¾ƒä½ï¼Œè€ƒè™‘å¢åŠ ç¼“å­˜å¤§å°æˆ–ä¼˜åŒ–ç¼“å­˜ç­–ç•¥")

        # å¹¶å‘å»ºè®®
        if len(self.test_runs) > 0:
            avg_test_duration = sum(
                (t.end_time - t.start_time).total_seconds() for t in self.test_runs if t.end_time
            ) / len([t for t in self.test_runs if t.end_time])

            if avg_test_duration > 300:  # 5åˆ†é’Ÿ
                recommendations.append("æµ‹è¯•æ‰§è¡Œæ—¶é—´è¾ƒé•¿ï¼Œè€ƒè™‘ä¼˜åŒ–æµ‹è¯•æ•°æ®å¤§å°æˆ–å¢åŠ å¹¶å‘æ•°")

        # èµ„æºå»ºè®®
        if self._resource_history:
            avg_cpu = sum(r["cpu_percent"] for r in self._resource_history) / len(
                self._resource_history
            )
            avg_memory = sum(r["memory_percent"] for r in self._resource_history) / len(
                self._resource_history
            )

            if avg_cpu > 80:
                recommendations.append("CPUä½¿ç”¨ç‡è¾ƒé«˜ï¼Œè€ƒè™‘ä¼˜åŒ–ç®—æ³•æˆ–å¢åŠ è®¡ç®—èµ„æº")
            if avg_memory > 80:
                recommendations.append("å†…å­˜ä½¿ç”¨ç‡è¾ƒé«˜ï¼Œè€ƒè™‘ä¼˜åŒ–æ•°æ®ç»“æ„æˆ–å¢åŠ å†…å­˜")

        if not recommendations:
            recommendations.append("ç¯å¢ƒé…ç½®åˆç†ï¼Œæ€§èƒ½è¡¨ç°è‰¯å¥½")

        return recommendations


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•
async def example_test_function(
    test_metrics: TestRunMetrics,
    data_manager: TestDataManager,
    environment: TestEnvironmentManager,
    sample_count: int = 100,
) -> Dict[str, Any]:
    """ç¤ºä¾‹æµ‹è¯•å‡½æ•°"""

    logger.info(f"ğŸ”„ å¼€å§‹ç¤ºä¾‹æµ‹è¯•ï¼Œç›®æ ‡æ ·æœ¬æ•°: {sample_count}")

    # åŠ è½½æµ‹è¯•æ•°æ®
    try:
        datasets = await data_manager.prepare_test_datasets(
            ["arc_easy"], {"arc_easy": sample_count}
        )

        if "arc_easy" not in datasets or not datasets["arc_easy"]:
            logger.warning("æœªæ‰¾åˆ°æµ‹è¯•æ•°æ®ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
            test_data = [{"id": i, "prompt": f"test question {i}"} for i in range(sample_count)]
        else:
            test_data = datasets["arc_easy"][:sample_count]

    except Exception as e:
        logger.warning(f"æ•°æ®åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®: {e}")
        test_data = [{"id": i, "prompt": f"test question {i}"} for i in range(sample_count)]

    # æ›´æ–°æµ‹è¯•æŒ‡æ ‡
    test_metrics.total_samples = len(test_data)

    # æ¨¡æ‹Ÿå¤„ç†
    successful_count = 0
    total_response_time = 0.0

    for i, sample in enumerate(test_data):
        # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        response_time = 0.1 + (i % 10) * 0.05  # 100-600ms
        await asyncio.sleep(response_time)

        # æ¨¡æ‹ŸæˆåŠŸ/å¤±è´¥
        if i % 10 != 0:  # 90%æˆåŠŸç‡
            successful_count += 1
            total_response_time += response_time * 1000  # è½¬æ¢ä¸ºms

        # æ›´æ–°è¿›åº¦
        if (i + 1) % 20 == 0:
            logger.info(f"ğŸ“Š å¤„ç†è¿›åº¦: {i + 1}/{len(test_data)}")

    # æ›´æ–°æµ‹è¯•æŒ‡æ ‡
    test_metrics.processed_samples = len(test_data)
    test_metrics.successful_samples = successful_count
    test_metrics.failed_samples = len(test_data) - successful_count
    test_metrics.average_response_time_ms = (
        total_response_time / successful_count if successful_count > 0 else 0
    )
    test_metrics.total_cost_usd = len(test_data) * 0.001  # æ¨¡æ‹Ÿæˆæœ¬

    return {
        "total_samples": len(test_data),
        "successful_samples": successful_count,
        "total_cost": test_metrics.total_cost_usd,
        "average_response_time_ms": test_metrics.average_response_time_ms,
    }


async def test_infrastructure_enhancement():
    """æµ‹è¯•åŸºç¡€è®¾æ–½å¢å¼ºæ¼”ç¤º"""

    # åˆ›å»ºæµ‹è¯•é…ç½®
    config = TestEnvironmentConfig(
        name="enhanced_performance_test_env",
        description="å¢å¼ºçš„æ€§èƒ½æµ‹è¯•ç¯å¢ƒ",
        max_concurrent_tests=20,
        memory_limit_mb=1024,
        enable_test_data_warming=True,
        enable_real_time_monitoring=True,
    )

    # åˆ›å»ºç¯å¢ƒç®¡ç†å™¨
    env_manager = TestEnvironmentManager(config)

    try:
        # è®¾ç½®ç¯å¢ƒ
        if not await env_manager.setup_environment():
            logger.error("âŒ ç¯å¢ƒè®¾ç½®å¤±è´¥")
            return

        # è¿è¡Œæµ‹è¯•
        logger.info("ğŸš€ å¼€å§‹è¿è¡Œæ€§èƒ½æµ‹è¯•...")

        test_tasks = []
        for i in range(3):
            test_name = f"enhanced_test_{i + 1}"
            task = env_manager.run_performance_test(
                example_test_function, test_name, {"sample_count": 50}
            )
            test_tasks.append(task)

        # ç­‰å¾…æ‰€æœ‰æµ‹è¯•å®Œæˆ
        results = await asyncio.gather(*test_tasks)

        # æ˜¾ç¤ºç»“æœ
        logger.info("ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»:")
        for result in results:
            logger.info(
                f"  {result.test_name}: "
                f"{result.successful_samples}/{result.total_samples} æˆåŠŸ "
                f"(æˆæœ¬: ${result.total_cost_usd:.4f})"
            )

        # æ˜¾ç¤ºç¯å¢ƒçŠ¶æ€
        status = env_manager.get_environment_status()
        logger.info(
            f"ğŸ¥ ç¯å¢ƒçŠ¶æ€: {status['health']['cpu_usage']:.1f}% CPU, {status['health']['memory_usage']:.1f}% å†…å­˜"
        )
        logger.info(f"ğŸ“¦ ç¼“å­˜æ•ˆç‡: {status['cache_stats']['cache_hit_rate']:.1%} å‘½ä¸­ç‡")

    finally:
        await env_manager.cleanup_environment()


if __name__ == "__main__":
    asyncio.run(test_infrastructure_enhancement())
