#!/usr/bin/env python3
"""
Cost-Controlled Large Scale Evaluation Test
æˆæœ¬æ§åˆ¶çš„å¤§è§„æ¨¡è¯„ä¼°æµ‹è¯• - æ¨¡æ‹ŸçœŸå®APIè°ƒç”¨å’Œæˆæœ¬åˆ†æ
"""

import asyncio
import json
import logging
import random
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

# Configure logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)


class CostControlledProvider:
    """æˆæœ¬æ§åˆ¶çš„LLMæä¾›å•†æ¨¡æ‹Ÿå™¨"""

    def __init__(self, provider_name: str, model_name: str, pricing_tier: str = "budget"):
        self.provider_name = provider_name
        self.model_name = model_name
        self.pricing_tier = pricing_tier
        self.request_count = 0
        self.total_tokens = 0
        self.total_cost = 0.0

        # çœŸå®ä»·æ ¼è¡¨ (æ¯1K tokens, USD)
        self.pricing = {
            "gemini-flash": {"input": 0.000075, "output": 0.0003},  # è¶…ä¾¿å®œ
            "claude-sonnet": {"input": 0.003, "output": 0.015},  # ä¸­ç­‰
            "gpt-3.5-turbo": {"input": 0.0005, "output": 0.0015},  # ä¾¿å®œ
            "gpt-4o-mini": {"input": 0.00015, "output": 0.0006},  # å¾ˆä¾¿å®œ
        }

        # æ¨¡å‹ç‰¹æ€§é…ç½®
        self.model_characteristics = {
            "gemini-flash": {
                "response_style": "structured",
                "avg_response_length": 120,
                "detail_level": "medium",
                "reasoning_depth": "moderate",
            },
            "claude-sonnet": {
                "response_style": "analytical",
                "avg_response_length": 150,
                "detail_level": "high",
                "reasoning_depth": "deep",
            },
            "gpt-3.5-turbo": {
                "response_style": "conversational",
                "avg_response_length": 100,
                "detail_level": "medium",
                "reasoning_depth": "moderate",
            },
            "gpt-4o-mini": {
                "response_style": "precise",
                "avg_response_length": 90,
                "detail_level": "focused",
                "reasoning_depth": "efficient",
            },
        }

    async def generate_response(
        self, prompt: str, context: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """ç”Ÿæˆå“åº”å¹¶è®¡ç®—ç²¾ç¡®æˆæœ¬"""

        # æ¨¡æ‹ŸAPIå»¶è¿Ÿ
        await asyncio.sleep(random.uniform(0.2, 0.8))

        # è·å–æ¨¡å‹ç‰¹æ€§
        char = self.model_characteristics.get(
            self.model_name, self.model_characteristics["gpt-3.5-turbo"]
        )

        # ç”Ÿæˆå“åº”å†…å®¹
        content = self._generate_realistic_response(prompt, context, char)

        # è®¡ç®—tokenä½¿ç”¨é‡
        input_tokens = self._calculate_tokens(prompt)
        output_tokens = self._calculate_tokens(content)

        # è®¡ç®—ç²¾ç¡®æˆæœ¬
        cost = self._calculate_cost(input_tokens, output_tokens)

        # æ›´æ–°ç»Ÿè®¡
        self.request_count += 1
        self.total_tokens += input_tokens + output_tokens
        self.total_cost += cost

        return {
            "content": content,
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "cost": cost,
            "model_name": self.model_name,
            "provider": self.provider_name,
        }

    def _generate_realistic_response(
        self, prompt: str, context: Dict[str, Any], char: Dict[str, str]
    ) -> str:
        """åŸºäºæ¨¡å‹ç‰¹æ€§ç”ŸæˆçœŸå®çš„å“åº”"""

        if context and context.get("source") == "ARC-Easy":
            return self._generate_science_response(prompt, context, char)
        elif context and context.get("source") == "GSM8K":
            return self._generate_math_response(prompt, context, char)
        else:
            return self._generate_general_response(prompt, char)

    def _generate_science_response(
        self, prompt: str, context: Dict[str, Any], char: Dict[str, str]
    ) -> str:
        """ç”Ÿæˆç§‘å­¦é—®é¢˜å“åº”"""
        expected = context.get("expected_output", "scientific answer")

        if char["response_style"] == "structured":
            return f"Based on scientific principles: {expected}. This follows from fundamental concepts in the field."
        elif char["response_style"] == "analytical":
            return f"Analyzing this scientifically, we can determine that {expected}. The reasoning involves understanding the underlying mechanisms and applying established scientific knowledge to reach this conclusion."
        elif char["response_style"] == "conversational":
            return f"So looking at this science question, the answer is {expected}. This makes sense when you think about how these processes work."
        else:  # precise
            return f"Answer: {expected}. Scientific basis: established principles."

    def _generate_math_response(
        self, prompt: str, context: Dict[str, Any], char: Dict[str, str]
    ) -> str:
        """ç”Ÿæˆæ•°å­¦é—®é¢˜å“åº”"""
        answer = context.get("ground_truth", "calculated result")

        if char["response_style"] == "structured":
            return f"Step-by-step solution: First, identify given values. Then apply appropriate operations. Final answer: {answer}."
        elif char["response_style"] == "analytical":
            return f"To solve this problem, I need to carefully analyze the given information and apply mathematical reasoning. Working through the calculations systematically, the answer is {answer}."
        elif char["response_style"] == "conversational":
            return (
                f"Let me work through this math problem. When I calculate it out, I get {answer}."
            )
        else:  # precise
            return f"Calculation: {answer}."

    def _generate_general_response(self, prompt: str, char: Dict[str, str]) -> str:
        """ç”Ÿæˆé€šç”¨å“åº”"""
        if char["response_style"] == "structured":
            return "Based on the information provided, I can offer a structured analysis of this topic with relevant details."
        elif char["response_style"] == "analytical":
            return "This question requires careful consideration of multiple factors. Let me provide a comprehensive analysis that addresses the key aspects."
        elif char["response_style"] == "conversational":
            return "That's an interesting question! Let me share some thoughts on this topic."
        else:  # precise
            return "Response: Direct answer addressing the core question."

    def _calculate_tokens(self, text: str) -> int:
        """è®¡ç®—tokenæ•°é‡ï¼ˆè¿‘ä¼¼ï¼‰"""
        # ç®€åŒ–çš„tokenè®¡ç®—ï¼šçº¦1.3å€å•è¯æ•°
        word_count = len(text.split())
        return max(1, int(word_count * 1.3))

    def _calculate_cost(self, input_tokens: int, output_tokens: int) -> float:
        """è®¡ç®—ç²¾ç¡®æˆæœ¬"""
        pricing = self.pricing.get(self.model_name, self.pricing["gpt-3.5-turbo"])

        input_cost = (input_tokens / 1000) * pricing["input"]
        output_cost = (output_tokens / 1000) * pricing["output"]

        return input_cost + output_cost


class RealisticLLMJudge:
    """çœŸå®çš„LLMè¯„åˆ¤è€…"""

    def __init__(self):
        self.evaluation_count = 0
        self.judge_provider = CostControlledProvider("Judge", "gpt-4o-mini", "budget")

    async def evaluate_responses(
        self,
        question: str,
        response_a: Dict[str, Any],
        response_b: Dict[str, Any],
        provider_a: str,
        provider_b: str,
        context: Dict[str, Any] = None,
    ) -> Dict[str, Any]:
        """è¯„ä¼°ä¸¤ä¸ªå“åº”"""

        # æ„å»ºè¯„ä¼°æç¤º
        judge_prompt = self._build_evaluation_prompt(
            question, response_a["content"], response_b["content"], context
        )

        # ä½¿ç”¨Judgeæ¨¡å‹è¿›è¡Œè¯„ä¼°
        judge_response = await self.judge_provider.generate_response(judge_prompt)

        # è§£æè¯„ä¼°ç»“æœ
        evaluation = self._parse_evaluation(
            judge_response["content"], response_a["content"], response_b["content"], context
        )
        evaluation["judge_cost"] = judge_response["cost"]
        evaluation["judge_tokens"] = (
            judge_response["input_tokens"] + judge_response["output_tokens"]
        )

        self.evaluation_count += 1
        return evaluation

    def _build_evaluation_prompt(
        self, question: str, response_a: str, response_b: str, context: Dict[str, Any]
    ) -> str:
        """æ„å»ºè¯„ä¼°æç¤º"""
        category = context.get("category", "general") if context else "general"

        return f"""
Evaluate which response is better for this {category} question.

Question: {question}

Response A: {response_a}

Response B: {response_b}

Consider: accuracy, clarity, completeness, helpfulness.
Choose: A, B, or Tie
Provide brief reasoning.
"""

    def _parse_evaluation(
        self, judge_response: str, response_a: str, response_b: str, context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """è§£æè¯„ä¼°ç»“æœ"""

        # å¯å‘å¼è¯„ä¼°é€»è¾‘
        len_a, len_b = len(response_a), len(response_b)

        # åŸºç¡€è¯„åˆ†
        score_a = 7.0
        score_b = 7.0

        # æ£€æŸ¥æ˜¯å¦åŒ…å«é¢„æœŸç­”æ¡ˆ
        if context:
            expected = str(context.get("expected_output", "")).lower()
            ground_truth = str(context.get("ground_truth", "")).lower()

            if expected and (expected in response_a.lower() or ground_truth in response_a.lower()):
                score_a += 1.5
            if expected and (expected in response_b.lower() or ground_truth in response_b.lower()):
                score_b += 1.5

        # é•¿åº¦å’Œè¯¦ç»†ç¨‹åº¦
        if len_a > len_b * 1.2:
            score_a += 0.5
        elif len_b > len_a * 1.2:
            score_b += 0.5

        # ç»“æ„åŒ–è¡¨è¾¾
        if any(word in response_a.lower() for word in ["step", "first", "analysis", "because"]):
            score_a += 0.5
        if any(word in response_b.lower() for word in ["step", "first", "analysis", "because"]):
            score_b += 0.5

        # ç¡®å®šè·èƒœè€…
        score_diff = abs(score_a - score_b)
        if score_a > score_b + 0.3:
            winner = "A"
            confidence = min(0.95, 0.6 + score_diff / 5)
        elif score_b > score_a + 0.3:
            winner = "B"
            confidence = min(0.95, 0.6 + score_diff / 5)
        else:
            winner = "Tie"
            confidence = 0.5

        # ç”Ÿæˆè¯„ä¼°ç†ç”±
        if winner == "A":
            reasoning = f"Response A scores higher ({score_a:.1f} vs {score_b:.1f}) due to better accuracy or detail."
        elif winner == "B":
            reasoning = f"Response B scores higher ({score_b:.1f} vs {score_a:.1f}) due to better accuracy or detail."
        else:
            reasoning = (
                f"Both responses are comparable in quality ({score_a:.1f} vs {score_b:.1f})."
            )

        return {
            "winner": winner,
            "confidence": round(confidence, 2),
            "reasoning": reasoning,
            "scores": {"response_a": round(score_a, 1), "response_b": round(score_b, 1)},
        }


class CostControlledTestRunner:
    """æˆæœ¬æ§åˆ¶çš„æµ‹è¯•è¿è¡Œå™¨ - æ€§èƒ½ä¼˜åŒ–ç‰ˆ"""

    def __init__(self, max_cost_usd: float = 2.0):
        # åˆå§‹åŒ–ä¾¿å®œçš„æ¨¡å‹
        self.provider_a = CostControlledProvider("Gemini", "gemini-flash")
        self.provider_b = CostControlledProvider("GPT-4o", "gpt-4o-mini")
        self.judge = RealisticLLMJudge()

        self.max_cost_usd = max_cost_usd
        self.test_results = []
        self.current_cost = 0.0

        # æ€§èƒ½ä¼˜åŒ–: æ•°æ®é¢„åŠ è½½å’Œç¼“å­˜
        self._cached_samples = None
        self._connection_pool = None
        self._batch_size = 10  # å¢åŠ æ‰¹å¤„ç†å¤§å°
        self._metrics_cache = {}

        # èµ„æºç®¡ç†ä¼˜åŒ–
        self._semaphore = asyncio.Semaphore(50)  # é™åˆ¶å¹¶å‘æ•°é¿å…èµ„æºè€—å°½
        self._start_time = None

    def load_test_samples(self, target_count: int = 100) -> List[Dict[str, Any]]:
        """åŠ è½½æµ‹è¯•æ ·æœ¬ - æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬"""
        # ç¼“å­˜æ ·æœ¬é¿å…é‡å¤åŠ è½½
        if self._cached_samples and len(self._cached_samples) >= target_count:
            return self._cached_samples[:target_count]

        datasets_dir = Path("data/processed")
        all_samples = []

        # å¹¶è¡ŒåŠ è½½æ•°æ®é›†ä»¥æé«˜æ•ˆç‡
        async def load_dataset_async():
            tasks = []

            # åŠ è½½ARCæ•°æ®é›†
            arc_file = datasets_dir / "arc_easy.json"
            if arc_file.exists():
                tasks.append(self._load_json_file(arc_file))

            # åŠ è½½GSM8Kæ•°æ®é›†
            gsm8k_file = datasets_dir / "gsm8k.json"
            if gsm8k_file.exists():
                tasks.append(self._load_json_file(gsm8k_file))

            if tasks:
                datasets = await asyncio.gather(*tasks)
                for dataset in datasets:
                    all_samples.extend(dataset[: target_count // len(datasets)])

        # å…¼å®¹åŒæ­¥è°ƒç”¨çš„ä¸´æ—¶è§£å†³æ–¹æ¡ˆ
        try:
            asyncio.get_event_loop().run_until_complete(load_dataset_async())
        except RuntimeError:
            # å›é€€åˆ°åŒæ­¥åŠ è½½
            arc_file = datasets_dir / "arc_easy.json"
            if arc_file.exists():
                with open(arc_file, "r", encoding="utf-8") as f:
                    arc_data = json.load(f)
                all_samples.extend(arc_data[: target_count // 2])

            gsm8k_file = datasets_dir / "gsm8k.json"
            if gsm8k_file.exists():
                with open(gsm8k_file, "r", encoding="utf-8") as f:
                    gsm8k_data = json.load(f)
                all_samples.extend(gsm8k_data[: target_count // 2])

        # é¢„å¤„ç†å’Œç¼“å­˜
        random.shuffle(all_samples)
        self._cached_samples = all_samples[: target_count * 2]  # ç¼“å­˜æ›´å¤šæ ·æœ¬ä¾›åç»­ä½¿ç”¨
        return self._cached_samples[:target_count]

    async def _load_json_file(self, file_path: Path) -> List[Dict[str, Any]]:
        """å¼‚æ­¥åŠ è½½JSONæ–‡ä»¶"""
        import aiofiles

        try:
            async with aiofiles.open(file_path, "r", encoding="utf-8") as f:
                content = await f.read()
                return json.loads(content)
        except Exception:
            # å›é€€åˆ°åŒæ­¥åŠ è½½
            with open(file_path, "r", encoding="utf-8") as f:
                return json.load(f)

    def estimate_cost_per_sample(self) -> float:
        """ä¼°ç®—æ¯ä¸ªæ ·æœ¬çš„æˆæœ¬"""
        # åŸºäºå®šä»·è®¡ç®—é¢„ä¼°æˆæœ¬
        avg_input_tokens = 100  # å¹³å‡è¾“å…¥tokenæ•°
        avg_output_tokens = 80  # å¹³å‡è¾“å‡ºtokenæ•°

        # Provider Aæˆæœ¬
        pricing_a = self.provider_a.pricing[self.provider_a.model_name]
        cost_a = (avg_input_tokens / 1000) * pricing_a["input"] + (
            avg_output_tokens / 1000
        ) * pricing_a["output"]

        # Provider Bæˆæœ¬
        pricing_b = self.provider_b.pricing[self.provider_b.model_name]
        cost_b = (avg_input_tokens / 1000) * pricing_b["input"] + (
            avg_output_tokens / 1000
        ) * pricing_b["output"]

        # Judgeæˆæœ¬
        judge_input = 200  # è¯„ä¼°æç¤ºæ›´é•¿
        judge_output = 50  # è¯„ä¼°ç»“æœè¾ƒçŸ­
        pricing_judge = self.judge.judge_provider.pricing[self.judge.judge_provider.model_name]
        cost_judge = (judge_input / 1000) * pricing_judge["input"] + (
            judge_output / 1000
        ) * pricing_judge["output"]

        return cost_a + cost_b + cost_judge

    async def run_cost_controlled_test(self, target_samples: int = 100) -> Dict[str, Any]:
        """è¿è¡Œæˆæœ¬æ§åˆ¶çš„æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹æˆæœ¬æ§åˆ¶çš„å¤§è§„æ¨¡LLMè¯„ä¼°æµ‹è¯•")
        logger.info("=" * 70)

        # æˆæœ¬é¢„ä¼°
        estimated_cost_per_sample = self.estimate_cost_per_sample()
        estimated_total_cost = estimated_cost_per_sample * target_samples

        logger.info(f"ğŸ’° æˆæœ¬é¢„ä¼°:")
        logger.info(f"  æ¯æ ·æœ¬é¢„ä¼°æˆæœ¬: ${estimated_cost_per_sample:.6f}")
        logger.info(f"  {target_samples}æ ·æœ¬é¢„ä¼°æ€»æˆæœ¬: ${estimated_total_cost:.4f}")
        logger.info(f"  é¢„ç®—ä¸Šé™: ${self.max_cost_usd:.2f}")

        if estimated_total_cost > self.max_cost_usd:
            max_safe_samples = int(self.max_cost_usd / estimated_cost_per_sample)
            logger.warning(f"âš ï¸ é¢„ä¼°æˆæœ¬è¶…å‡ºé¢„ç®—ï¼Œå»ºè®®é™åˆ¶ä¸º {max_safe_samples} æ ·æœ¬")
            target_samples = min(target_samples, max_safe_samples)

        # åŠ è½½æµ‹è¯•æ•°æ®
        test_samples = self.load_test_samples(target_samples)
        actual_samples = len(test_samples)

        logger.info(f"ğŸ“š å·²åŠ è½½ {actual_samples} ä¸ªæµ‹è¯•æ ·æœ¬")
        logger.info(
            f"ğŸ¤– Provider A: {self.provider_a.provider_name} ({self.provider_a.model_name})"
        )
        logger.info(
            f"ğŸ¤– Provider B: {self.provider_b.provider_name} ({self.provider_b.model_name})"
        )
        logger.info(
            f"âš–ï¸ Judge: {self.judge.judge_provider.provider_name} ({self.judge.judge_provider.model_name})"
        )

        start_time = time.time()
        completed_samples = 0

        # æ‰¹é‡å¹¶å‘å¤„ç†æ ·æœ¬ï¼Œæé«˜æµ‹è¯•æ•ˆç‡
        batch_size = min(self._batch_size, len(test_samples))
        batches = [
            test_samples[i : i + batch_size] for i in range(0, len(test_samples), batch_size)
        ]

        for batch_idx, batch in enumerate(batches):
            # æ£€æŸ¥æˆæœ¬é™åˆ¶
            if self.current_cost >= self.max_cost_usd:
                logger.warning(f"ğŸ’° è¾¾åˆ°æˆæœ¬ä¸Šé™ ${self.max_cost_usd:.2f}ï¼Œåœæ­¢æµ‹è¯•")
                break

            try:
                # å¹¶å‘å¤„ç†æ‰¹æ¬¡
                batch_tasks = []
                for i, sample in enumerate(batch):
                    sample_idx = batch_idx * batch_size + i + 1
                    task = self._process_sample_with_semaphore(sample, sample_idx)
                    batch_tasks.append(task)

                # ç­‰å¾…æ‰¹æ¬¡å®Œæˆ
                batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)

                # å¤„ç†æ‰¹æ¬¡ç»“æœ
                for result in batch_results:
                    if isinstance(result, Exception):
                        logger.error(f"âŒ æ‰¹æ¬¡å¤„ç†å¤±è´¥: {result}")
                        continue

                    if result:  # ç¡®ä¿ç»“æœæœ‰æ•ˆ
                        self.test_results.append(result)
                        completed_samples += 1

                # æ›´æ–°å½“å‰æˆæœ¬
                self.current_cost = (
                    self.provider_a.total_cost
                    + self.provider_b.total_cost
                    + self.judge.judge_provider.total_cost
                )

                # è¿›åº¦æŠ¥å‘Š
                progress = (completed_samples / actual_samples) * 100
                logger.info(
                    f"ğŸ“Š æ‰¹æ¬¡ {batch_idx + 1}/{len(batches)} å®Œæˆ | è¿›åº¦: {completed_samples}/{actual_samples} ({progress:.1f}%) | å½“å‰æˆæœ¬: ${self.current_cost:.4f}"
                )

                # æ‰¹æ¬¡é—´çŸ­æš‚å»¶è¿Ÿï¼Œé¿å…è¿‡åº¦è´Ÿè½½
                if batch_idx < len(batches) - 1:
                    await asyncio.sleep(0.1)

            except Exception as e:
                logger.error(f"âŒ æ‰¹æ¬¡ {batch_idx + 1} å¤„ç†å¤±è´¥: {e}")
                continue

        total_time = time.time() - start_time

        # ç”Ÿæˆæ±‡æ€»
        summary = self._generate_cost_summary(total_time, target_samples, completed_samples)

        # ä¿å­˜ç»“æœ
        self._save_cost_results(summary)

        return summary

    async def _process_sample_with_semaphore(
        self, sample: Dict[str, Any], sample_num: int
    ) -> Optional[Dict[str, Any]]:
        """ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘çš„æ ·æœ¬å¤„ç†"""
        async with self._semaphore:
            try:
                return await self.evaluate_single_sample(sample, sample_num)
            except Exception as e:
                logger.error(f"âŒ æ ·æœ¬ {sample_num} å¤„ç†å¤±è´¥: {e}")
                return None

    async def evaluate_single_sample(
        self, sample: Dict[str, Any], sample_num: int
    ) -> Dict[str, Any]:
        """è¯„ä¼°å•ä¸ªæ ·æœ¬"""
        question = sample["prompt"]

        # å¹¶å‘è·å–å“åº”
        response_a_task = self.provider_a.generate_response(question, sample)
        response_b_task = self.provider_b.generate_response(question, sample)

        start_time = time.time()
        response_a, response_b = await asyncio.gather(response_a_task, response_b_task)
        response_time = time.time() - start_time

        # è¯„ä¼°
        eval_start_time = time.time()
        evaluation = await self.judge.evaluate_responses(
            question,
            response_a,
            response_b,
            self.provider_a.provider_name,
            self.provider_b.provider_name,
            sample,
        )
        eval_time = time.time() - eval_start_time

        # è®¡ç®—æ€»æˆæœ¬
        total_cost = response_a["cost"] + response_b["cost"] + evaluation["judge_cost"]

        return {
            "sample_num": sample_num,
            "sample_data": {
                "id": sample["id"],
                "category": sample.get("category", "unknown"),
                "source": sample.get("source", "unknown"),
            },
            "responses": {
                "provider_a": {
                    "name": f"{response_a['provider']} ({response_a['model_name']})",
                    "content": (
                        response_a["content"][:150] + "..."
                        if len(response_a["content"]) > 150
                        else response_a["content"]
                    ),
                    "tokens": response_a["input_tokens"] + response_a["output_tokens"],
                    "cost": response_a["cost"],
                },
                "provider_b": {
                    "name": f"{response_b['provider']} ({response_b['model_name']})",
                    "content": (
                        response_b["content"][:150] + "..."
                        if len(response_b["content"]) > 150
                        else response_b["content"]
                    ),
                    "tokens": response_b["input_tokens"] + response_b["output_tokens"],
                    "cost": response_b["cost"],
                },
            },
            "evaluation": evaluation,
            "timing": {
                "response_time": round(response_time, 3),
                "evaluation_time": round(eval_time, 3),
            },
            "total_cost": round(total_cost, 6),
        }

    def _generate_cost_summary(
        self, total_time: float, target_samples: int, completed_samples: int
    ) -> Dict[str, Any]:
        """ç”Ÿæˆæˆæœ¬æ±‡æ€»"""
        if not self.test_results:
            return {}

        # åŸºç¡€ç»Ÿè®¡
        provider_a_wins = sum(1 for r in self.test_results if r["evaluation"]["winner"] == "A")
        provider_b_wins = sum(1 for r in self.test_results if r["evaluation"]["winner"] == "B")
        ties = sum(1 for r in self.test_results if r["evaluation"]["winner"] == "Tie")

        # æˆæœ¬åˆ†æ
        total_cost = self.current_cost
        avg_cost_per_sample = total_cost / completed_samples if completed_samples > 0 else 0

        provider_a_cost = self.provider_a.total_cost
        provider_b_cost = self.provider_b.total_cost
        judge_cost = self.judge.judge_provider.total_cost

        # Tokenåˆ†æ
        total_tokens = (
            self.provider_a.total_tokens
            + self.provider_b.total_tokens
            + self.judge.judge_provider.total_tokens
        )

        # æ€§èƒ½åˆ†æ
        avg_response_time = (
            sum(r["timing"]["response_time"] for r in self.test_results) / completed_samples
        )
        avg_eval_time = (
            sum(r["timing"]["evaluation_time"] for r in self.test_results) / completed_samples
        )

        # æŒ‰ç±»å‹åˆ†æ
        category_analysis = {}
        for result in self.test_results:
            category = result["sample_data"]["category"]
            if category not in category_analysis:
                category_analysis[category] = {"A": 0, "B": 0, "Tie": 0, "total": 0}

            winner = result["evaluation"]["winner"]
            category_analysis[category][winner] += 1
            category_analysis[category]["total"] += 1

        return {
            "test_info": {
                "test_name": "Cost-Controlled LLM Evaluation",
                "timestamp": datetime.now().isoformat(),
                "target_samples": target_samples,
                "completed_samples": completed_samples,
                "completion_rate": (
                    round(completed_samples / target_samples, 3) if target_samples > 0 else 0
                ),
                "total_time": round(total_time, 2),
            },
            "providers": {
                "provider_a": {
                    "name": self.provider_a.provider_name,
                    "model": self.provider_a.model_name,
                    "requests": self.provider_a.request_count,
                    "tokens": self.provider_a.total_tokens,
                    "cost": round(provider_a_cost, 6),
                },
                "provider_b": {
                    "name": self.provider_b.provider_name,
                    "model": self.provider_b.model_name,
                    "requests": self.provider_b.request_count,
                    "tokens": self.provider_b.total_tokens,
                    "cost": round(provider_b_cost, 6),
                },
                "judge": {
                    "name": self.judge.judge_provider.provider_name,
                    "model": self.judge.judge_provider.model_name,
                    "evaluations": self.judge.evaluation_count,
                    "tokens": self.judge.judge_provider.total_tokens,
                    "cost": round(judge_cost, 6),
                },
            },
            "results": {
                "provider_a_wins": provider_a_wins,
                "provider_b_wins": provider_b_wins,
                "ties": ties,
                "win_rate_a": (
                    round(provider_a_wins / completed_samples, 3) if completed_samples > 0 else 0
                ),
                "win_rate_b": (
                    round(provider_b_wins / completed_samples, 3) if completed_samples > 0 else 0
                ),
            },
            "cost_analysis": {
                "total_cost": round(total_cost, 6),
                "budget_used": round((total_cost / self.max_cost_usd) * 100, 1),
                "avg_cost_per_sample": round(avg_cost_per_sample, 6),
                "cost_breakdown": {
                    "provider_a": (
                        round((provider_a_cost / total_cost) * 100, 1) if total_cost > 0 else 0
                    ),
                    "provider_b": (
                        round((provider_b_cost / total_cost) * 100, 1) if total_cost > 0 else 0
                    ),
                    "judge": round((judge_cost / total_cost) * 100, 1) if total_cost > 0 else 0,
                },
            },
            "token_analysis": {
                "total_tokens": total_tokens,
                "avg_tokens_per_sample": (
                    round(total_tokens / completed_samples, 1) if completed_samples > 0 else 0
                ),
                "token_distribution": {
                    "provider_a": self.provider_a.total_tokens,
                    "provider_b": self.provider_b.total_tokens,
                    "judge": self.judge.judge_provider.total_tokens,
                },
            },
            "performance": {
                "avg_response_time": round(avg_response_time, 3),
                "avg_evaluation_time": round(avg_eval_time, 3),
                "throughput": round(completed_samples / total_time, 2),
                "total_time": round(total_time, 2),
            },
            "category_analysis": category_analysis,
            "sample_results": self.test_results[:5],  # åªä¿å­˜å‰5ä¸ªè¯¦ç»†ç»“æœ
        }

    def _save_cost_results(self, summary: Dict[str, Any]):
        """ä¿å­˜æˆæœ¬æ§åˆ¶æµ‹è¯•ç»“æœ"""
        results_dir = Path("logs/cost_controlled_results")
        results_dir.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = results_dir / f"cost_controlled_test_{timestamp}.json"

        with open(results_file, "w", encoding="utf-8") as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)

        logger.info(f"\nğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {results_file}")

    def display_cost_summary(self, summary: Dict[str, Any]):
        """æ˜¾ç¤ºæˆæœ¬æ§åˆ¶æµ‹è¯•æ±‡æ€»"""
        if not summary:
            return

        logger.info("\n" + "=" * 70)
        logger.info("ğŸ“Š æˆæœ¬æ§åˆ¶ LLM è¯„ä¼°æµ‹è¯•æ±‡æ€»")
        logger.info("=" * 70)

        test_info = summary["test_info"]
        providers = summary["providers"]
        results = summary["results"]
        cost = summary["cost_analysis"]
        tokens = summary["token_analysis"]
        performance = summary["performance"]

        # åŸºæœ¬ä¿¡æ¯
        logger.info(f"ğŸ•’ æµ‹è¯•æ—¶é—´: {test_info['timestamp']}")
        logger.info(f"ğŸ“Š ç›®æ ‡æ ·æœ¬: {test_info['target_samples']}")
        logger.info(
            f"âœ… å®Œæˆæ ·æœ¬: {test_info['completed_samples']} ({test_info['completion_rate']:.1%})"
        )
        logger.info(f"â±ï¸ æ€»è€—æ—¶: {test_info['total_time']}ç§’")
        logger.info(f"ğŸš€ ååé‡: {performance['throughput']} æ ·æœ¬/ç§’")

        # æä¾›å•†å¯¹æ¯”
        logger.info(f"\nğŸ¤– æä¾›å•†å¯¹æ¯”:")
        logger.info(
            f"  Provider A: {providers['provider_a']['name']} ({providers['provider_a']['model']})"
        )
        logger.info(
            f"    è¯·æ±‚æ•°: {providers['provider_a']['requests']} | Tokens: {providers['provider_a']['tokens']:,} | æˆæœ¬: ${providers['provider_a']['cost']}"
        )
        logger.info(
            f"  Provider B: {providers['provider_b']['name']} ({providers['provider_b']['model']})"
        )
        logger.info(
            f"    è¯·æ±‚æ•°: {providers['provider_b']['requests']} | Tokens: {providers['provider_b']['tokens']:,} | æˆæœ¬: ${providers['provider_b']['cost']}"
        )
        logger.info(f"  Judge: {providers['judge']['name']} ({providers['judge']['model']})")
        logger.info(
            f"    è¯„ä¼°æ•°: {providers['judge']['evaluations']} | Tokens: {providers['judge']['tokens']:,} | æˆæœ¬: ${providers['judge']['cost']}"
        )

        # æ¯”èµ›ç»“æœ
        logger.info(f"\nğŸ† æ¯”èµ›ç»“æœ:")
        logger.info(
            f"  Provider A è·èƒœ: {results['provider_a_wins']} æ¬¡ ({results['win_rate_a']:.1%})"
        )
        logger.info(
            f"  Provider B è·èƒœ: {results['provider_b_wins']} æ¬¡ ({results['win_rate_b']:.1%})"
        )
        logger.info(f"  å¹³å±€: {results['ties']} æ¬¡")

        # æˆæœ¬åˆ†æ
        logger.info(f"\nğŸ’° æˆæœ¬åˆ†æ:")
        logger.info(f"  æ€»æˆæœ¬: ${cost['total_cost']}")
        logger.info(f"  é¢„ç®—ä½¿ç”¨: {cost['budget_used']}%")
        logger.info(f"  æ¯æ ·æœ¬å¹³å‡æˆæœ¬: ${cost['avg_cost_per_sample']}")
        logger.info(
            f"  æˆæœ¬åˆ†å¸ƒ: A-{cost['cost_breakdown']['provider_a']}% | B-{cost['cost_breakdown']['provider_b']}% | Judge-{cost['cost_breakdown']['judge']}%"
        )

        # Tokenåˆ†æ
        logger.info(f"\nğŸ”¤ Tokenåˆ†æ:")
        logger.info(f"  æ€»Tokenæ•°: {tokens['total_tokens']:,}")
        logger.info(f"  æ¯æ ·æœ¬å¹³å‡Token: {tokens['avg_tokens_per_sample']:.0f}")

        # ç»“è®º
        logger.info(f"\nğŸ¯ æµ‹è¯•ç»“è®º:")
        if results["provider_a_wins"] > results["provider_b_wins"]:
            winner = providers["provider_a"]["name"]
            winner_rate = results["win_rate_a"]
        elif results["provider_b_wins"] > results["provider_a_wins"]:
            winner = providers["provider_b"]["name"]
            winner_rate = results["win_rate_b"]
        else:
            winner = "å¹³å±€"
            winner_rate = 0.5

        if winner_rate >= 0.6:
            logger.info(f"  ğŸ‰ {winner} è¡¨ç°æ›´ä¼˜ç§€ (èƒœç‡: {winner_rate:.1%})")
        else:
            logger.info(f"  âš–ï¸ ä¸¤ä¸ªæ¨¡å‹è¡¨ç°ç›¸è¿‘")

        logger.info(
            f"  ğŸ’¡ æˆæœ¬æ•ˆç‡: ${cost['total_cost']:.4f} æ€»æˆæœ¬ï¼Œæ¯æ ·æœ¬ ${cost['avg_cost_per_sample']:.6f}"
        )

        # æˆæœ¬é¢„ä¼°
        if cost["total_cost"] > 0:
            cost_per_100 = cost["avg_cost_per_sample"] * 100
            cost_per_1000 = cost["avg_cost_per_sample"] * 1000
            logger.info(
                f"  ğŸ“ˆ æˆæœ¬é¢„ä¼°: 100æ ·æœ¬çº¦${cost_per_100:.3f} | 1000æ ·æœ¬çº¦${cost_per_1000:.2f}"
            )


async def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸ¯ æˆæœ¬æ§åˆ¶çš„å¤§è§„æ¨¡LLMè¯„ä¼°æµ‹è¯•")
    logger.info("ä½¿ç”¨è¶…ä¾¿å®œæ¨¡å‹ç»„åˆè¿›è¡Œ100æ ·æœ¬è¯„ä¼°")

    # åˆ›å»ºæµ‹è¯•è¿è¡Œå™¨
    tester = CostControlledTestRunner(max_cost_usd=1.0)  # é¢„ç®—$1

    # è¿è¡Œæµ‹è¯•
    summary = await tester.run_cost_controlled_test(target_samples=100)

    if summary:
        # æ˜¾ç¤ºç»“æœ
        tester.display_cost_summary(summary)
        logger.info("\nâœ… æˆæœ¬æ§åˆ¶æµ‹è¯•å®Œæˆï¼")
    else:
        logger.error("âŒ æµ‹è¯•å¤±è´¥")

    return summary


if __name__ == "__main__":
    asyncio.run(main())
