#!/usr/bin/env python3
"""
Performance Test Infrastructure Optimizer
ä¸“ä¸ºLLM A/Bæµ‹è¯•å¹³å°è®¾è®¡çš„æ€§èƒ½æµ‹è¯•ä¼˜åŒ–å™¨
"""

import asyncio
import json
import logging
import threading
import time
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional

import aiofiles
import psutil

# Configure logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)


@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç±»"""

    test_name: str
    start_time: datetime
    end_time: Optional[datetime] = None
    samples_processed: int = 0
    total_samples: int = 0
    success_rate: float = 0.0
    average_response_time: float = 0.0
    throughput: float = 0.0  # samples per second

    # èµ„æºä½¿ç”¨æŒ‡æ ‡
    peak_memory_mb: float = 0.0
    average_cpu_percent: float = 0.0
    network_io_mb: float = 0.0

    # æˆæœ¬ç›¸å…³æŒ‡æ ‡
    total_cost: float = 0.0
    cost_per_sample: float = 0.0

    # é”™è¯¯ç»Ÿè®¡
    error_count: int = 0
    error_types: Dict[str, int] = field(default_factory=dict)

    # ç¼“å­˜æ•ˆç‡
    cache_hit_rate: float = 0.0
    cache_size_mb: float = 0.0


class PerformanceTestOptimizer:
    """æ€§èƒ½æµ‹è¯•ä¼˜åŒ–å™¨æ ¸å¿ƒç±»"""

    def __init__(
        self,
        cache_dir: Path = Path("tests/performance_cache"),
        max_concurrent_tasks: int = 50,
        batch_size: int = 10,
    ):

        self.cache_dir = cache_dir
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        self.max_concurrent_tasks = max_concurrent_tasks
        self.batch_size = batch_size

        # æ€§èƒ½ç›‘æ§ç»„ä»¶
        self._metrics_cache: Dict[str, Any] = {}
        self._resource_monitor = None
        self._monitoring_thread = None
        self._stop_monitoring = threading.Event()

        # æ•°æ®ç¼“å­˜ç»„ä»¶
        self._dataset_cache: Dict[str, List[Dict[str, Any]]] = {}
        self._result_cache: Dict[str, Any] = {}

        # å¹¶å‘æ§åˆ¶
        self._semaphore = asyncio.Semaphore(max_concurrent_tasks)
        self._executor = ThreadPoolExecutor(max_workers=max_concurrent_tasks)

        # æ€§èƒ½ç»Ÿè®¡
        self.metrics = PerformanceMetrics(test_name="optimizer", start_time=datetime.now())

    def start_monitoring(self):
        """å¯åŠ¨æ€§èƒ½ç›‘æ§"""
        if self._monitoring_thread and self._monitoring_thread.is_alive():
            return

        self._stop_monitoring.clear()
        self._monitoring_thread = threading.Thread(target=self._monitor_resources)
        self._monitoring_thread.daemon = True
        self._monitoring_thread.start()
        logger.info("ğŸ” æ€§èƒ½ç›‘æ§å·²å¯åŠ¨")

    def stop_monitoring(self):
        """åœæ­¢æ€§èƒ½ç›‘æ§"""
        self._stop_monitoring.set()
        if self._monitoring_thread:
            self._monitoring_thread.join(timeout=1.0)
        logger.info("ğŸ“Š æ€§èƒ½ç›‘æ§å·²åœæ­¢")

    def _monitor_resources(self):
        """èµ„æºç›‘æ§çº¿ç¨‹"""
        memory_samples = []
        cpu_samples = []

        while not self._stop_monitoring.is_set():
            try:
                # å†…å­˜ä½¿ç”¨
                process = psutil.Process()
                memory_mb = process.memory_info().rss / 1024 / 1024
                memory_samples.append(memory_mb)

                # CPUä½¿ç”¨
                cpu_percent = psutil.cpu_percent(interval=None)
                cpu_samples.append(cpu_percent)

                # æ›´æ–°å³°å€¼
                self.metrics.peak_memory_mb = max(self.metrics.peak_memory_mb, memory_mb)

                # æ¯10ç§’æ›´æ–°å¹³å‡å€¼
                if len(cpu_samples) >= 10:
                    self.metrics.average_cpu_percent = sum(cpu_samples) / len(cpu_samples)
                    cpu_samples = cpu_samples[-5:]  # ä¿ç•™æœ€è¿‘5ä¸ªæ ·æœ¬

                time.sleep(1)

            except Exception as e:
                logger.warning(f"èµ„æºç›‘æ§é”™è¯¯: {e}")
                time.sleep(1)

    async def load_dataset_cached(
        self, dataset_path: Path, max_samples: Optional[int] = None
    ) -> List[Dict[str, Any]]:
        """ç¼“å­˜ä¼˜åŒ–çš„æ•°æ®é›†åŠ è½½"""

        cache_key = f"{dataset_path.name}_{max_samples or 'all'}"

        # æ£€æŸ¥å†…å­˜ç¼“å­˜
        if cache_key in self._dataset_cache:
            logger.info(f"ğŸ“¦ ä»å†…å­˜ç¼“å­˜åŠ è½½æ•°æ®é›†: {cache_key}")
            return self._dataset_cache[cache_key]

        # æ£€æŸ¥ç£ç›˜ç¼“å­˜
        cache_file = self.cache_dir / f"{cache_key}.json"
        if cache_file.exists():
            try:
                async with aiofiles.open(cache_file, "r", encoding="utf-8") as f:
                    content = await f.read()
                    cached_data = json.loads(content)
                    self._dataset_cache[cache_key] = cached_data
                    logger.info(f"ğŸ’¾ ä»ç£ç›˜ç¼“å­˜åŠ è½½æ•°æ®é›†: {cache_key}")
                    return cached_data
            except Exception as e:
                logger.warning(f"ç£ç›˜ç¼“å­˜åŠ è½½å¤±è´¥: {e}")

        # åŠ è½½åŸå§‹æ•°æ®
        logger.info(f"ğŸ”„ åŠ è½½åŸå§‹æ•°æ®é›†: {dataset_path}")
        start_time = time.time()

        async with aiofiles.open(dataset_path, "r", encoding="utf-8") as f:
            content = await f.read()
            dataset = json.loads(content)

        if max_samples:
            dataset = dataset[:max_samples]

        load_time = time.time() - start_time
        logger.info(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆ: {len(dataset)} æ ·æœ¬, è€—æ—¶ {load_time:.2f}s")

        # ç¼“å­˜åˆ°å†…å­˜å’Œç£ç›˜
        self._dataset_cache[cache_key] = dataset

        try:
            async with aiofiles.open(cache_file, "w", encoding="utf-8") as f:
                await f.write(json.dumps(dataset, ensure_ascii=False, indent=2))
        except Exception as e:
            logger.warning(f"ç£ç›˜ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")

        return dataset

    async def batch_process(
        self,
        items: List[Any],
        process_func: Callable,
        batch_size: Optional[int] = None,
        progress_callback: Optional[Callable] = None,
    ) -> List[Any]:
        """æ‰¹é‡å¹¶å‘å¤„ç†ä¼˜åŒ–å™¨"""

        if not items:
            return []

        batch_size = batch_size or self.batch_size
        total_items = len(items)
        results = []

        # åˆ†æ‰¹å¤„ç†
        for batch_idx in range(0, total_items, batch_size):
            batch = items[batch_idx : batch_idx + batch_size]

            # å¹¶å‘å¤„ç†æ‰¹æ¬¡
            batch_tasks = []
            for item in batch:
                task = self._process_with_semaphore(process_func, item)
                batch_tasks.append(task)

            # ç­‰å¾…æ‰¹æ¬¡å®Œæˆ
            batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)

            # å¤„ç†ç»“æœ
            valid_results = []
            for result in batch_results:
                if isinstance(result, Exception):
                    self.metrics.error_count += 1
                    error_type = type(result).__name__
                    self.metrics.error_types[error_type] = (
                        self.metrics.error_types.get(error_type, 0) + 1
                    )
                    logger.error(f"æ‰¹æ¬¡å¤„ç†é”™è¯¯: {result}")
                else:
                    valid_results.append(result)

            results.extend(valid_results)

            # è¿›åº¦å›è°ƒ
            if progress_callback:
                progress = min(batch_idx + batch_size, total_items) / total_items
                await progress_callback(progress, len(results), self.metrics)

            # æ‰¹æ¬¡é—´å»¶è¿Ÿï¼Œé¿å…è¿‡è½½
            if batch_idx + batch_size < total_items:
                await asyncio.sleep(0.05)

        return results

    async def _process_with_semaphore(self, process_func: Callable, item: Any) -> Any:
        """ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘çš„å¤„ç†å‡½æ•°"""
        async with self._semaphore:
            if asyncio.iscoroutinefunction(process_func):
                return await process_func(item)
            else:
                # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒåŒæ­¥å‡½æ•°
                loop = asyncio.get_event_loop()
                return await loop.run_in_executor(self._executor, process_func, item)

    def cache_result(self, key: str, result: Any, ttl_hours: int = 24):
        """ç¼“å­˜æµ‹è¯•ç»“æœ"""
        cache_entry = {
            "result": result,
            "timestamp": datetime.now().isoformat(),
            "ttl_hours": ttl_hours,
        }

        # å†…å­˜ç¼“å­˜
        self._result_cache[key] = cache_entry

        # ç£ç›˜ç¼“å­˜
        cache_file = self.cache_dir / f"result_{key}.json"
        try:
            with open(cache_file, "w", encoding="utf-8") as f:
                json.dump(cache_entry, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.warning(f"ç»“æœç¼“å­˜ä¿å­˜å¤±è´¥: {e}")

    def get_cached_result(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜çš„æµ‹è¯•ç»“æœ"""

        # æ£€æŸ¥å†…å­˜ç¼“å­˜
        if key in self._result_cache:
            entry = self._result_cache[key]
            timestamp = datetime.fromisoformat(entry["timestamp"])
            ttl = timedelta(hours=entry["ttl_hours"])

            if datetime.now() - timestamp < ttl:
                logger.info(f"ğŸ“¦ ä½¿ç”¨å†…å­˜ç¼“å­˜ç»“æœ: {key}")
                return entry["result"]
            else:
                # è¿‡æœŸï¼Œåˆ é™¤
                del self._result_cache[key]

        # æ£€æŸ¥ç£ç›˜ç¼“å­˜
        cache_file = self.cache_dir / f"result_{key}.json"
        if cache_file.exists():
            try:
                with open(cache_file, "r", encoding="utf-8") as f:
                    entry = json.load(f)
                    timestamp = datetime.fromisoformat(entry["timestamp"])
                    ttl = timedelta(hours=entry["ttl_hours"])

                    if datetime.now() - timestamp < ttl:
                        # åŠ è½½åˆ°å†…å­˜ç¼“å­˜
                        self._result_cache[key] = entry
                        logger.info(f"ğŸ’¾ ä½¿ç”¨ç£ç›˜ç¼“å­˜ç»“æœ: {key}")
                        return entry["result"]
                    else:
                        # è¿‡æœŸï¼Œåˆ é™¤æ–‡ä»¶
                        cache_file.unlink()
            except Exception as e:
                logger.warning(f"ç£ç›˜ç¼“å­˜è¯»å–å¤±è´¥: {e}")

        return None

    def update_metrics(
        self, samples_processed: int = 0, success_count: int = 0, total_cost: float = 0.0
    ):
        """æ›´æ–°æ€§èƒ½æŒ‡æ ‡"""

        self.metrics.samples_processed += samples_processed
        if samples_processed > 0:
            self.metrics.success_rate = success_count / samples_processed

        self.metrics.total_cost += total_cost
        if self.metrics.samples_processed > 0:
            self.metrics.cost_per_sample = self.metrics.total_cost / self.metrics.samples_processed

        # è®¡ç®—ååé‡
        if self.metrics.start_time:
            elapsed = (datetime.now() - self.metrics.start_time).total_seconds()
            if elapsed > 0:
                self.metrics.throughput = self.metrics.samples_processed / elapsed

    def generate_performance_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½ä¼˜åŒ–æŠ¥å‘Š"""

        self.metrics.end_time = datetime.now()
        total_time = (self.metrics.end_time - self.metrics.start_time).total_seconds()

        report = {
            "performance_optimization_report": {
                "timestamp": self.metrics.end_time.isoformat(),
                "test_duration_seconds": total_time,
                "optimization_metrics": {
                    "total_samples_processed": self.metrics.samples_processed,
                    "success_rate": round(self.metrics.success_rate, 3),
                    "throughput_samples_per_second": round(self.metrics.throughput, 2),
                    "average_response_time_ms": round(self.metrics.average_response_time, 2),
                },
                "resource_usage": {
                    "peak_memory_mb": round(self.metrics.peak_memory_mb, 2),
                    "average_cpu_percent": round(self.metrics.average_cpu_percent, 1),
                    "network_io_mb": round(self.metrics.network_io_mb, 2),
                },
                "cost_analysis": {
                    "total_cost_usd": round(self.metrics.total_cost, 6),
                    "cost_per_sample_usd": round(self.metrics.cost_per_sample, 6),
                },
                "error_analysis": {
                    "total_errors": self.metrics.error_count,
                    "error_types": self.metrics.error_types,
                    "error_rate": round(
                        self.metrics.error_count / max(self.metrics.samples_processed, 1), 3
                    ),
                },
                "cache_performance": {
                    "hit_rate": round(self.metrics.cache_hit_rate, 3),
                    "cache_size_mb": round(self.metrics.cache_size_mb, 2),
                    "cached_datasets": len(self._dataset_cache),
                    "cached_results": len(self._result_cache),
                },
                "optimization_recommendations": self._generate_recommendations(),
            }
        }

        return report

    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆæ€§èƒ½ä¼˜åŒ–å»ºè®®"""
        recommendations = []

        # ååé‡å»ºè®®
        if self.metrics.throughput < 5.0:
            recommendations.append("è€ƒè™‘å¢åŠ å¹¶å‘æ•°æˆ–ä¼˜åŒ–å¤„ç†é€»è¾‘ä»¥æé«˜ååé‡")

        # æˆåŠŸç‡å»ºè®®
        if self.metrics.success_rate < 0.95:
            recommendations.append("æˆåŠŸç‡è¾ƒä½ï¼Œæ£€æŸ¥é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶")

        # å†…å­˜ä½¿ç”¨å»ºè®®
        if self.metrics.peak_memory_mb > 1000:
            recommendations.append("å†…å­˜ä½¿ç”¨è¾ƒé«˜ï¼Œè€ƒè™‘ä¼˜åŒ–æ•°æ®ç»“æ„æˆ–å¢åŠ åˆ†é¡µå¤„ç†")

        # CPUä½¿ç”¨å»ºè®®
        if self.metrics.average_cpu_percent > 80:
            recommendations.append("CPUä½¿ç”¨ç‡é«˜ï¼Œè€ƒè™‘ä¼˜åŒ–ç®—æ³•æˆ–å‡å°‘å¹¶å‘æ•°")

        # æˆæœ¬å»ºè®®
        if self.metrics.cost_per_sample > 0.01:
            recommendations.append("æ¯æ ·æœ¬æˆæœ¬è¾ƒé«˜ï¼Œè€ƒè™‘ä½¿ç”¨æ›´ä¾¿å®œçš„æ¨¡å‹æˆ–ä¼˜åŒ–æç¤º")

        # ç¼“å­˜å»ºè®®
        if self.metrics.cache_hit_rate < 0.5:
            recommendations.append("ç¼“å­˜å‘½ä¸­ç‡ä½ï¼Œè€ƒè™‘è°ƒæ•´ç¼“å­˜ç­–ç•¥æˆ–å¢åŠ ç¼“å­˜å¤§å°")

        # é”™è¯¯ç‡å»ºè®®
        error_rate = self.metrics.error_count / max(self.metrics.samples_processed, 1)
        if error_rate > 0.05:
            recommendations.append("é”™è¯¯ç‡è¶…è¿‡5%ï¼Œéœ€è¦æ”¹è¿›é”™è¯¯å¤„ç†æœºåˆ¶")

        if not recommendations:
            recommendations.append("æ€§èƒ½è¡¨ç°è‰¯å¥½ï¼Œç»§ç»­ä¿æŒå½“å‰ä¼˜åŒ–ç­–ç•¥")

        return recommendations

    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self.stop_monitoring()
        self._executor.shutdown(wait=True)

        # æ¸…ç†è¿‡æœŸç¼“å­˜
        await self._cleanup_expired_cache()

        logger.info("ğŸ§¹ æ€§èƒ½ä¼˜åŒ–å™¨èµ„æºæ¸…ç†å®Œæˆ")

    async def _cleanup_expired_cache(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜æ–‡ä»¶"""
        try:
            cache_files = list(self.cache_dir.glob("*.json"))
            cleaned_count = 0

            for cache_file in cache_files:
                try:
                    with open(cache_file, "r", encoding="utf-8") as f:
                        data = json.load(f)
                        if "timestamp" in data and "ttl_hours" in data:
                            timestamp = datetime.fromisoformat(data["timestamp"])
                            ttl = timedelta(hours=data["ttl_hours"])

                            if datetime.now() - timestamp > ttl:
                                cache_file.unlink()
                                cleaned_count += 1
                except Exception:
                    # åˆ é™¤æŸåçš„ç¼“å­˜æ–‡ä»¶
                    cache_file.unlink()
                    cleaned_count += 1

            if cleaned_count > 0:
                logger.info(f"ğŸ—‘ï¸ æ¸…ç†äº† {cleaned_count} ä¸ªè¿‡æœŸç¼“å­˜æ–‡ä»¶")

        except Exception as e:
            logger.warning(f"ç¼“å­˜æ¸…ç†å¤±è´¥: {e}")


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•
async def example_usage():
    """æ€§èƒ½ä¼˜åŒ–å™¨ä½¿ç”¨ç¤ºä¾‹"""

    optimizer = PerformanceTestOptimizer(
        cache_dir=Path("tests/performance_cache"), max_concurrent_tasks=20, batch_size=5
    )

    try:
        # å¯åŠ¨ç›‘æ§
        optimizer.start_monitoring()

        # æ¨¡æ‹Ÿæ•°æ®åŠ è½½
        logger.info("ğŸ“š å¼€å§‹æ•°æ®åŠ è½½æµ‹è¯•...")
        datasets_dir = Path("data/processed")

        if (datasets_dir / "arc_easy.json").exists():
            dataset = await optimizer.load_dataset_cached(
                datasets_dir / "arc_easy.json", max_samples=100
            )
            logger.info(f"âœ… åŠ è½½æ•°æ®é›†: {len(dataset)} æ ·æœ¬")

        # æ¨¡æ‹Ÿæ‰¹é‡å¤„ç†
        async def mock_process_item(item):
            await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            return {"processed": True, "item_id": item.get("id", "unknown")}

        async def progress_callback(progress, completed, metrics):
            logger.info(f"ğŸ“Š å¤„ç†è¿›åº¦: {progress:.1%} ({completed} å®Œæˆ)")

        if "dataset" in locals():
            logger.info("ğŸ”„ å¼€å§‹æ‰¹é‡å¤„ç†æµ‹è¯•...")
            results = await optimizer.batch_process(
                dataset[:20],  # æµ‹è¯•20ä¸ªæ ·æœ¬
                mock_process_item,
                batch_size=5,
                progress_callback=progress_callback,
            )

            optimizer.update_metrics(
                samples_processed=len(results),
                success_count=len(results),
                total_cost=0.05,  # æ¨¡æ‹Ÿæˆæœ¬
            )

            logger.info(f"âœ… å¤„ç†å®Œæˆ: {len(results)} ç»“æœ")

        # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
        report = optimizer.generate_performance_report()

        # ä¿å­˜æŠ¥å‘Š
        report_file = Path("performance_optimization_report.json")
        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        logger.info(f"ğŸ“Š æ€§èƒ½æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        logger.info("ğŸ¯ ä¼˜åŒ–å»ºè®®:")
        for recommendation in report["performance_optimization_report"][
            "optimization_recommendations"
        ]:
            logger.info(f"  â€¢ {recommendation}")

    finally:
        await optimizer.cleanup()


if __name__ == "__main__":
    asyncio.run(example_usage())
